# ICCV-2023-Papers
![Alt text](af0c53186833a908a200f58867b6dcf.png)

## å®˜ç½‘é“¾æ¥ï¼šhttps://iccv2023.thecvf.com/

### ç ”è®¨ä¼š:bell:ï¼š2023 å¹´ 10 æœˆ 2 æ—¥è‡³ 3 æ—¥<br>
### ä¸»ä¼š:bell:ï¼š2023 å¹´ 10 æœˆ 4 æ—¥è‡³ 6 æ—¥

## å†å¹´ç»¼è¿°è®ºæ–‡åˆ†ç±»æ±‡æ€»æˆ³è¿™é‡Œâ†˜ï¸[CV-Surveys](https://github.com/52CV/CV-Surveys)æ–½å·¥ä¸­~~~~~~~~~~

## 2023 å¹´è®ºæ–‡åˆ†ç±»æ±‡æ€»æˆ³è¿™é‡Œ
â†˜ï¸[CVPR-2023-Papers](https://github.com/52CV/CVPR-2023-Papers)
â†˜ï¸[WACV-2023-Papers](https://github.com/52CV/WACV-2023-Papers)
â†˜ï¸[ICCV-2023-Papers](https://github.com/52CV/ICCV-2023-Papers)

## 2022 å¹´è®ºæ–‡åˆ†ç±»æ±‡æ€»æˆ³è¿™é‡Œ
â†˜ï¸[CVPR-2022-Papers](https://github.com/52CV/CVPR-2022-Papers)
â†˜ï¸[WACV-2022-Papers](https://github.com/52CV/WACV-2022-Papers)
â†˜ï¸[ECCV-2022-Papers](https://github.com/52CV/ECCV-2022-Papers)

## 2021å¹´è®ºæ–‡åˆ†ç±»æ±‡æ€»æˆ³è¿™é‡Œ
â†˜ï¸[ICCV-2021-Papers](https://github.com/52CV/ICCV-2021-Papers)
â†˜ï¸[CVPR-2021-Papers](https://github.com/52CV/CVPR-2021-Papers)

## 2020 å¹´è®ºæ–‡åˆ†ç±»æ±‡æ€»æˆ³è¿™é‡Œ
â†˜ï¸[CVPR-2020-Papers](https://github.com/52CV/CVPR-2020-Papers)
â†˜ï¸[ECCV-2020-Papers](https://github.com/52CV/ECCV-2020-Papers)

è®¡650+3 ç¯‡ã€‚ 

## ç›®å½•

|:cat:|:dog:|:tiger:|:wolf:|
|------|------|------|------|
|[1.å…¶å®ƒ(others)](#1)|[2.3D(ä¸‰ç»´é‡å»º\ä¸‰ç»´è§†è§‰)](#2)|[3.Object Detection(ç›®æ ‡æ£€æµ‹)](#3)|[4.Object Tracking(ç›®æ ‡è·Ÿè¸ª)](#4)|
|[5.Biometric Recognition(ç”Ÿç‰©ç‰¹å¾è¯†åˆ«)](#5)|[6.Face(äººè„¸)](#6)|[7.Image Progress(ä½å±‚å›¾åƒå¤„ç†ã€è´¨é‡è¯„ä»·)](#7)|[8.Image Segmentation(å›¾åƒåˆ†å‰²)](#8)|
|[9.Image Classification(å›¾åƒåˆ†ç±»)](#9)|[10.Image Synthesis(å›¾åƒåˆæˆ)](#10)|[11.Image/Video Editing(å›¾åƒ/è§†é¢‘ç¼–è¾‘)](#11)|[12.Medical Image(åŒ»å­¦å½±åƒ)](#12)|
|[13.Image Captions(å›¾åƒå­—å¹•)](#13)|[14.Image/Video Composition(å›¾åƒ/è§†é¢‘å‹ç¼©)](#14)|[15.Image/Video Retrieval(å›¾åƒ/è§†é¢‘æ£€ç´¢)](#15)|[16.Super-Resolution(è¶…åˆ†è¾¨ç‡)](#16)|
|[17.GAN](#17)|[18.Pose](#18)|[19.UAV/Remote Sensing/Satellite Image(æ— äººæœº/é¥æ„Ÿ/å«æ˜Ÿå›¾åƒ)](#19)|[20.Reid](#20)|
|[21.Point Cloud(ç‚¹äº‘)](#21)|[22.OCR](#22)|[23.Optical Flow Estimation(å…‰æµä¼°è®¡)](#23)|[24.Few/Zero-Shot Learning/Domain Generalization/Adaptation(å°/é›¶æ ·æœ¬/åŸŸæ³›åŒ–/åŸŸé€‚åº”)](#24)|
|[25.Model Compression/KD/Pruning(æ¨¡å‹å‹ç¼©/çŸ¥è¯†è’¸é¦/å‰ªæ)](#25)|[26.ML(æœºå™¨å­¦ä¹ )](#26)|[27.Self/Semi-Supervised Learning](#27)|[28.Style Transfer(é£æ ¼è¿ç§»)](#28)|
|[29.Autonomous vehicles(è‡ªåŠ¨é©¾é©¶)](#30)|[30.SLAM/AR/VR/Robotics(å¢å¼º/è™šæ‹Ÿç°å®/æœºå™¨äºº)](#30)|[31.HOI(äººç‰©äº¤äº’)](#31)|[32.Sign Language Recognition(æ‰‹è¯­)](#32)|
|[33.Video](#33)|[34.Action Detection](#34)|[35.Human Motion Prediction(äººä½“è¿åŠ¨é¢„æµ‹)](#35)|[36.Vision Question Answering(è§†è§‰é—®ç­”)](#36)|
|[37.Object Pose Estimation(ç‰©ä½“å§¿åŠ¿ä¼°è®¡)](#37)|[38.Vision-Language(è§†è§‰è¯­è¨€)](#38)|[39.Keypoint Detection(å…³é”®ç‚¹æ£€æµ‹)](#39)|[40.Anomaly Detection(å¼‚å¸¸æ£€æµ‹)](#40)|
|[41.Vision Transformers](#41)|[42.Dataset/Benchmark](#42)|[43.Neural Radiance Fields](#43)|[44.Rendering(æ¸²æŸ“)](#44)|
|[45.Scene Graph Generation(åœºæ™¯å›¾åˆæˆ)](#45)|[46.Edge Detection](#46)|[47.Image-to-Image Translation](#47)|[48.Image Reconstruction](#48)|
|[49.Image Fusion(å›¾åƒèåˆ)](#49)|[50.Image Matching(å›¾åƒåŒ¹é…)](#50)|[51.Visual Place Recognition](#51)|[52.View Synthesis(è§†å›¾åˆæˆ)](#52)|
|[53.Computed Imaging(è®¡ç®—æˆåƒï¼Œå¦‚å…‰å­¦ã€å‡ ä½•ã€å…‰åœºæˆåƒç­‰)](#53)|[54.Gaze Estimation](#54)|[55.sound(è¯­éŸ³)](#55)|


## ğŸ’¥ğŸ’¥ğŸ’¥ICCV 2023 è·å¥–è®ºæ–‡
### æœ€ä½³è®ºæ–‡å¥–â€”â€”é©¬å°”å¥–
* [Adding Conditional Control to Text-to-Image Diffusion Models](https://arxiv.org/pdf/2302.05543.pdf)<br>:star:[code](https://github.com/lllyasviel/ControlNet)
* [Passive Ultra-Wideband Single-Photon Imaging](https://appleswithacapitala.github.io/static/docs/paper.pdf)<br>:star:[code](https://appleswithacapitala.github.io/)
### æœ€ä½³è®ºæ–‡å¥–æå
* [Segment Anything](https://arxiv.org/abs/2304.02643)<br>:house:[project](https://segment-anything.com/)
### æœ€ä½³å­¦ç”Ÿè®ºæ–‡å¥–
* [Tracking Everything Everywhere All at Once](https://browse.arxiv.org/pdf/2306.05422.pdf)<br>:house:[project](https://github.com/qianqianwang68/omnimotion)


## Sketch
* [CLIPascene: Scene Sketching with Different Types and Levels of Abstraction](http://arxiv.org/abs/2211.17256)

## Spiking Neural Networks
* [Towards Memory- and Time-Efficient Backpropagation for Training Spiking Neural Networks](https://openaccess.thecvf.com/content/ICCV2023/papers/Meng_Towards_Memory-_and_Time-Efficient_Backpropagation_for_Training_Spiking_Neural_Networks_ICCV_2023_paper.pdf)

## Dense Prediction(å¯†é›†é¢„æµ‹)
* [Multi-Task Learning with Knowledge Distillation for Dense Prediction](https://openaccess.thecvf.com/content/ICCV2023/papers/Xu_Multi-Task_Learning_with_Knowledge_Distillation_for_Dense_Prediction_ICCV_2023_paper.pdf)
* [Consistent Depth Prediction for Transparent Object Reconstruction from RGB-D Camera](https://openaccess.thecvf.com/content/ICCV2023/papers/Cai_Consistent_Depth_Prediction_for_Transparent_Object_Reconstruction_from_RGB-D_Camera_ICCV_2023_paper.pdf)

## Data Augmentation(æ•°æ®å¢å¼º)
* [HybridAugment++: Unified Frequency Spectra Perturbations for Model Robustness](http://arxiv.org/abs/2307.11823v1)
* [MixBag: Bag-Level Data Augmentation for Learning from Label Proportions](http://arxiv.org/abs/2308.08822v1)
* [When to Learn What: Model-Adaptive Data Augmentation Curriculum](http://arxiv.org/abs/2309.04747)
* [Diverse Data Augmentation with Diffusions for Effective Test-time Prompt Tuning](http://arxiv.org/abs/2308.06038)

## Active Learning(ä¸»åŠ¨å­¦ä¹ )
* [TiDAL: Learning Training Dynamics for Active Learning](http://arxiv.org/abs/2210.06788)
* [HAL3D: Hierarchical Active Learning for Fine-Grained 3D Part Labeling](http://arxiv.org/abs/2301.10460)
* [Knowledge-Aware Federated Active Learning with Non-IID Data](http://arxiv.org/abs/2211.13579)
* [Skip-Plan: Procedure Planning in Instructional Videos via Condensed Action Space Learning](https://openaccess.thecvf.com/content/ICCV2023/papers/Li_Skip-Plan_Procedure_Planning_in_Instructional_Videos_via_Condensed_Action_Space_ICCV_2023_paper.pdf)

## Affordance Learning(å¯ç¤ºå­¦ä¹ )
* [MAAL: Multimodality-Aware Autoencoder-Based Affordance Learning for 3D Articulated Objects](https://openaccess.thecvf.com/content/ICCV2023/papers/Liang_MAAL_Multimodality-Aware_Autoencoder-Based_Affordance_Learning_for_3D_Articulated_Objects_ICCV_2023_paper.pdf)

## Clustering(èšç±»)
* [Cross-modal Scalable Hyperbolic Hierarchical Clustering](https://openaccess.thecvf.com/content/ICCV2023/papers/Long_Cross-modal_Scalable_Hierarchical_Clustering_in_Hyperbolic_space_ICCV_2023_paper.pdf)
* [Cross-view Topology Based Consistent and Complementary Information for Deep Multi-view Clustering](https://openaccess.thecvf.com/content/ICCV2023/papers/Dong_Cross-view_Topology_Based_Consistent_and_Complementary_Information_for_Deep_Multi-view_ICCV_2023_paper.pdf)
* [MHCN: A Hyperbolic Neural Network Model for Multi-view Hierarchical Clustering](https://openaccess.thecvf.com/content/ICCV2023/papers/Lin_MHCN_A_Hyperbolic_Neural_Network_Model_for_Multi-view_Hierarchical_Clustering_ICCV_2023_paper.pdf)
* [Stable Cluster Discrimination for Deep Clustering](https://openaccess.thecvf.com/content/ICCV2023/papers/Qian_Stable_Cluster_Discrimination_for_Deep_Clustering_ICCV_2023_paper.pdf)

## Open Set Recognition(å¼€é›†è¯†åˆ«)
* [FedPD: Federated Open Set Recognition with Parameter Disentanglement](https://openaccess.thecvf.com/content/ICCV2023/papers/Yang_FedPD_Federated_Open_Set_Recognition_with_Parameter_Disentanglement_ICCV_2023_paper.pdf)

## Graph Neural Networks(å›¾ç¥ç»ç½‘ç»œ)
* [VertexSerum: Poisoning Graph Neural Networks for Link Inference](http://arxiv.org/abs/2308.01469)
* [Learning Adaptive Neighborhoods for Graph Neural Networks](http://arxiv.org/abs/2307.09065)

## Deepfake Detectors
* [Towards Understanding the Generalization of Deepfake Detectors from a Game-Theoretical View](https://openaccess.thecvf.com/content/ICCV2023/papers/Yao_Towards_Understanding_the_Generalization_of_Deepfake_Detectors_from_a_Game-Theoretical_ICCV_2023_paper.pdf)
* [Quality-Agnostic Deepfake Detection with Intra-model Collaborative Learning](http://arxiv.org/abs/2309.05911)
* [SeeABLE: Soft Discrepancies and Bounded Contrastive Learning for Exposing Deepfakes](http://arxiv.org/abs/2211.11296)
* [UCF: Uncovering Common Features for Generalizable Deepfake Detection](http://arxiv.org/abs/2304.13949)

## Scene Understanding(åœºæ™¯ç†è§£)
* [Shape Anchor Guided Holistic Indoor Scene Understanding](http://arxiv.org/abs/2309.11133)
* [Efficient Computation Sharing for Multi-Task Visual Scene Understanding](http://arxiv.org/abs/2303.09663)
* [Ordered Atomic Activity for Fine-grained Interactive Traffic Scenario Understanding](https://openaccess.thecvf.com/content/ICCV2023/papers/Agarwal_Ordered_Atomic_Activity_for_Fine-grained_Interactive_Traffic_Scenario_Understanding_ICCV_2023_paper.pdf)
* [Clutter Detection and Removal in 3D Scenes with View-Consistent Inpainting](http://arxiv.org/abs/2304.03763)

## Industrial Defect Detectors
* å·¥ä¸šç¼ºé™·å®šä½
  * [Removing Anomalies as Noises for Industrial Defect Localization](https://openaccess.thecvf.com/content/ICCV2023/papers/Lu_Removing_Anomalies_as_Noises_for_Industrial_Defect_Localization_ICCV_2023_paper.pdf)
* å·¥ä¸šå¼‚å¸¸æ£€æµ‹
  * [PNI : Industrial Anomaly Detection using Position and Neighborhood Information](http://arxiv.org/abs/2211.12634)
* è£‚ç¼æ£€æµ‹
  * [The Devil is in the Crack Orientation: A New Perspective for Crack Detection](https://openaccess.thecvf.com/content/ICCV2023/papers/Chen_The_Devil_is_in_the_Crack_Orientation_A_New_Perspective_ICCV_2023_paper.pdf)


## Group Affect Recognition(ç¾¤ä½“æƒ…æ„Ÿè¯†åˆ«)
* [Most Important Person-Guided Dual-Branch Cross-Patch Attention for Group Affect Recognition](https://openaccess.thecvf.com/content/ICCV2023/papers/Xie_Most_Important_Person-Guided_Dual-Branch_Cross-Patch_Attention_for_Group_Affect_Recognition_ICCV_2023_paper.pdf)




## Visual Localization(è§†è§‰å®šä½)
* [EP2P-Loc: End-to-End 3D Point to 2D Pixel Localization for Large-Scale Visual Localization](http://arxiv.org/abs/2309.07471v1)
* [Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance](http://arxiv.org/abs/2309.07403v1)

## Copyright Protection(ç‰ˆæƒä¿æŠ¤/ä¿¡æ¯å®‰å…¨)
* [Towards Robust Model Watermark via Reducing Parametric Vulnerability](http://arxiv.org/abs/2309.04777v1)<br>:star:[code](https://github.com/GuanhaoGan/robust-model-watermarking)

## scene flow estimation(åœºæ™¯æµä¼°è®¡)
* [EMR-MSF: Self-Supervised Recurrent Monocular Scene Flow Exploiting Ego-Motion Rigidity](http://arxiv.org/abs/2309.01296v1)
* [Fast Neural Scene Flow](http://arxiv.org/abs/2304.09121)
* [Multi-Scale Bidirectional Recurrent Network with Hybrid Correlation for Point Cloud Based Scene Flow Estimation](https://openaccess.thecvf.com/content/ICCV2023/papers/Cheng_Multi-Scale_Bidirectional_Recurrent_Network_with_Hybrid_Correlation_for_Point_Cloud_ICCV_2023_paper.pdf)
* [IHNet: Iterative Hierarchical Network Guided by High-Resolution Estimated Information for Scene Flow Estimation](https://openaccess.thecvf.com/content/ICCV2023/papers/Wang_IHNet_Iterative_Hierarchical_Network_Guided_by_High-Resolution_Estimated_Information_for_ICCV_2023_paper.pdf)

## Semantic Scene Completion(è¯­ä¹‰åœºæ™¯è¡¥å…¨)
* [NDC-Scene: Boost Monocular 3D Semantic Scene Completion in Normalized Device Coordinates Space](http://arxiv.org/abs/2309.14616v1)<br>:star:[code](https://jiawei-yao0812.github.io/NDC-Scene/)<br>:star:[code](https://github.com/Jiawei-Yao0812/NDCScene)
* [DDIT: Semantic Scene Completion via Deformable Deep Implicit Templates](https://openaccess.thecvf.com/content/ICCV2023/papers/Li_DDIT_Semantic_Scene_Completion_via_Deformable_Deep_Implicit_Templates_ICCV_2023_paper.pdf)
* [CVSformer: Cross-View Synthesis Transformer for Semantic Scene Completion](http://arxiv.org/abs/2307.07938)

## NAS
* [ShiftNAS: Improving One-shot NAS via Probability Shift](http://arxiv.org/abs/2307.08300)
* [ROME: Robustifying Memory-Efficient NAS via Topology Disentanglement and Gradient Accumulation](http://arxiv.org/abs/2011.11233)
* [Extensible and Efficient Proxy for Neural Architecture Search](https://openaccess.thecvf.com/content/ICCV2023/papers/Li_Extensible_and_Efficient_Proxy_for_Neural_Architecture_Search_ICCV_2023_paper.pdf)
* [MixPath: A Unified Approach for One-shot Neural Architecture Search](http://arxiv.org/abs/2001.05887)
* [Unleashing the Power of Gradient Signal-to-Noise Ratio for Zero-Shot NAS](https://openaccess.thecvf.com/content/ICCV2023/papers/Sun_Unleashing_the_Power_of_Gradient_Signal-to-Noise_Ratio_for_Zero-Shot_NAS_ICCV_2023_paper.pdf)

<a name="55"/>

## 55.sound(è¯­éŸ³)
* [Be Everywhere - Hear Everything (BEE): Audio Scene Reconstruction by Sparse Audio-Visual Samples](https://openaccess.thecvf.com/content/ICCV2023/papers/Chen_Be_Everywhere_-_Hear_Everything_BEE_Audio_Scene_Reconstruction_by_ICCV_2023_paper.pdf)
* [On the Audio-visual Synchronization for Lip-to-Speech Synthesis](http://arxiv.org/abs/2303.00502)
* [Boosting Positive Segments for Weakly-Supervised Audio-Visual Video Parsing](https://openaccess.thecvf.com/content/ICCV2023/papers/Rachavarapu_Boosting_Positive_Segments_for_Weakly-Supervised_Audio-Visual_Video_Parsing_ICCV_2023_paper.pdf)
* [DiffV2S: Diffusion-based Video-to-Speech Synthesis with Vision-guided Speaker Embedding](http://arxiv.org/abs/2308.07787v1)
* [Omnidirectional Information Gathering for Knowledge Transfer-based Audio-Visual Navigation](http://arxiv.org/abs/2308.10306v1)
* [Sound Source Localization is All about Cross-Modal Alignment](http://arxiv.org/abs/2309.10724v1)
* å»æ··å“
  * [AdVerb: Visually Guided Audio Dereverberation](http://arxiv.org/abs/2308.12370v1)<br>:house:[project](https://gamma.umd.edu/researchdirections/speech/adverb)
* å”‡è¯­è¯†åˆ«
  * [Lip Reading for Low-resource Languages by Learning and Combining General Speech Knowledge and Language-specific Knowledge](http://arxiv.org/abs/2308.09311v1)
* éŸ³é¢‘-è§†é¢‘ç”Ÿæˆ
  * [The Power of Sound (TPoS): Audio Reactive Video Generation with Stable Diffusion](http://arxiv.org/abs/2309.04509v1)<br>:star:[code](https://ku-vai.github.io/TPoS/)
* éŸ³è§†è§‰å¯¼èˆª
  * [Omnidirectional Information Gathering for Knowledge Transfer-Based Audio-Visual Navigation](http://arxiv.org/abs/2308.10306)
* å£°éŸ³å®šä½
  * [Sound Localization from Motion: Jointly Learning Sound Direction and Camera Rotation](http://arxiv.org/abs/2303.11329)
* è§†å¬åˆ†å‰²
  * [Multimodal Variational Auto-encoder based Audio-Visual Segmentation](https://openaccess.thecvf.com/content/ICCV2023/papers/Mao_Multimodal_Variational_Auto-encoder_based_Audio-Visual_Segmentation_ICCV_2023_paper.pdf)

<a name="54"/>

## 54.Gaze Estimation
* [DVGaze: Dual-View Gaze Estimation](http://arxiv.org/abs/2308.10310v1)<br>:star:[code](https://github.com/yihuacheng/DVGaze)

<a name="53"/>

## 53.Computed Imaging(è®¡ç®—æˆåƒï¼Œå¦‚å…‰å­¦ã€å‡ ä½•ã€å…‰åœºæˆåƒç­‰)
* [Event Camera Data Pre-training](http://arxiv.org/abs/2301.01928)
* [Deep Geometrized Cartoon Line Inbetweening](https://openaccess.thecvf.com/content/ICCV2023/papers/Siyao_Deep_Geometrized_Cartoon_Line_Inbetweening_ICCV_2023_paper.pdf)
* [Examining Autoexposure for Challenging Scenes](http://arxiv.org/abs/2309.04542v1)
* [Vanishing Point Estimation in Uncalibrated Images with Prior Gravity Direction](http://arxiv.org/abs/2308.10694v1)<br>:star:[code](https://github.com/cvg/VP-Estimation-with-Prior-Gravity)
* [Robust Frame-to-Frame Camera Rotation Estimation in Crowded Scenes](http://arxiv.org/abs/2309.08588v1)<br>:house:[project](https://fabiendelattre.com/robust-rotation-estimation)
* [Exploring Positional Characteristics of Dual-Pixel Data for Camera Autofocus](https://openaccess.thecvf.com/content/ICCV2023/papers/Choi_Exploring_Positional_Characteristics_of_Dual-Pixel_Data_for_Camera_Autofocus_ICCV_2023_paper.pdf)
* [Enhancing Non-line-of-sight Imaging via Learnable Inverse Kernel and Attention Mechanisms](https://openaccess.thecvf.com/content/ICCV2023/papers/Yu_Enhancing_Non-line-of-sight_Imaging_via_Learnable_Inverse_Kernel_and_Attention_Mechanisms_ICCV_2023_paper.pdf)
* ç›¸æœºå§¿åŠ¿ä¼°è®¡
  * [Multi-body Depth and Camera Pose Estimation from Multiple Views](https://openaccess.thecvf.com/content/ICCV2023/papers/Dal_Cin_Multi-body_Depth_and_Camera_Pose_Estimation_from_Multiple_Views_ICCV_2023_paper.pdf)

<a name="52"/>

## 52.View Synthesis(è§†å›¾åˆæˆ)
* [iVS-Net: Learning Human View Synthesis from Internet Videos](https://openaccess.thecvf.com/content/ICCV2023/papers/Dong_iVS-Net_Learning_Human_View_Synthesis_from_Internet_Videos_ICCV_2023_paper.pdf)
* [Multi-task View Synthesis with Neural Radiance Fields](https://openaccess.thecvf.com/content/ICCV2023/papers/Zheng_Multi-task_View_Synthesis_with_Neural_Radiance_Fields_ICCV_2023_paper.pdf)
* [IntrinsicNeRF: Learning Intrinsic Neural Radiance Fields for Editable Novel View Synthesis](http://arxiv.org/abs/2210.00647)
* [Generative Novel View Synthesis with 3D-Aware Diffusion Models](http://arxiv.org/abs/2304.02602)
* [SparseNeRF: Distilling Depth Ranking for Few-shot Novel View Synthesis](http://arxiv.org/abs/2303.16196)
* [Neural LiDAR Fields for Novel View Synthesis](https://openaccess.thecvf.com/content/ICCV2023/papers/Huang_Neural_LiDAR_Fields_for_Novel_View_Synthesis_ICCV_2023_paper.pdf)
* [LoLep: Single-View View Synthesis with Locally-Learned Planes and Self-Attention Occlusion Inference](http://arxiv.org/abs/2307.12217v1)
* [Learning Unified Decompositional and Compositional NeRF for Editable Novel View Synthesis](http://arxiv.org/abs/2308.02840v1)<br>:star:[code](https://w-ted.github.io/publications/udc-nerf)
* [Efficient View Synthesis with Neural Radiance Distribution Field](http://arxiv.org/abs/2308.11130v1)
* [NeO 360: Neural Fields for Sparse View Synthesis of Outdoor Scenes](http://arxiv.org/abs/2308.12967v1)<br>:star:[code](https://zubair-irshad.github.io/projects/neo360.html)
* [PARF: Primitive-Aware Radiance Fusion for Indoor Scene Novel View Synthesis](https://openaccess.thecvf.com/content/ICCV2023/papers/Ying_PARF_Primitive-Aware_Radiance_Fusion_for_Indoor_Scene_Novel_View_Synthesis_ICCV_2023_paper.pdf)
* [Urban Radiance Field Representation with Deformable Neural Mesh Primitives](http://arxiv.org/abs/2307.10776v1)<br>:house:[project](https://dnmp.github.io/)
* [FlipNeRF: Flipped Reflection Rays for Few-shot Novel View Synthesis](http://arxiv.org/abs/2306.17723)
* [SAMPLING: Scene-adaptive Hierarchical Multiplane Images Representation for Novel View Synthesis from a Single Image](http://arxiv.org/abs/2309.06323)
* [A Large-Scale Outdoor Multi-Modal Dataset and Benchmark for Novel View Synthesis and Implicit Scene Reconstruction](http://arxiv.org/abs/2301.06782)
* [Cross-Ray Neural Radiance Fields for Novel-View Synthesis from Unconstrained Image Collections](http://arxiv.org/abs/2307.08093)
* [Long-Term Photometric Consistent Novel View Synthesis with Diffusion Models](http://arxiv.org/abs/2304.10700)
* [NEMTO: Neural Environment Matting for Novel View and Relighting Synthesis of Transparent Objects](https://openaccess.thecvf.com/content/ICCV2023/papers/Wang_NEMTO_Neural_Environment_Matting_for_Novel_View_and_Relighting_Synthesis_ICCV_2023_paper.pdf)

<a name="51"/>

## 51.Visual Place Recognition
* [CASSPR: Cross Attention Single Scan Place Recognition](http://arxiv.org/abs/2211.12542)
* [EigenPlaces: Training Viewpoint Robust Models for Visual Place Recognition](http://arxiv.org/abs/2308.10832v1)<br>:star:[code](https://github.com/gmberton/auto_VPR)<br>:star:[code](https://github.com/gmberton/EigenPlaces)

<a name="50"/>

## 50.Image Matching(å›¾åƒåŒ¹é…)
* [OccNet: Robust Image Matching Based on 3D Occupancy Estimation for Occluded Regions](https://openaccess.thecvf.com/content/ICCV2023/papers/Fan_Occ2Net_Robust_Image_Matching_Based_on_3D_Occupancy_Estimation_for_ICCV_2023_paper.pdf)<br>:thumbsup:[ICCV 2023|Occ2Netï¼Œä¸€ç§åŸºäº3D å æ®ä¼°è®¡çš„æœ‰æ•ˆä¸”ç¨³å¥çš„å¸¦æœ‰é®æŒ¡åŒºåŸŸçš„å›¾åƒåŒ¹é…æ–¹æ³•](https://mp.weixin.qq.com/s/mbk5tnYlzCLOg4_xfyKRyA)
* [Scene-Aware Feature Matching](http://arxiv.org/abs/2308.09949v1)
* [Improving Transformer-based Image Matching by Cascaded Capturing Spatially Informative Keypoints](http://arxiv.org/abs/2303.02885)
* [GlueStick: Robust Image Matching by Sticking Points and Lines Together](https://openaccess.thecvf.com/content/ICCV2023/papers/Pautrat_GlueStick_Robust_Image_Matching_by_Sticking_Points_and_Lines_Together_ICCV_2023_paper.pdf)

<a name="49"/>

## 49.Image Fusion(å›¾åƒèåˆ)
* [DDFM: Denoising Diffusion Model for Multi-Modality Image Fusion](http://arxiv.org/abs/2303.06840)
* [MEFLUT: Unsupervised 1D Lookup Tables for Multi-exposure Image Fusion](http://arxiv.org/abs/2309.11847)
* [Learned Image Reasoning Prior Penetrates Deep Unfolding Network for Panchromatic and Multi-Spectral Image Fusion](http://arxiv.org/abs/2308.16083v1)<br>:star:[code](https://manman1995.github.io/)
* [Degradation-Resistant Unfolding Network for Heterogeneous Image Fusion](https://openaccess.thecvf.com/content/ICCV2023/papers/He_Degradation-Resistant_Unfolding_Network_for_Heterogeneous_Image_Fusion_ICCV_2023_paper.pdf)
* [Learned Image Reasoning Prior Penetrates Deep Unfolding Network for Panchromatic and Multi-spectral Image Fusion](http://arxiv.org/abs/2308.16083)
* [UniFusion: Unified Multi-View Fusion Transformer for Spatial-Temporal Representation in Bird's-Eye-View](https://openaccess.thecvf.com/content/ICCV2023/papers/Qin_UniFusion_Unified_Multi-View_Fusion_Transformer_for_Spatial-Temporal_Representation_in_Birds-Eye-View_ICCV_2023_paper.pdf)

<a name="48"/>

## 48.Image Reconstruction
* [Pixel Adaptive Deep Unfolding Transformer for Hyperspectral Image Reconstruction](http://arxiv.org/abs/2308.10820v1)<br>:star:[code](https://github.com/MyuLi/PADUT)
* [RawHDR: High Dynamic Range Image Reconstruction from a Single Raw Image](http://arxiv.org/abs/2309.02020v1)
* [Learning Continuous Exposure Value Representations for Single-Image HDR Reconstruction](http://arxiv.org/abs/2309.03900v1)<br>:star:[code](https://skchen1993.github.io/CEVR_web/)

<a name="47"/>

## 47.Image-to-Image Translation
* [General Image-to-Image Translation with One-Shot Image Guidance](http://arxiv.org/abs/2307.14352v1)<br>:star:[code](https://github.com/CrystalNeuro/visual-concept-translator)
* [Scenimefy: Learning to Craft Anime Scene via Semi-Supervised Image-to-Image Translation](http://arxiv.org/abs/2308.12968v1)<br>:star:[code](https://github.com/Yuxinn-J/Scenimefy)<br>:star:[code](https://yuxinn-j.github.io/projects/Scenimefy.html)

<a name="46"/>

## 46.Edge Detection
* [Tiny and Efficient Model for the Edge Detection Generalization](http://arxiv.org/abs/2308.06468v1)<br>:star:[code](https://github.com/xavysp/TEED)

<a name="45"/>

## 45.Scene Graph Generation(åœºæ™¯å›¾åˆæˆ)
* [SGAligner: 3D Scene Alignment with Scene Graphs](http://arxiv.org/abs/2304.14880)
* [Environment-Invariant Curriculum Relation Learning for Fine-Grained Scene Graph Generation](http://arxiv.org/abs/2308.03282v1)
* [Compositional Feature Augmentation for Unbiased Scene Graph Generation](http://arxiv.org/abs/2308.06712v1)
* [Vision Relation Transformer for Unbiased Scene Graph Generation](http://arxiv.org/abs/2308.09472v1)
* [TextPSG: Panoptic Scene Graph Generation from Textual Descriptions](https://openaccess.thecvf.com/content/ICCV2023/papers/Zhao_TextPSG_Panoptic_Scene_Graph_Generation_from_Textual_Descriptions_ICCV_2023_paper.pdf)
* [HiLo: Exploiting High Low Frequency Relations for Unbiased Panoptic Scene Graph Generation](http://arxiv.org/abs/2303.15994)
* [Visually-Prompted Language Model for Fine-Grained Scene Graph Generation in an Open World](http://arxiv.org/abs/2303.13233)
* [Visual Traffic Knowledge Graph Generation from Scene Images](https://openaccess.thecvf.com/content/ICCV2023/papers/Guo_Visual_Traffic_Knowledge_Graph_Generation_from_Scene_Images_ICCV_2023_paper.pdf)

<a name="44"/>

## 44.Rendering(æ¸²æŸ“)
* [LiveHand: Real-time and Photorealistic Neural Hand Rendering](http://arxiv.org/abs/2302.07672)
* [NeMF: Inverse Volume Rendering with Neural Microflake Field](http://arxiv.org/abs/2304.00782)
* [ActorsNeRF: Animatable Few-shot Human Rendering with Generalizable NeRFs](http://arxiv.org/abs/2304.14401)
* [HollowNeRF: Pruning Hashgrid-Based NeRFs with Trainable Collision Mitigation](http://arxiv.org/abs/2308.10122v1)
* [DNA-Rendering: A Diverse Neural Actor Repository for High-Fidelity Human-centric Rendering](https://arxiv.org/abs/2307.10173)<br>:house:[project](https://dna-rendering.github.io/)
* [S3IM: Stochastic Structural SIMilarity and Its Unreasonable Effectiveness for Neural Fields](http://arxiv.org/abs/2308.07032v1)<br>:star:[code](https://github.com/Madaoer/S3IM)<br>:thumbsup:[ICCV 2023 | NeRF æç‚¹çš„ Magic Loss â€”â€” S3IM éšæœºç»“æ„ç›¸ä¼¼æ€§](https://mp.weixin.qq.com/s/w5IUykx6_-7lBE_2r_Onkg)
* [TransHuman: A Transformer-based Human Representation for Generalizable Neural Human Rendering](http://arxiv.org/abs/2307.12291v1)<br>:star:[code](https://pansanity666.github.io/TransHuman/)
* [Tri-MipRF: Tri-Mip Representation for Efficient Anti-Aliasing Neural Radiance Fields](http://arxiv.org/abs/2307.11335v1)<br>:star:[code](https://wbhu.github.io/projects/Tri-MipRF)
* [Rendering Humans from Object-Occluded Monocular Videos](http://arxiv.org/abs/2308.04622v1)<br>:house:[project](https://cs.stanford.edu/~xtiange/projects/occnerf/)
* [ScatterNeRF: Seeing Through Fog with Physically-Based Inverse Neural Rendering](http://arxiv.org/abs/2305.02103)
* [DNA-Rendering: A Diverse Neural Actor Repository for High-Fidelity Human-Centric Rendering](https://openaccess.thecvf.com/content/ICCV2023/papers/Cheng_DNA-Rendering_A_Diverse_Neural_Actor_Repository_for_High-Fidelity_Human-Centric_Rendering_ICCV_2023_paper.pdf)
* [Neural Microfacet Fields for Inverse Rendering](http://arxiv.org/abs/2303.17806)

<a name="43"/>

## 43.Neural Radiance Fields
* [Learning Neural Implicit Surfaces with Object-Aware Radiance Fields](https://openaccess.thecvf.com/content/ICCV2023/papers/Zhang_Learning_Neural_Implicit_Surfaces_with_Object-Aware_Radiance_Fields_ICCV_2023_paper.pdf)
* [ClimateNeRF: Extreme Weather Synthesis in Neural Radiance Field](http://arxiv.org/abs/2211.13226)
* [ReNeRF: Relightable Neural Radiance Fields with Nearfield Lighting](https://openaccess.thecvf.com/content/ICCV2023/papers/Xu_ReNeRF_Relightable_Neural_Radiance_Fields_with_Nearfield_Lighting_ICCV_2023_paper.pdf)
* [E2NeRF: Event Enhanced Neural Radiance Fields from Blurry Images](https://openaccess.thecvf.com/content/ICCV2023/papers/Qi_E2NeRF_Event_Enhanced_Neural_Radiance_Fields_from_Blurry_Images_ICCV_2023_paper.pdf)
* [Neural Fields for Structured Lighting](https://openaccess.thecvf.com/content/ICCV2023/papers/Shandilya_Neural_Fields_for_Structured_Lighting_ICCV_2023_paper.pdf)
* [NeRF-MS: Neural Radiance Fields with Multi-Sequence](https://openaccess.thecvf.com/content/ICCV2023/papers/Li_NeRF-MS_Neural_Radiance_Fields_with_Multi-Sequence_ICCV_2023_paper.pdf)
* [StegaNeRF: Embedding Invisible Information within Neural Radiance Fields](http://arxiv.org/abs/2212.01602)
* [DeformToon3D: Deformable Neural Radiance Fields for 3D Toonification](https://openaccess.thecvf.com/content/ICCV2023/papers/Zhang_DeformToon3D_Deformable_Neural_Radiance_Fields_for_3D_Toonification_ICCV_2023_paper.pdf)
* [Nerfbusters: Removing Ghostly Artifacts from Casually Captured NeRFs](http://arxiv.org/abs/2304.10532)
* [Tetra-NeRF: Representing Neural Radiance Fields Using Tetrahedra](https://openaccess.thecvf.com/content/ICCV2023/papers/Kulhanek_Tetra-NeRF_Representing_Neural_Radiance_Fields_Using_Tetrahedra_ICCV_2023_paper.pdf)
* [Zip-NeRF: Anti-Aliased Grid-Based Neural Radiance Fields](https://openaccess.thecvf.com/content/ICCV2023/papers/Barron_Zip-NeRF_Anti-Aliased_Grid-Based_Neural_Radiance_Fields_ICCV_2023_paper.pdf)
* [NeRFrac: Neural Radiance Fields through Refractive Surface](https://openaccess.thecvf.com/content/ICCV2023/papers/Zhan_NeRFrac_Neural_Radiance_Fields_through_Refractive_Surface_ICCV_2023_paper.pdf)
* [MonoNeRF: Learning a Generalizable Dynamic Radiance Field from Monocular Videos](http://arxiv.org/abs/2212.13056)
* [Reference-guided Controllable Inpainting of Neural Radiance Fields](http://arxiv.org/abs/2304.09677)
* [DeLiRa: Self-Supervised Depth, Light, and Radiance Fields](http://arxiv.org/abs/2304.02797)
* [Neural Radiance Field with LiDAR maps](https://openaccess.thecvf.com/content/ICCV2023/papers/Chang_Neural_Radiance_Field_with_LiDAR_maps_ICCV_2023_paper.pdf)
* [Dynamic Mesh-Aware Radiance Fields](http://arxiv.org/abs/2309.04581v1)
* [Locally Stylized Neural Radiance Fields](http://arxiv.org/abs/2309.10684v1)
* [Generalizable Neural Fields as Partially Observed Neural Processes](http://arxiv.org/abs/2309.06660v1)
* [DeformToon3D: Deformable 3D Toonification from Neural Radiance Fields](http://arxiv.org/abs/2309.04410v1)<br>:house:[project](https://www.mmlab-ntu.com/project/deformtoon3d/)<br>:star:[code](https://github.com/junzhezhang/DeformToon3D)
* [Robust e-NeRF: NeRF from Sparse & Noisy Events under Non-Uniform Motion](http://arxiv.org/abs/2309.08596v1)<br>:star:[code](https://wengflow.github.io/robust-e-nerf)
* [Pose-Free Neural Radiance Fields via Implicit Pose Regularization](http://arxiv.org/abs/2308.15049v1)
* [Canonical Factors for Hybrid Neural Fields](http://arxiv.org/abs/2308.15461v1)<br>:star:[code](https://brentyi.github.io/tilted/)
* [Multi-Modal Neural Radiance Field for Monocular Dense SLAM with a Light-Weight ToF Sensor](http://arxiv.org/abs/2308.14383v1)<br>:star:[code](https://zju3dv.github.io/tof_slam/)
* [Blending-NeRF: Text-Driven Localized Editing in Neural Radiance Fields](http://arxiv.org/abs/2308.11974v1)
* [Strata-NeRF : Neural Radiance Fields for Stratified Scenes](http://arxiv.org/abs/2308.10337v1)<br>:star:[code](https://ankitatiisc.github.io/Strata-NeRF/)
* [DReg-NeRF: Deep Registration for Neural Radiance Fields](http://arxiv.org/abs/2308.09386v1)<br>:star:[code](https://github.com/AIBluefisher/DReg-NeRF)
* [Seal-3D: Interactive Pixel-Level Editing for Neural Radiance Fields](http://arxiv.org/abs/2307.15131v1)<br>:house:[project](https://windingwind.github.io/seal-3d/)<br>:star:[code](https://github.com/windingwind/seal-3d/)
* [WaveNeRF: Wavelet-based Generalizable Neural Radiance Fields](http://arxiv.org/abs/2308.04826v1)
* [UrbanGIRAFFE: Representing Urban Scenes as Compositional Generative Neural Feature Fields](http://arxiv.org/abs/2303.14167)
* [LERF: Language Embedded Radiance Fields](http://arxiv.org/abs/2303.09553)
* ç‰ˆæƒä¿æŠ¤
  * [CopyRNeRF: Protecting the CopyRight of Neural Radiance Fields](http://arxiv.org/abs/2307.11526v1)

<a name="42"/>

## 42.Dataset/Benchmark
* æ•°æ®é›†
  * [Building3D: An Urban-Scale Dataset and Benchmarks for Learning Roof Structures from Point Clouds](https://arxiv.org/pdf/2307.11914.pdf)<br>:sunflower:[dataset](https://building3d.ucalgary.ca/#)<br>:thumbsup:[ICCV2023 é¦–ä¸ªåŸå¸‚çº§åˆ«çš„åŸºäºèˆªç©ºç‚¹äº‘çš„æˆ¿å±‹å»ºæ¨¡æ•°æ®é›† Building3D](https://mp.weixin.qq.com/s/gKFByZ8ud2aNlG7C7t2-2Q)
  * [Zenseact Open Dataset: A Large-Scale and Diverse Multimodal Dataset for Autonomous Driving](http://arxiv.org/abs/2305.02008)
  * [Lecture Presentations Multimodal Dataset: Towards Understanding Multimodality in Educational Videos](https://openaccess.thecvf.com/content/ICCV2023/papers/Lee_Lecture_Presentations_Multimodal_Dataset_Towards_Understanding_Multimodality_in_Educational_Videos_ICCV_2023_paper.pdf)
  * [RealGraph: A Multiview Dataset for 4D Real-world Context Graph Generation](https://openaccess.thecvf.com/content/ICCV2023/papers/Lin_RealGraph_A_Multiview_Dataset_for_4D_Real-world_Context_Graph_Generation_ICCV_2023_paper.pdf)
  * [Video Background Music Generation: Dataset, Method and Evaluation](http://arxiv.org/abs/2211.11248)
  * [Thinking Image Color Aesthetics Assessment: Models, Datasets and Benchmarks](https://openaccess.thecvf.com/content/ICCV2023/papers/He_Thinking_Image_Color_Aesthetics_Assessment_Models_Datasets_and_Benchmarks_ICCV_2023_paper.pdf)
  * [Snow Removal in Video: A New Dataset and A Novel Method](https://openaccess.thecvf.com/content/ICCV2023/papers/Chen_Snow_Removal_in_Video_A_New_Dataset_and_A_Novel_ICCV_2023_paper.pdf)
  * [SportsMOT: A Large Multi-Object Tracking Dataset in Multiple Sports Scenes](http://arxiv.org/abs/2304.05170)
  * [EmoSet: A Large-scale Visual Emotion Dataset with Rich Attributes](http://arxiv.org/abs/2307.07961)
  * [DetermiNet: A Large-Scale Diagnostic Dataset for Complex Visually-Grounded Referencing using Determiners](http://arxiv.org/abs/2309.03483)
  * [PointOdyssey: A Large-Scale Synthetic Dataset for Long-Term Point Tracking](http://arxiv.org/abs/2307.15055)
  * [SynBody: Synthetic Dataset with Layered Human Models for 3D Human Perception and Modeling](http://arxiv.org/abs/2303.17368)
  * [MOSE: A New Dataset for Video Object Segmentation in Complex Scenes](http://arxiv.org/abs/2302.01872)
  * [Audio-Visual Deception Detection: DOLOS Dataset and Parameter-Efficient Crossmodal Learning](http://arxiv.org/abs/2303.12745)
  * [3DMiner: Discovering Shapes from Large-Scale Unannotated Image Datasets](https://openaccess.thecvf.com/content/ICCV2023/papers/Cheng_3DMiner_Discovering_Shapes_from_Large-Scale_Unannotated_Image_Datasets_ICCV_2023_paper.pdf)
  * [MatrixCity: A Large-scale City Dataset for City-scale Neural Rendering and Beyond](https://openaccess.thecvf.com/content/ICCV2023/papers/Li_MatrixCity_A_Large-scale_City_Dataset_for_City-scale_Neural_Rendering_and_ICCV_2023_paper.pdf)
  * [LaRS: A Diverse Panoptic Maritime Obstacle Detection Dataset and Benchmark](http://arxiv.org/abs/2308.09618v1)<br>:star:[code](https://lojzezust.github.io/lars-dataset)
  * [EgoObjects: A Large-Scale Egocentric Dataset for Fine-Grained Object Understanding](http://arxiv.org/abs/2309.08816v1)<br>:star:[code](https://github.com/facebookresearch/EgoObjects)
  * [Towards Universal Image Embeddings: A Large-Scale Dataset and Challenge for Generic Image Representations](http://arxiv.org/abs/2309.01858v1)<br>:house:[project](https://cmp.felk.cvut.cz/univ_emb/)
  * [ScanNet++: A High-Fidelity Dataset of 3D Indoor Scenes](http://arxiv.org/abs/2308.11417v1)<br>:house:[project](https://youtu.be/E6P9e2r6M8I)<br>:star:[code](https://cy94.github.io/scannetpp/)
  * [Learning Optical Flow from Event Camera with Rendered Dataset](https://arxiv.org/abs/2303.11011)
  * [Efficient neural supersampling on a novel gaming dataset](http://arxiv.org/abs/2308.01483v1)
  * [AIDE: A Vision-Driven Multi-View, Multi-Modal, Multi-Tasking Dataset for Assistive Driving Perception](http://arxiv.org/abs/2307.13933v1)<br>:star:[code](https://github.com/ydk122024/AIDE)
  * [360VOT: A New Benchmark Dataset for Omnidirectional Visual Object Tracking](http://arxiv.org/abs/2307.14630v1)<br>:house:[project](https://360vot.hkustvgd.com)<br>:star:[code](https://github.com/HuajianUP/360VOT)å…¨å‘è§†è§‰ç›®æ ‡è·Ÿè¸ª
  * [Harvard Glaucoma Detection and Progression: A Multimodal Multitask Dataset and Generalization-Reinforced Semi-Supervised Learning](http://arxiv.org/abs/2308.13411v1)<br>:house:[project](https://ophai.hms.harvard.edu/datasets/harvard-gdp1000)
  * [FishNet: A Large-scale Dataset and Benchmark for Fish Recognition, Detection, and Functional Trait Prediction](https://openaccess.thecvf.com/content/ICCV2023/papers/Khan_FishNet_A_Large-scale_Dataset_and_Benchmark_for_Fish_Recognition_Detection_ICCV_2023_paper.pdf)
* åŸºå‡†
  * [From Sky to the Ground: A Large-scale Benchmark and Simple Baseline Towards Real Rain Removal](http://arxiv.org/abs/2308.03867v1)
  * [Beyond Object Recognition: A New Benchmark towards Object Concept Learning](http://arxiv.org/abs/2212.02710)
  * [ClothPose: A Real-world Benchmark for Visual Analysis of Garment Pose via An Indirect Recording Solution](https://openaccess.thecvf.com/content/ICCV2023/papers/Xu_ClothPose_A_Real-world_Benchmark_for_Visual_Analysis_of_Garment_Pose_ICCV_2023_paper.pdf)
  * [REAP: A Large-Scale Realistic Adversarial Patch Benchmark](http://arxiv.org/abs/2212.05680)
  * [Chaotic World: A Large and Challenging Benchmark for Human Behavior Understanding in Chaotic Events](https://openaccess.thecvf.com/content/ICCV2023/papers/Ong_Chaotic_World_A_Large_and_Challenging_Benchmark_for_Human_Behavior_ICCV_2023_paper.pdf)
  * [Breaking Common Sense: WHOOPS! A Vision-and-Language Benchmark of Synthetic and Compositional Images](http://arxiv.org/abs/2303.07274)
  * [FACET: Fairness in Computer Vision Evaluation Benchmark](http://arxiv.org/abs/2309.00035)
  * [A Benchmark for Chinese-English Scene Text Image Super-Resolution](http://arxiv.org/abs/2308.03262)
  * [Ego-Humans: An Ego-Centric 3D Multi-Human Benchmark](https://openaccess.thecvf.com/content/ICCV2023/papers/Khirodkar_Ego-Humans_An_Ego-Centric_3D_Multi-Human_Benchmark_ICCV_2023_paper.pdf)
  * [Towards Real-World Burst Image Super-Resolution: Benchmark and Method](http://arxiv.org/abs/2309.04803v1)<br>:star:[code](https://github.com/yjsunnn/FBANet)
  * [COCO-O: A Benchmark for Object Detectors under Natural Distribution Shifts](http://arxiv.org/abs/2307.12730v1)<br>:star:[code](https://github.com/alibaba/easyrobust/tree/main/benchmarks/coco_o)
  * [Dancing in the Dark: A Benchmark towards General Low-light Video Enhancement](https://openaccess.thecvf.com/content/ICCV2023/papers/Fu_Dancing_in_the_Dark_A_Benchmark_towards_General_Low-light_Video_ICCV_2023_paper.pdf)
  * [DiLiGenT-Pi: Photometric Stereo for Planar Surfaces with Rich Details - Benchmark Dataset and Beyond](https://openaccess.thecvf.com/content/ICCV2023/papers/Wang_DiLiGenT-Pi_Photometric_Stereo_for_Planar_Surfaces_with_Rich_Details_-_ICCV_2023_paper.pdf)
* æ–¹æ³•
  * [Prototype-based Dataset Comparison](http://arxiv.org/abs/2309.02401v1)<br>:star:[code](https://github.com/Nanne/ProtoSim)

<a name="41"/>

## 41.Vision Transformers
* [Scale-Aware Modulation Meet Transformer](http://arxiv.org/abs/2307.08579)
* [BiViT: Extremely Compressed Binary Vision Transformers](http://arxiv.org/abs/2211.07091)
* [Fcaformer: Forward Cross Attention in Hybrid Vision Transformer](http://arxiv.org/abs/2211.07198)
* [GET: Group Event Transformer for Event-Based Vision](https://openaccess.thecvf.com/content/ICCV2023/papers/Peng_GET_Group_Event_Transformer_for_Event-Based_Vision_ICCV_2023_paper.pdf)
* [DiffRate : Differentiable Compression Rate for Efficient Vision Transformers](http://arxiv.org/abs/2305.17997)
* [Scratching Visual Transformer's Back with Uniform Attention](https://openaccess.thecvf.com/content/ICCV2023/papers/Hyeon-Woo_Scratching_Visual_Transformers_Back_with_Uniform_Attention_ICCV_2023_paper.pdf)
* [Skill Transformer: A Monolithic Policy for Mobile Manipulation](http://arxiv.org/abs/2308.09873)
* [A Multidimensional Analysis of Social Biases in Vision Transformers](http://arxiv.org/abs/2308.01948)
* [Token-Label Alignment for Vision Transformers](http://arxiv.org/abs/2210.06455)
* [Building Vision Transformers with Hierarchy Aware Feature Aggregation](https://openaccess.thecvf.com/content/ICCV2023/papers/Chen_Building_Vision_Transformers_with_Hierarchy_Aware_Feature_Aggregation_ICCV_2023_paper.pdf)
* [TripLe: Revisiting Pretrained Model Reuse and Progressive Learning for Efficient Vision Transformer Scaling and Searching](https://openaccess.thecvf.com/content/ICCV2023/papers/Fu_TripLe_Revisiting_Pretrained_Model_Reuse_and_Progressive_Learning_for_Efficient_ICCV_2023_paper.pdf)
* [DarSwin: Distortion Aware Radial Swin Transformer](https://openaccess.thecvf.com/content/ICCV2023/papers/Athwale_DarSwin_Distortion_Aware_Radial_Swin_Transformer_ICCV_2023_paper.pdf)
* [Robustifying Token Attention for Vision Transformers](http://arxiv.org/abs/2303.11126)
* [FLatten Transformer: Vision Transformer using Focused Linear Attention](http://arxiv.org/abs/2308.00442)
* [Detection Transformer with Stable Matching](http://arxiv.org/abs/2304.04742)
* [LaPE: Layer-adaptive Position Embedding for Vision Transformers with Independent Layer Normalization](https://openaccess.thecvf.com/content/ICCV2023/papers/Yu_LaPE_Layer-adaptive_Position_Embedding_for_Vision_Transformers_with_Independent_Layer_ICCV_2023_paper.pdf)
* [M2T: Masking Transformers Twice for Faster Decoding](https://openaccess.thecvf.com/content/ICCV2023/papers/Mentzer_M2T_Masking_Transformers_Twice_for_Faster_Decoding_ICCV_2023_paper.pdf)
* [FDViT: Improve the Hierarchical Architecture of Vision Transformer](https://openaccess.thecvf.com/content/ICCV2023/papers/Xu_FDViT_Improve_the_Hierarchical_Architecture_of_Vision_Transformer_ICCV_2023_paper.pdf)
* [Jumping through Local Minima: Quantization in the Loss Landscape of Vision Transformers](http://arxiv.org/abs/2308.10814)
* [Rethinking Vision Transformers for MobileNet Size and Speed](http://arxiv.org/abs/2212.08059)
* [Structure Invariant Transformation for better Adversarial Transferability](http://arxiv.org/abs/2309.14700v1)<br>:star:[code](https://github.com/xiaosen-wang/SIT)
* [SG-Former: Self-guided Transformer with Evolving Token Reallocation](http://arxiv.org/abs/2308.12216v1)<br>:star:[code](https://github.com/OliverRensu/SG-Former)
* [Pre-training Vision Transformers with Very Limited Synthesized Images](http://arxiv.org/abs/2307.14710v1)
* [SMMix: Self-Motivated Image Mixing for Vision Transformers](https://arxiv.org/abs/2212.12977)
* [Revisiting Vision Transformer from the View of Path Ensemble](http://arxiv.org/abs/2308.06548v1)
* [SwinLSTM:Improving Spatiotemporal Prediction Accuracy using Swin Transformer and LSTM](http://arxiv.org/abs/2308.09891v1)<br>:star:[code](https://github.com/SongTang-x/SwinLSTM)
* [Eventful Transformers: Leveraging Temporal Redundancy in Vision Transformers](http://arxiv.org/abs/2308.13494v1)
* [Contrastive Feature Masking Open-Vocabulary Vision Transformer](http://arxiv.org/abs/2309.00775v1)
* [MMST-ViT: Climate Change-aware Crop Yield Prediction via Multi-Modal Spatial-Temporal Vision Transformer](http://arxiv.org/abs/2309.09067v1)
* [SAL-ViT: Towards Latency Efficient Private Inference on ViT using Selective Attention Search with a Learnable Softmax Approximation](https://openaccess.thecvf.com/content/ICCV2023/papers/Zhang_SAL-ViT_Towards_Latency_Efficient_Private_Inference_on_ViT_using_Selective_ICCV_2023_paper.pdf)
* [SwiftFormer: Efficient Additive Attention for Transformer-based Real-time Mobile Vision Applications](http://arxiv.org/abs/2303.15446)

<a name="40"/>

## 40.Anomaly Detection(å¼‚å¸¸æ£€æµ‹)
* [Unilaterally Aggregated Contrastive Learning with Hierarchical Augmentation for Anomaly Detection](http://arxiv.org/abs/2308.10155v1)
* [Unsupervised Surface Anomaly Detection with Diffusion Probabilistic Model](https://openaccess.thecvf.com/content/ICCV2023/papers/Zhang_Unsupervised_Surface_Anomaly_Detection_with_Diffusion_Probabilistic_Model_ICCV_2023_paper.pdf)
* [Anomaly Detection using Score-based Perturbation Resilience](https://openaccess.thecvf.com/content/ICCV2023/papers/Shin_Anomaly_Detection_using_Score-based_Perturbation_Resilience_ICCV_2023_paper.pdf)
* [Remembering Normality: Memory-guided Knowledge Distillation for Unsupervised Anomaly Detection](https://openaccess.thecvf.com/content/ICCV2023/papers/Gu_Remembering_Normality_Memory-guided_Knowledge_Distillation_for_Unsupervised_Anomaly_Detection_ICCV_2023_paper.pdf)
* [Template-guided Hierarchical Feature Restoration for Anomaly Detection](https://openaccess.thecvf.com/content/ICCV2023/papers/Guo_Template-guided_Hierarchical_Feature_Restoration_for_Anomaly_Detection_ICCV_2023_paper.pdf)
* å›¾åƒå¼‚å¸¸æ£€æµ‹
  * [Focus the Discrepancy: Intra- and Inter-Correlation Learning for Image Anomaly Detection](http://arxiv.org/abs/2308.02983v1)<br>:star:[code](https://github.com/xcyao00/FOD)
* OOD
  * [CLIPN for Zero-Shot OOD Detection: Teaching CLIP to Say No](http://arxiv.org/abs/2308.12213v1)<br>:star:[code](https://github.com/xmed-lab/CLIPN)
  * [Meta OOD Learning for Continuously Adaptive OOD Detection](http://arxiv.org/abs/2309.11705v1)
  * [Simple and Effective Out-of-Distribution Detection via Cosine-based Softmax Loss](https://openaccess.thecvf.com/content/ICCV2023/papers/Noh_Simple_and_Effective_Out-of-Distribution_Detection_via_Cosine-based_Softmax_Loss_ICCV_2023_paper.pdf)
  * [Nearest Neighbor Guidance for Out-of-Distribution Detection](http://arxiv.org/abs/2309.14888v1)<br>:star:[code](https://github.com/roomo7time/nnguide)
  * [Understanding the Feature Norm for Out-of-Distribution Detection](https://openaccess.thecvf.com/content/ICCV2023/papers/Park_Understanding_the_Feature_Norm_for_Out-of-Distribution_Detection_ICCV_2023_paper.pdf)
  * [Meta OOD Learning For Continuously Adaptive OOD Detection](http://arxiv.org/abs/2309.11705)
  * [SAFE: Sensitivity-Aware Features for Out-of-Distribution Object Detection](http://arxiv.org/abs/2208.13930)
  * [Unified Out-Of-Distribution Detection: A Model-Specific Perspective](http://arxiv.org/abs/2304.06813)
  * [Revisit PCA-based Technique for Out-of-Distribution Detection](https://openaccess.thecvf.com/content/ICCV2023/papers/Guan_Revisit_PCA-based_Technique_for_Out-of-Distribution_Detection_ICCV_2023_paper.pdf)
  * [Hierarchical Visual Categories Modeling: A Joint Representation Learning and Density Estimation Framework for Out-of-Distribution Detection](https://openaccess.thecvf.com/content/ICCV2023/papers/Li_Hierarchical_Visual_Categories_Modeling_A_Joint_Representation_Learning_and_Density_ICCV_2023_paper.pdf)

<a name="39"/>

## 39.Keypoint Detection(å…³é”®ç‚¹æ£€æµ‹)
* [Neural Interactive Keypoint Detection](http://arxiv.org/abs/2308.10174v1)<br>:star:[code](https://github.com/IDEA-Research/Click-Pose)
* [3D Implicit Transporter for Temporally Consistent Keypoint Discovery](http://arxiv.org/abs/2309.05098v1)<br>:star:[code](https://github.com/zhongcl-thu/3D-Implicit-Transporter)

<a name="38"/>

## 38.Vision-Language(è§†è§‰è¯­è¨€)
* [Gloss-Free Sign Language Translation: Improving from Visual-Language Pretraining](https://openaccess.thecvf.com/content/ICCV2023/papers/Zhou_Gloss-Free_Sign_Language_Translation_Improving_from_Visual-Language_Pretraining_ICCV_2023_paper.pdf)
* [SuS-X: Training-Free Name-Only Transfer of Vision-Language Models](https://openaccess.thecvf.com/content/ICCV2023/papers/Udandarao_SuS-X_Training-Free_Name-Only_Transfer_of_Vision-Language_Models_ICCV_2023_paper.pdf)
* [Bayesian Prompt Learning for Image-Language Model Generalization](http://arxiv.org/abs/2210.02390)
* [eP-ALM: Efficient Perceptual Augmentation of Language Models](https://openaccess.thecvf.com/content/ICCV2023/papers/Shukor_eP-ALM_Efficient_Perceptual_Augmentation_of_Language_Models_ICCV_2023_paper.pdf)
* [Task-Oriented Multi-Modal Mutual Leaning for Vision-Language Models](http://arxiv.org/abs/2303.17169)
* [SLAN: Self-Locator Aided Network for Vision-Language Understanding](https://openaccess.thecvf.com/content/ICCV2023/papers/Zhai_SLAN_Self-Locator_Aided_Network_for_Vision-Language_Understanding_ICCV_2023_paper.pdf)
* [VL-Match: Enhancing Vision-Language Pretraining with Token-Level and Instance-Level Matching](https://openaccess.thecvf.com/content/ICCV2023/papers/Bi_VL-Match_Enhancing_Vision-Language_Pretraining_with_Token-Level_and_Instance-Level_Matching_ICCV_2023_paper.pdf)
* [A Retrospect to Multi-prompt Learning across Vision and Language](https://openaccess.thecvf.com/content/ICCV2023/papers/Chen_A_Retrospect_to_Multi-prompt_Learning_across_Vision_and_Language_ICCV_2023_paper.pdf)
* [CiT: Curation in Training for Effective Vision-Language Data](http://arxiv.org/abs/2301.02241)
* [EgoTV: Egocentric Task Verification from Natural Language Task Descriptions](http://arxiv.org/abs/2303.16975)
* [Gradient-Regulated Meta-Prompt Learning for Generalizable Vision-Language Models](http://arxiv.org/abs/2303.06571)
* [Towards Unifying Medical Vision-and-Language Pre-Training via Soft Prompts](http://arxiv.org/abs/2302.08958)
* [Preventing Zero-Shot Transfer Degradation in Continual Learning of Vision-Language Models](http://arxiv.org/abs/2303.06628)
* [Perceptual Grouping in Contrastive Vision-Language Models](http://arxiv.org/abs/2210.09996)
* [Black Box Few-Shot Adaptation for Vision-Language Models](https://openaccess.thecvf.com/content/ICCV2023/papers/Ouali_Black_Box_Few-Shot_Adaptation_for_Vision-Language_Models_ICCV_2023_paper.pdf)
* [CTP:Towards Vision-Language Continual Pretraining via Compatible Momentum Contrast and Topology Preservation](http://arxiv.org/abs/2308.07146)
* [VL-PET: Vision-and-Language Parameter-Efficient Tuning via Granularity Control](https://openaccess.thecvf.com/content/ICCV2023/papers/Hu_VL-PET_Vision-and-Language_Parameter-Efficient_Tuning_via_Granularity_Control_ICCV_2023_paper.pdf)
* [GrowCLIP: Data-Aware Automatic Model Growing for Large-scale Contrastive Language-Image Pre-Training](http://arxiv.org/abs/2308.11331)
* [I Can't Believe There's No Images! Learning Visual Tasks Using only Language Supervision](https://openaccess.thecvf.com/content/ICCV2023/papers/Gu_I_Cant_Believe_Theres_No_Images_Learning_Visual_Tasks_Using_ICCV_2023_paper.pdf)
* [Too Large; Data Reduction for Vision-Language Pre-Training](http://arxiv.org/abs/2305.20087)
* [Equivariant Similarity for Vision-Language Foundation Models](http://arxiv.org/abs/2303.14465)
* [Going Beyond Nouns With Vision & Language Models Using Synthetic Data](http://arxiv.org/abs/2303.17590)
* [SINC: Self-Supervised In-Context Learning for Vision-Language Tasks](http://arxiv.org/abs/2307.07742)
* [Unified Visual Relationship Detection with Vision and Language Models](http://arxiv.org/abs/2303.08998)
* [ProbVLM: Probabilistic Adapter for Frozen Vison-Language Models](http://arxiv.org/abs/2307.00398)
* [Distilling Large Vision-Language Model with Out-of-Distribution Generalizability](http://arxiv.org/abs/2307.03135)
* [Distribution-Aware Prompt Tuning for Vision-Language Models](http://arxiv.org/abs/2309.03406v1)<br>:star:[code](https://github.com/mlvlab/DAPT)
* [LoGoPrompt: Synthetic Text Images Can Be Good Visual Prompts for Vision-Language Models](http://arxiv.org/abs/2309.01155v1)<br>:star:[code](https://chengshiest.github.io/logo)
* [CLIPTrans: Transferring Visual Knowledge with Pre-trained Models for Multimodal Machine Translation](http://arxiv.org/abs/2308.15226v1)
* [GrowCLIP: Data-aware Automatic Model Growing for Large-scale Contrastive Language-Image Pre-training](http://arxiv.org/abs/2308.11331v1)
* [RLIPv2: Fast Scaling of Relational Language-Image Pre-training](http://arxiv.org/abs/2308.09351v1)<br>:star:[code](https://github.com/JacobYuan7/RLIPv2)
* [Regularized Mask Tuning: Uncovering Hidden Knowledge in Pre-trained Vision-Language Models](http://arxiv.org/abs/2307.15049v1)<br>:star:[code](https://wuw2019.github.io/RMT/)
* [Why Is Prompt Tuning for Vision-Language Models Robust to Noisy Labels?](http://arxiv.org/abs/2307.11978v1)<br>:star:[code](https://github.com/CEWu/PTNL)
* [Set-level Guidance Attack: Boosting Adversarial Transferability of Vision-Language Pre-training Models](http://arxiv.org/abs/2307.14061v1)<br>:thumbsup:[ICCV 2023 Oral | å—ç§‘å¤§VIP Lab | é’ˆå¯¹VLPæ¨¡å‹çš„é›†åˆçº§å¼•å¯¼æ”»å‡»](https://mp.weixin.qq.com/s/bE97oBoa4nH1c5XuOz4WWA)
* [CTP: Towards Vision-Language Continual Pretraining via Compatible Momentum Contrast and Topology Preservation](http://arxiv.org/abs/2308.07146v1)<br>:star:[code](https://github.com/KevinLight831/CTP)
* [VL-PET: Vision-and-Language Parameter-Efficient Tuning via Granularity Control](http://arxiv.org/abs/2308.09804v1)<br>:star:[code](https://github.com/HenryHZY/VL-PET)
* [Knowledge-Aware Prompt Tuning for Generalizable Vision-Language Models](http://arxiv.org/abs/2308.11186v1)
* [VLSlice: Interactive Vision-and-Language Slice Discovery](http://arxiv.org/abs/2309.06703v1)<br>:house:[project](https://ericslyman.com/vlslice/)
* [What does CLIP know about a red circle? Visual prompt engineering for VLMs](http://arxiv.org/abs/2304.06712)
* è§†è§‰è¡¨ç¤ºå­¦ä¹ 
  * [Hallucination Improves the Performance of Unsupervised Visual Representation Learning](http://arxiv.org/abs/2307.12168v1)
  * [ViLLA: Fine-Grained Vision-Language Representation Learning from Real-World Data](http://arxiv.org/abs/2308.11194v1)
* VLN
  * [Learning Vision-and-Language Navigation from YouTube Videos](http://arxiv.org/abs/2307.11984v1)<br>:star:[code](https://github.com/JeremyLinky/YouTube-VLN)
  * [Learning Navigational Visual Representations with Semantic Map Supervision](http://arxiv.org/abs/2307.12335)
  * [Grounded Entity-Landmark Adaptive Pre-Training for Vision-and-Language Navigation](http://arxiv.org/abs/2308.12587)
  * [GridMM: Grid Memory Map for Vision-and-Language Navigation](http://arxiv.org/abs/2307.12907v1)
  * [Scaling Data Generation in Vision-and-Language Navigation](http://arxiv.org/abs/2307.15644v1)
  * [Bird's-Eye-View Scene Graph for Vision-Language Navigation](http://arxiv.org/abs/2308.04758v1)<br>:star:[code](https://github.com/DefaultRui/BEV-Scene-Graph)
  * [AerialVLN: Vision-and-Language Navigation for UAVs](http://arxiv.org/abs/2308.06735v1)<br>:star:[code](https://github.com/AirVLN/AirVLN)
  * [DREAMWALKER: Mental Planning for Continuous Vision-Language Navigation](http://arxiv.org/abs/2308.07498v1)<br>:star:[code](https://github.com/hanqingwangai/Dreamwalker)
  * [VLN-PETL: Parameter-Efficient Transfer Learning for Vision-and-Language Navigation](http://arxiv.org/abs/2308.10172v1)
  * [March in Chat: Interactive Prompting for Remote Embodied Referring Expression](http://arxiv.org/abs/2308.10141v1)
  * [Grounded Entity-Landmark Adaptive Pre-training for Vision-and-Language Navigation](http://arxiv.org/abs/2308.12587v1)
  * [BEVBert: Multimodal Map Pre-training for Language-guided Navigation](https://arxiv.org/pdf/2212.04385.pdf)<br>:star:[code](https://github.com/MarSaKi/VLN-BEVBert)
* Visual Grounding(è§†è§‰å®šä½)
  * [ViewRefer: Grasp the Multi-view Knowledge for 3D Visual Grounding](http://arxiv.org/abs/2303.16894)
  * [Distilling Coarse-to-Fine Semantic Matching Knowledge for Weakly Supervised 3D Visual Grounding](http://arxiv.org/abs/2307.09267)
  * [Confidence-aware Pseudo-label Learning for Weakly Supervised Visual Grounding](https://openaccess.thecvf.com/content/ICCV2023/papers/Liu_Confidence-aware_Pseudo-label_Learning_for_Weakly_Supervised_Visual_Grounding_ICCV_2023_paper.pdf)
* Video-Language
  * [HiTeA: Hierarchical Temporal-Aware Video-Language Pre-training](http://arxiv.org/abs/2212.14546) 
  * [HiVLP: Hierarchical Interactive Video-Language Pre-Training](https://openaccess.thecvf.com/content/ICCV2023/papers/Shao_HiVLP_Hierarchical_Interactive_Video-Language_Pre-Training_ICCV_2023_paper.pdf)
  * [Verbs in Action: Improving Verb Understanding in Video-Language Models](http://arxiv.org/abs/2304.06708)
  * [Exploring Temporal Concurrency for Video-Language Representation Learning](https://openaccess.thecvf.com/content/ICCV2023/papers/Zhang_Exploring_Temporal_Concurrency_for_Video-Language_Representation_Learning_ICCV_2023_paper.pdf)
  * [EgoVLPv2: Egocentric Video-Language Pre-training with Fusion in the Backbone](http://arxiv.org/abs/2307.05463)
  * [SMAUG: Sparse Masked Autoencoder for Efficient Video-Language Pre-Training](http://arxiv.org/abs/2211.11446)
 

<a name="37"/>

## 37.Object Pose Estimation(ç‰©ä½“å§¿åŠ¿ä¼°è®¡)
* [IST-Net: Prior-Free Category-Level Pose Estimation with Implicit Space Transformation](https://openaccess.thecvf.com/content/ICCV2023/papers/Liu_IST-Net_Prior-Free_Category-Level_Pose_Estimation_with_Implicit_Space_Transformation_ICCV_2023_paper.pdf)
* [LU-NeRF: Scene and Pose Estimation by Synchronizing Local Unposed NeRFs](https://openaccess.thecvf.com/content/ICCV2023/papers/Cheng_LU-NeRF_Scene_and_Pose_Estimation_by_Synchronizing_Local_Unposed_NeRFs_ICCV_2023_paper.pdf)
* [PoseDiffusion: Solving Pose Estimation via Diffusion-aided Bundle Adjustment](http://arxiv.org/abs/2306.15667)
* [Nonrigid Object Contact Estimation With Regional Unwrapping Transformer](http://arxiv.org/abs/2308.14074v1)
* 6D
  * [Deep Fusion Transformer Network with Weighted Vector-Wise Keypoints Voting for Robust 6D Object Pose Estimation](http://arxiv.org/abs/2308.05438v1)
  * [Pseudo Flow Consistency for Self-Supervised 6D Object Pose Estimation](http://arxiv.org/abs/2308.10016v1)
  * [Center-Based Decoupled Point-cloud Registration for 6D Object Pose Estimation](https://openaccess.thecvf.com/content/ICCV2023/papers/Jiang_Center-Based_Decoupled_Point-cloud_Registration_for_6D_Object_Pose_Estimation_ICCV_2023_paper.pdf)
  * [Query6DoF: Learning Sparse Queries as Implicit Shape Prior for Category-Level 6DoF Pose Estimation](https://openaccess.thecvf.com/content/ICCV2023/papers/Wang_Query6DoF_Learning_Sparse_Queries_as_Implicit_Shape_Prior_for_Category-Level_ICCV_2023_paper.pdf)
  * [VI-Net: Boosting Category-level 6D Object Pose Estimation via Learning Decoupled Rotations on the Spherical Representations](http://arxiv.org/abs/2308.09916v1)<br>:star:[code](https://github.com/JiehongLin/VI-Net)
  * [Learning Symmetry-Aware Geometry Correspondences for 6D Object Pose Estimation](https://openaccess.thecvf.com/content/ICCV2023/papers/Zhao_Learning_Symmetry-Aware_Geometry_Correspondences_for_6D_Object_Pose_Estimation_ICCV_2023_paper.pdf)
  * [3D Neural Embedding Likelihood: Probabilistic Inverse Graphics for Robust 6D Pose Estimation](http://arxiv.org/abs/2302.03744)
* ç‰©ä½“è®¡æ•°
  * [STEERER: Resolving Scale Variations for Counting and Localization via Selective Inheritance Learning](http://arxiv.org/abs/2308.10468v1)<br>:star:[code](https://github.com/taohan10200/STEERER)
  * [Interactive Class-Agnostic Object Counting](http://arxiv.org/abs/2309.05277)
  * [A Low-Shot Object Counting Network With Iterative Prototype Adaptation](https://openaccess.thecvf.com/content/ICCV2023/papers/Dukic_A_Low-Shot_Object_Counting_Network_With_Iterative_Prototype_Adaptation_ICCV_2023_paper.pdf)
* åŠ¨ç‰©å§¿åŠ¿ä¼°è®¡
  * [Animal3D: A Comprehensive Dataset of 3D Animal Pose and Shape](http://arxiv.org/abs/2308.11737)

<a name="36"/>

## 36.Vision Question Answering(è§†è§‰é—®ç­”)
* [Toward Unsupervised Realistic Visual Question Answering](http://arxiv.org/abs/2303.05068)
* [Variational Causal Inference Network for Explanatory Visual Question Answering](https://openaccess.thecvf.com/content/ICCV2023/papers/Xue_Variational_Causal_Inference_Network_for_Explanatory_Visual_Question_Answering_ICCV_2023_paper.pdf)
* [TIFA: Accurate and Interpretable Text-to-Image Faithfulness Evaluation with Question Answering](http://arxiv.org/abs/2303.11897)
* [Encyclopedic VQA: Visual Questions About Detailed Properties of Fine-Grained Categories](http://arxiv.org/abs/2306.09224)
* [PromptCap: Prompt-Guided Image Captioning for VQA with GPT-3](https://openaccess.thecvf.com/content/ICCV2023/papers/Hu_PromptCap_Prompt-Guided_Image_Captioning_for_VQA_with_GPT-3_ICCV_2023_paper.pdf)
* [Decouple Before Interact: Multi-Modal Prompt Learning for Continual Visual Question Answering](https://openaccess.thecvf.com/content/ICCV2023/papers/Qian_Decouple_Before_Interact_Multi-Modal_Prompt_Learning_for_Continual_Visual_Question_ICCV_2023_paper.pdf)
* Video-QA
  * [Discovering Spatio-Temporal Rationales for Video Question Answering](http://arxiv.org/abs/2307.12058v1)<br>:star:[code](https://github.com/yl3800/TranSTR)
  * [Knowledge Proxy Intervention for Deconfounded Video Question Answering](https://openaccess.thecvf.com/content/ICCV2023/papers/Li_Knowledge_Proxy_Intervention_for_Deconfounded_Video_Question_Answering_ICCV_2023_paper.pdf)
  * [Tem-adapter: Adapting Image-Text Pretraining for Video Question Answer](http://arxiv.org/abs/2308.08414v1)
  * [Open-vocabulary Video Question Answering: A New Benchmark for Evaluating the Generalizability of Video Question Answering Models](http://arxiv.org/abs/2308.09363v1)<br>:star:[code](https://github.com/mlvlab/OVQA)
  * [Tem-Adapter: Adapting Image-Text Pretraining for Video Question Answer](https://openaccess.thecvf.com/content/ICCV2023/papers/Chen_Tem-Adapter_Adapting_Image-Text_Pretraining_for_Video_Question_Answer_ICCV_2023_paper.pdf)

<a name="35"/>

## 35.Human Motion Prediction(äººä½“è¿åŠ¨é¢„æµ‹)
* [Auxiliary Tasks Benefit 3D Skeleton-based Human Motion Prediction](http://arxiv.org/abs/2308.08942v1)<br>:star:[code](https://github.com/MediaBrain-SJTU/AuxFormer)
* [Forecast-MAE: Self-supervised Pre-training for Motion Forecasting with Masked Autoencoders](http://arxiv.org/abs/2308.09882v1)<br>:star:[code](https://github.com/jchengai/forecast-mae)
* [Priority-Centric Human Motion Generation in Discrete Latent Space](http://arxiv.org/abs/2308.14480v1)
* [MotionLM: Multi-Agent Motion Forecasting as Language Modeling](https://openaccess.thecvf.com/content/ICCV2023/papers/Seff_MotionLM_Multi-Agent_Motion_Forecasting_as_Language_Modeling_ICCV_2023_paper.pdf)
* [HumanMAC: Masked Motion Completion for Human Motion Prediction](http://arxiv.org/abs/2302.03665)
* [Joint-Relation Transformer for Multi-Person Motion Prediction](http://arxiv.org/abs/2308.04808)
* [Bootstrap Motion Forecasting With Self-Consistent Constraints](http://arxiv.org/abs/2204.05859)
* [PhysDiff: Physics-Guided Human Motion Diffusion Model](http://arxiv.org/abs/2212.02500)
* [AttT2M: Text-Driven Human Motion Generation with Multi-Perspective Attention Mechanism](http://arxiv.org/abs/2309.00796)

<a name="34"/>

## 34.Action Detection(åŠ¨ä½œè¯†åˆ«)
* [Multimodal Distillation for Egocentric Action Recognition](http://arxiv.org/abs/2307.07483)
* [Memory-and-Anticipation Transformer for Online Action Understanding](http://arxiv.org/abs/2308.07893v1)<br>:star:[code](https://github.com/Echo0125/Memory-and-Anticipation-Transformer)
* [Masked Motion Predictors are Strong 3D Action Representation Learners](http://arxiv.org/abs/2308.07092v1)<br>:star:[code](https://github.com/maoyunyao/MAMP)
* [Efficient Video Action Detection with Token Dropout and Context Refinement](http://arxiv.org/abs/2304.08451)
* [E2E-LOAD: End-to-End Long-form Online Action Detection](https://openaccess.thecvf.com/content/ICCV2023/papers/Cao_E2E-LOAD_End-to-End_Long-form_Online_Action_Detection_ICCV_2023_paper.pdf)
* [Ego-Only: Egocentric Action Detection without Exocentric Transferring](https://openaccess.thecvf.com/content/ICCV2023/papers/Wang_Ego-Only_Egocentric_Action_Detection_without_Exocentric_Transferring_ICCV_2023_paper.pdf)
* [Cross-Modal Learning with 3D Deformable Attention for Action Recognition](http://arxiv.org/abs/2212.05638)
* [DiffTAD: Temporal Action Detection with Proposal Denoising Diffusion](http://arxiv.org/abs/2303.14863)
* [STPrivacy: Spatio-Temporal Privacy-Preserving Action Recognition](http://arxiv.org/abs/2301.03046)
* [MiniROAD: Minimal RNN Framework for Online Action Detection](https://openaccess.thecvf.com/content/ICCV2023/papers/An_MiniROAD_Minimal_RNN_Framework_for_Online_Action_Detection_ICCV_2023_paper.pdf)
* [Video Action Recognition with Attentive Semantic Units](http://arxiv.org/abs/2303.09756)
* [A Large-scale Study of Spatiotemporal Representation Learning with a New Benchmark on Action Recognition](http://arxiv.org/abs/2303.13505)
* [What Can a Cook in Italy Teach a Mechanic in India? Action Recognition Generalisation Over Scenarios and Locations](http://arxiv.org/abs/2306.08713)
* åŸºäºéª¨æ¶çš„åŠ¨ä½œè¯†åˆ«
  * [LAC -- Latent Action Composition for Skeleton-based Action Segmentation](http://arxiv.org/abs/2308.14500v1)
  * [Generative Action Description Prompts for Skeleton-based Action Recognition](http://arxiv.org/abs/2208.05318)
  * [Parallel Attention Interaction Network for Few-Shot Skeleton-Based Action Recognition](https://openaccess.thecvf.com/content/ICCV2023/papers/Liu_Parallel_Attention_Interaction_Network_for_Few-Shot_Skeleton-Based_Action_Recognition_ICCV_2023_paper.pdf)
  * [Leveraging Spatio-Temporal Dependency for Skeleton-Based Action Recognition](http://arxiv.org/abs/2212.04761)
  * [Hierarchically Decomposed Graph Convolutional Networks for Skeleton-Based Action Recognition](http://arxiv.org/abs/2208.10741)
  * [Modeling the Relative Visual Tempo for Self-supervised Skeleton-based Action Recognition](https://openaccess.thecvf.com/content/ICCV2023/papers/Zhu_Modeling_the_Relative_Visual_Tempo_for_Self-supervised_Skeleton-based_Action_Recognition_ICCV_2023_paper.pdf)
  * [SkeleTR: Towrads Skeleton-based Action Recognition in the Wild](http://arxiv.org/abs/2309.11445v1)
  * [Hard No-Box Adversarial Attack on Skeleton-Based Human Action Recognition with Skeleton-Motion-Informed Gradient](http://arxiv.org/abs/2308.05681)
  * [FSAR: Federated Skeleton-based Action Recognition with Adaptive Topology Structure and Knowledge Distillation](http://arxiv.org/abs/2306.11046)
* å¼€é›†åŠ¨ä½œè¯†åˆ«
  * [SOAR: Scene-debiasing Open-set Action Recognition](http://arxiv.org/abs/2309.01265v1)<br>:star:[code](https://github.com/yhZhai/SOAR)
* é›¶æ ·æœ¬åŠ¨ä½œè¯†åˆ«
  * [MAtch, eXpand and Improve: Unsupervised Finetuning for Zero-Shot Action Recognition with Language Knowledge](http://arxiv.org/abs/2303.08914)
* å°æ ·æœ¬åŠ¨ä½œè¯†åˆ«
  * [Boosting Few-shot Action Recognition with Graph-guided Hybrid Matching](http://arxiv.org/abs/2308.09346v1)<br>:star:[code](https://github.com/jiazheng-xing/GgHM)
* æ—¶åºåŠ¨ä½œå®šä½
  * [DDG-Net: Discriminability-Driven Graph Network for Weakly-supervised Temporal Action Localization](http://arxiv.org/abs/2307.16415v1)<br>:star:[code](https://github.com/XiaojunTang22/ICCV2023-DDGNet)
  * [Self-Feedback DETR for Temporal Action Detection](http://arxiv.org/abs/2308.10570v1)
  * [Action Sensitivity Learning for Temporal Action Localization](http://arxiv.org/abs/2305.15701)
  * [Revisiting Foreground and Background Separation in Weakly-supervised Temporal Action Localization](https://openaccess.thecvf.com/content/ICCV2023/papers/Liu_Revisiting_Foreground_and_Background_Separation_in_Weakly-supervised_Temporal_Action_Localization_ICCV_2023_paper.pdf)
* å¼±ç›‘ç£åŠ¨ä½œå®šä½
  * [Weakly-Supervised Action Localization by Hierarchically-structured Latent Attention Modeling](http://arxiv.org/abs/2308.09946v1)
* å°æ ·æœ¬åŠ¨ä½œå®šä½
  * [Few-Shot Common Action Localization via Cross-Attentional Fusion of Context and Temporal Dynamics](https://openaccess.thecvf.com/content/ICCV2023/papers/Lee_Few-Shot_Common_Action_Localization_via_Cross-Attentional_Fusion_of_Context_and_ICCV_2023_paper.pdf)
* åŠ¨ä½œç†è§£
  * [Memory-and-Anticipation Transformer for Online Action Understanding](http://arxiv.org/abs/2308.07893)

<a name="33"/>

## 33.Video(è§†é¢‘)
* [Neural Video Depth Stabilizer](http://arxiv.org/abs/2307.08695)
* [Localizing Moments in Long Video Via Multimodal Guidance](http://arxiv.org/abs/2302.13372)
* [Order-Prompted Tag Sequence Generation for Video Tagging](https://openaccess.thecvf.com/content/ICCV2023/papers/Ma_Order-Prompted_Tag_Sequence_Generation_for_Video_Tagging_ICCV_2023_paper.pdf)
* [Moment Detection in Long Tutorial Videos](https://openaccess.thecvf.com/content/ICCV2023/papers/Croitoru_Moment_Detection_in_Long_Tutorial_Videos_ICCV_2023_paper.pdf)
* [MMVP: Motion-Matrix-based Video Prediction](http://arxiv.org/abs/2308.16154v1)<br>:star:[code](https://github.com/Kay1794/MMVP-motion-matrix-based-video-prediction)
* [D3G: Exploring Gaussian Prior for Temporal Sentence Grounding with Glance Annotation](http://arxiv.org/abs/2308.04197v1)<br>:star:[code](https://github.com/solicucu/D3G)
* [LAN-HDR: Luminance-based Alignment Network for High Dynamic Range Video Reconstruction](http://arxiv.org/abs/2308.11116v1)
* [TALL: Thumbnail Layout for Deepfake Video Detection](http://arxiv.org/abs/2307.07494)
* [Spatio-temporal Prompting Network for Robust Video Feature Extraction](https://openaccess.thecvf.com/content/ICCV2023/papers/Sun_Spatio-temporal_Prompting_Network_for_Robust_Video_Feature_Extraction_ICCV_2023_paper.pdf)
* [Neural Reconstruction of Relightable Human Model from Monocular Video](https://openaccess.thecvf.com/content/ICCV2023/papers/Sun_Neural_Reconstruction_of_Relightable_Human_Model_from_Monocular_Video_ICCV_2023_paper.pdf)
* è§†é¢‘ç†è§£
  * [Long-range Multimodal Pretraining for Movie Understanding](http://arxiv.org/abs/2308.09775v1)
  * [RefEgo: Referring Expression Comprehension Dataset from First-Person Perception of Ego4D](http://arxiv.org/abs/2308.12035v1)
  * [UniFormerV2: Unlocking the Potential of Image ViTs for Video Understanding](https://openaccess.thecvf.com/content/ICCV2023/papers/Li_UniFormerV2_Unlocking_the_Potential_of_Image_ViTs_for_Video_Understanding_ICCV_2023_paper.pdf)
* è§†é¢‘åˆ†ç±»
  * [ReGen: A good Generative Zero-Shot Video Classifier Should be Rewarded](https://openaccess.thecvf.com/content/ICCV2023/papers/Bulat_ReGen_A_good_Generative_Zero-Shot_Video_Classifier_Should_be_Rewarded_ICCV_2023_paper.pdf)
  * [Gram-based Attentive Neural Ordinary Differential Equations Network for Video Nystagmography Classification](https://openaccess.thecvf.com/content/ICCV2023/papers/Qiu_Gram-based_Attentive_Neural_Ordinary_Differential_Equations_Network_for_Video_Nystagmography_ICCV_2023_paper.pdf)
  * [Few-Shot Video Classification via Representation Fusion and Promotion Learning](https://openaccess.thecvf.com/content/ICCV2023/papers/Xia_Few-Shot_Video_Classification_via_Representation_Fusion_and_Promotion_Learning_ICCV_2023_paper.pdf)
* è§†é¢‘åˆæˆ
  * [StyleInV: A Temporal Style Modulated Inversion Network for Unconditional Video Generation](http://arxiv.org/abs/2308.16909v1)<br>:house:[project](https://www.mmlab-ntu.com/project/styleinv/index.html)<br>:star:[code](https://github.com/johannwyh/StyleInV)
  * [Mixed Neural Voxels for Fast Multi-view Video Synthesis](http://arxiv.org/abs/2212.00190)
  * [WALDO: Future Video Synthesis Using Object Layer Decomposition and Parametric Flow Prediction](http://arxiv.org/abs/2211.14308)
  * [DreamPose: Fashion Video Synthesis with Stable Diffusion](https://openaccess.thecvf.com/content/ICCV2023/papers/Karras_DreamPose_Fashion_Video_Synthesis_with_Stable_Diffusion_ICCV_2023_paper.pdf)
* è§†é¢‘ç¨³å®š
  * [Fast Full-frame Video Stabilization with Iterative Optimization](http://arxiv.org/abs/2307.12774v1)
* Video Grounding(è§†é¢‘å®šä½)
  * [G2L: Semantically Aligned and Uniform Video Grounding via Geodesic and Game Theory](http://arxiv.org/abs/2307.14277v1)
  * [UniVTG: Towards Unified Video-Language Temporal Grounding](http://arxiv.org/abs/2307.16715v1)<br>:star:[code](https://github.com/showlab/UniVTG)
  * [Knowing Where to Focus: Event-aware Transformer for Video Grounding](http://arxiv.org/abs/2308.06947v1)<br>:star:[code](https://github.com/jinhyunj/EaTR)
  * [Scanning Only Once: An End-to-end Framework for Fast Temporal Grounding in Long Videos](http://arxiv.org/abs/2303.08345)
* è§†é¢‘åˆ†å‰²
  * [XMem++: Production-level Video Segmentation From Few Annotated Frames](http://arxiv.org/abs/2307.15958v1)
  * [Rethinking Amodal Video Segmentation from Learning Supervised Signals with Object-centric Representation](http://arxiv.org/abs/2309.13248v1)<br>:star:[code](https://github.com/kfan21/EoRaS)
  * [GraphEcho: Graph-Driven Unsupervised Domain Adaptation for Echocardiogram Video Segmentation](http://arxiv.org/abs/2309.11145v1)<br>:star:[code](https://github.com/xmed-lab/GraphEcho)
  * [MeViS: A Large-scale Benchmark for Video Segmentation with Motion Expressions](http://arxiv.org/abs/2308.08544v1)<br>:star:[code](https://henghuiding.github.io/MeViS)<br>:star:[code](https://henghuiding.github.io/MeViS/)<br>:thumbsup:[ICCV2023ï½œæ–°æ•°æ®é›† MeViSï¼šåŸºäºåŠ¨ä½œæè¿°çš„è§†é¢‘åˆ†å‰²](https://mp.weixin.qq.com/s/hHAgfiQdA_g0DkmgWPzLeg)
  * [MEGA: Multimodal Alignment Aggregation and Distillation For Cinematic Video Segmentation](http://arxiv.org/abs/2308.11185v1)
  * [Tracking Anything with Decoupled Video Segmentation](http://arxiv.org/abs/2309.03903v1)<br>:star:[code](https://hkchengrex.github.io/Tracking-Anything-with-DEVA)
  * [The Making and Breaking of Camouflage](http://arxiv.org/abs/2309.03899v1)
  * [Tube-Link: A Flexible Cross Tube Framework for Universal Video Segmentation](https://openaccess.thecvf.com/content/ICCV2023/papers/Li_Tube-Link_A_Flexible_Cross_Tube_Framework_for_Universal_Video_Segmentation_ICCV_2023_paper.pdf)
* è§†é¢‘å¯¹åº”
  * [Learning Fine-Grained Features for Pixel-wise Video Correspondences](http://arxiv.org/abs/2308.03040v1)<br>:star:[code](https://github.com/qianduoduolr/FGVC)
* è§†é¢‘æ„ŸçŸ¥
  * [ResQ: Residual Quantization for Video Perception](http://arxiv.org/abs/2308.09511v1)
* è§†é¢‘è¯†åˆ«
  * [Audio-Visual Glance Network for Efficient Video Recognition](http://arxiv.org/abs/2308.09322v1)
  * [Learning from Semantic Alignment between Unpaired Multiviews for Egocentric Video Recognition](http://arxiv.org/abs/2308.11489v1)<br>:star:[code](https://github.com/wqtwjt1996/SUM-L)
  * [Implicit Temporal Modeling with Learnable Alignment for Video Recognition](http://arxiv.org/abs/2304.10465)
* è§†é¢‘ä¿®è¡¥
  * [ProPainter: Improving Propagation and Transformer for Video Inpainting](http://arxiv.org/abs/2309.03897v1)<br>:star:[code](https://github.com/sczhou/ProPainter)
* è§†é¢‘è¡¨ç¤ºå­¦ä¹ 
  * [MGMAE: Motion Guided Masking for Video Masked Autoencoding](http://arxiv.org/abs/2308.10794v1)
  * [Spatio-Temporal Crop Aggregation for Video Representation Learning](http://arxiv.org/abs/2211.17042)
* VAD
  * [TeD-SPAD: Temporal Distinctiveness for Self-supervised Privacy-preservation for video Anomaly Detection](http://arxiv.org/abs/2308.11072v1)<br>:star:[code](https://joefioresi718.github.io/TeD-SPAD_webpage/)
  * [Video Anomaly Detection via Sequentially Learning Multiple Pretext Tasks](https://openaccess.thecvf.com/content/ICCV2023/papers/Shi_Video_Anomaly_Detection_via_Sequentially_Learning_Multiple_Pretext_Tasks_ICCV_2023_paper.pdf)
  * [Feature Prediction Diffusion Model for Video Anomaly Detection](https://openaccess.thecvf.com/content/ICCV2023/papers/Yan_Feature_Prediction_Diffusion_Model_for_Video_Anomaly_Detection_ICCV_2023_paper.pdf)
* Video Localization
  * [UnLoc: A Unified Framework for Video Localization Tasks](http://arxiv.org/abs/2308.11062v1)<br>:star:[code](https://github.com/google-research/scenic)
  * [Video OWL-ViT: Temporally-consistent open-world localization in video](http://arxiv.org/abs/2308.11093v1)
  * [Multimodal Motion Conditioned Diffusion Model for Skeleton-based Video Anomaly Detection](https://openaccess.thecvf.com/content/ICCV2023/papers/Flaborea_Multimodal_Motion_Conditioned_Diffusion_Model_for_Skeleton-based_Video_Anomaly_Detection_ICCV_2023_paper.pdf)
  * [TeD-SPAD: Temporal Distinctiveness for Self-Supervised Privacy-Preservation for Video Anomaly Detection](https://openaccess.thecvf.com/content/ICCV2023/papers/Fioresi_TeD-SPAD_Temporal_Distinctiveness_for_Self-Supervised_Privacy-Preservation_for_Video_Anomaly_Detection_ICCV_2023_paper.pdf)
* è§†é¢‘é¢„æµ‹
  * [MMVP: Motion-Matrix-Based Video Prediction](http://arxiv.org/abs/2308.16154)
  * [Efficient Video Prediction via Sparsely Conditioned Flow Matching](http://arxiv.org/abs/2211.14575)
* è§†é¢‘ç»ç’ƒåˆ†å‰²
  * [Multi-view Spectral Polarization Propagation for Video Glass Segmentation](https://openaccess.thecvf.com/content/ICCV2023/papers/Qiao_Multi-view_Spectral_Polarization_Propagation_for_Video_Glass_Segmentation_ICCV_2023_paper.pdf)
* è§†é¢‘å¸§æ’å€¼
  * [Rethinking Video Frame Interpolation from Shutter Mode Induced Degradation](https://openaccess.thecvf.com/content/ICCV2023/papers/Ji_Rethinking_Video_Frame_Interpolation_from_Shutter_Mode_Induced_Degradation_ICCV_2023_paper.pdf)
* è§†é¢‘è¯­ä¹‰å‹ç¼©
  * [Non-Semantics Suppressed Mask Learning for Unsupervised Video Semantic Compression](https://openaccess.thecvf.com/content/ICCV2023/papers/Tian_Non-Semantics_Suppressed_Mask_Learning_for_Unsupervised_Video_Semantic_Compression_ICCV_2023_paper.pdf)
* è§†é¢‘-è§†é¢‘ç¿»è¯‘ 
  * [Shortcut-V2V: Compression Framework for Video-to-Video Translation Based on Temporal Redundancy Reduction](https://openaccess.thecvf.com/content/ICCV2023/papers/Chung_Shortcut-V2V_Compression_Framework_for_Video-to-Video_Translation_Based_on_Temporal_Redundancy_ICCV_2023_paper.pdf)

<a name="32"/>

## 32.Sign Language Recognition(æ‰‹è¯­)
* [Human Part-wise 3D Motion Context Learning for Sign Language Recognition](http://arxiv.org/abs/2308.09305v1)
* [CoSign: Exploring Co-occurrence Signals in Skeleton-based Continuous Sign Language Recognition](https://openaccess.thecvf.com/content/ICCV2023/papers/Jiao_CoSign_Exploring_Co-occurrence_Signals_in_Skeleton-based_Continuous_Sign_Language_Recognition_ICCV_2023_paper.pdf)
* [Improving Continuous Sign Language Recognition with Cross-Lingual Signs](http://arxiv.org/abs/2308.10809v1)
* [C2ST: Cross-Modal Contextualized Sequence Transduction for Continuous Sign Language Recognition](https://openaccess.thecvf.com/content/ICCV2023/papers/Zhang_C2ST_Cross-Modal_Contextualized_Sequence_Transduction_for_Continuous_Sign_Language_Recognition_ICCV_2023_paper.pdf)
* æ‰‹è¯­ç¿»è¯‘
  * [Sign Language Translation with Iterative Prototype](http://arxiv.org/abs/2308.12191v1)
  * [Gloss-free Sign Language Translation: Improving from Visual-Language Pretraining](http://arxiv.org/abs/2307.14768v1)<br>:star:[code](https://github.com/zhoubenjia/GFSLT-VLP)

<a name="31"/>

## 31.Human-Object Interaction(äººç‰©äº¤äº’)
* [Efficient Adaptive Human-Object Interaction Detection with Concept-guided Memory](http://arxiv.org/abs/2309.03696)
* [Persistent-Transient Duality: A Multi-mechanism Approach for Modeling Human-Object Interaction](http://arxiv.org/abs/2307.12729v1)
* [Re-mine, Learn and Reason: Exploring the Cross-modal Semantic Correlations for Language-guided HOI detection](http://arxiv.org/abs/2307.13529v1)
* [Agglomerative Transformer for Human-Object Interaction Detection](http://arxiv.org/abs/2308.08370v1)
* [InterDiff: Generating 3D Human-Object Interactions with Physics-Informed Diffusion](http://arxiv.org/abs/2308.16905v1)<br>:star:[code](https://sirui-xu.github.io/InterDiff/)
* [Exploring Predicate Visual Context in Detecting of Human-Object Interactions](http://arxiv.org/abs/2308.06202)
* [Persistent-Transient Duality: A Multi-Mechanism Approach for Modeling Human-Object Interaction](http://arxiv.org/abs/2307.12729)
* [Narrator: Towards Natural Control of Human-Scene Interaction Generation via Relationship Reasoning](http://arxiv.org/abs/2303.09410)
* [Open Set Video HOI detection from Action-Centric Chain-of-Look Prompting](https://openaccess.thecvf.com/content/ICCV2023/papers/Xi_Open_Set_Video_HOI_detection_from_Action-Centric_Chain-of-Look_Prompting_ICCV_2023_paper.pdf)
* [Hierarchical Generation of Human-Object Interactions with Diffusion Probabilistic Models](https://openaccess.thecvf.com/content/ICCV2023/papers/Pi_Hierarchical_Generation_of_Human-Object_Interactions_with_Diffusion_Probabilistic_Models_ICCV_2023_paper.pdf)
* æ‰‹ç‰©äº¤äº’
  * [EgoPCA: A New Framework for Egocentric Hand-Object Interaction Understanding](http://arxiv.org/abs/2309.02423v1)<br>:house:[project](https://mvig-rhos.com/ego_pca)
  * [Diffusion-Guided Reconstruction of Everyday Hand-Object Interaction Clips](http://arxiv.org/abs/2309.05663v1)<br>:star:[code](https://judyye.github.io/diffhoi-www/)
  * [AffordPose: A Large-scale Dataset of Hand-Object Interactions with Affordance-driven Hand Pose](http://arxiv.org/abs/2309.08942v1)<br>:star:[code](https://github.com/GentlesJan/AffordPose)
  * [Novel-View Synthesis and Pose Estimation for Hand-Object Interaction from Sparse Views](http://arxiv.org/abs/2308.11198)

<a name="30"/>

## 30.SLAM/Augmented Reality/Virtual Reality/Robotics(å¢å¼º/è™šæ‹Ÿç°å®/æœºå™¨äºº)
* è™šæ‹Ÿäººç‰©ç”Ÿæˆ
  * [MODA: Mapping-Once Audio-driven Portrait Animation with Dual Attentions](http://arxiv.org/abs/2307.10008v1)
  * [NSF: Neural Surface Fields for Human Modeling from Monocular Depth](http://arxiv.org/abs/2308.14847v1)<br>:house:[project](https://yuxuan-xue.com/nsf)
  * [AvatarCraft: Transforming Text into Neural Human Avatars with Parameterized Shape and Pose Control](http://arxiv.org/abs/2303.17606)
* æœºå™¨äºº
  * [Leveraging SE(3) Equivariance for Learning 3D Geometric Shape Assembly](http://arxiv.org/abs/2309.06810v1)<br>:star:[code](https://github.com/crtie/Leveraging-SE-3-Equivariance-for-Learning-3D-Geometric-Shape-Assembly)<br>:star:[code](https://crtie.github.io/SE-3-part-assembly/)
  * [PourIt!: Weakly-Supervised Liquid Perception from a Single Image for Visual Closed-Loop Robotic Pouring](http://arxiv.org/abs/2307.11299)
* AR/VR
  * [HMD-NeMo: Online 3D Avatar Motion Generation From Sparse Observations](http://arxiv.org/abs/2308.11261v1)
* SLAM
  * [GO-SLAM: Global Optimization for Consistent 3D Instant Reconstruction](http://arxiv.org/abs/2309.02436v1)<br>:star:[code](https://youmi-zym.github.io/projects/GO-SLAM/)<br>:star:[code](https://github.com/youmi-zym/GO-SLAM)
  * [Point-SLAM: Dense Neural Point Cloud-based SLAM](https://openaccess.thecvf.com/content/ICCV2023/papers/Sandstrom_Point-SLAM_Dense_Neural_Point_Cloud-based_SLAM_ICCV_2023_paper.pdf)
* è™šæ‹Ÿè¯•ç©¿
  * [Virtual Try-On with Pose-Garment Keypoints Guided Inpainting](https://openaccess.thecvf.com/content/ICCV2023/papers/Li_Virtual_Try-On_with_Pose-Garment_Keypoints_Guided_Inpainting_ICCV_2023_paper.pdf)
  * [Size Does Matter: Size-aware Virtual Try-on via Clothing-oriented Transformation Try-on Network](https://openaccess.thecvf.com/content/ICCV2023/papers/Chen_Size_Does_Matter_Size-aware_Virtual_Try-on_via_Clothing-oriented_Transformation_Try-on_ICCV_2023_paper.pdf)

<a name="29"/>

## 29.Autonomous vehicles(è‡ªåŠ¨é©¾é©¶)
* [HM-ViT: Hetero-Modal Vehicle-to-Vehicle Cooperative Perception with Vision Transformer](https://openaccess.thecvf.com/content/ICCV2023/papers/Xiang_HM-ViT_Hetero-Modal_Vehicle-to-Vehicle_Cooperative_Perception_with_Vision_Transformer_ICCV_2023_paper.pdf)(https://github.com/XHwind/HM-ViT)
* [TransIFF: An Instance-Level Feature Fusion Framework for Vehicle-Infrastructure Cooperative 3D Detection with Transformers](https://openaccess.thecvf.com/content/ICCV2023/papers/Chen_TransIFF_An_Instance-Level_Feature_Fusion_Framework_for_Vehicle-Infrastructure_Cooperative_3D_ICCV_2023_paper.pdf)
* è‡ªåŠ¨é©¾é©¶
  * [Improving Online Lane Graph Extraction by Object-Lane Clustering](http://arxiv.org/abs/2307.10947v1)
  * [Optimizing the Placement of Roadside LiDARs for Autonomous Driving](https://openaccess.thecvf.com/content/ICCV2023/papers/Jiang_Optimizing_the_Placement_of_Roadside_LiDARs_for_Autonomous_Driving_ICCV_2023_paper.pdf)
  * [VAD: Vectorized Scene Representation for Efficient Autonomous Driving](http://arxiv.org/abs/2303.12077)
  * [Domain Generalization of 3D Semantic Segmentation in Autonomous Driving](http://arxiv.org/abs/2212.04245)
  * [Unsupervised 3D Perception with 2D Vision-Language Distillation for Autonomous Driving](http://arxiv.org/abs/2309.14491v1)
  * [DriveAdapter: Breaking the Coupling Barrier of Perception and Planning in End-to-End Autonomous Driving](http://arxiv.org/abs/2308.00398v1)<br>:star:[code](https://github.com/OpenDriveLab/DriveAdapter)
  * [Video Task Decathlon: Unifying Image and Video Tasks in Autonomous Driving](http://arxiv.org/abs/2309.04422v1)<br>:house:[project](https://www.vis.xyz/pub/vtd)
  * [Does Physical Adversarial Example Really Matter to Autonomous Driving? Towards System-Level Effect of Adversarial Object Evasion Attack](http://arxiv.org/abs/2308.11894v1)
  * [Towards Viewpoint Robustness in Bird's Eye View Segmentation](http://arxiv.org/abs/2309.05192v1)<br>:star:[code](https://nvlabs.github.io/viewpoint-robustness)
  * [GameFormer: Game-theoretic Modeling and Learning of Transformer-based Interactive Prediction and Planning for Autonomous Driving](http://arxiv.org/abs/2303.05760)
* è½¨è¿¹é¢„æµ‹
  * [ADAPT: Efficient Multi-Agent Trajectory Prediction with Adaptation](http://arxiv.org/abs/2307.14187v1)<br>:star:[code](https://KUIS-AI.github.io/adapt)
  * [Fast Inference and Update of Probabilistic Density Estimation on Trajectory Prediction](http://arxiv.org/abs/2308.08824v1)<br>:star:[code](https://github.com/meaten/FlowChain-ICCV2023)
  * [Learn TAROT with MENTOR: A Meta-Learned Self-Supervised Approach for Trajectory Prediction](https://openaccess.thecvf.com/content/ICCV2023/papers/Pourkeshavarz_Learn_TAROT_with_MENTOR_A_Meta-Learned_Self-Supervised_Approach_for_Trajectory_ICCV_2023_paper.pdf)
  * [Sparse Instance Conditioned Multimodal Trajectory Prediction](https://openaccess.thecvf.com/content/ICCV2023/papers/Dong_Sparse_Instance_Conditioned_Multimodal_Trajectory_Prediction_ICCV_2023_paper.pdf)
  * [Trajectory Unified Transformer for Pedestrian Trajectory Prediction](https://openaccess.thecvf.com/content/ICCV2023/papers/Shi_Trajectory_Unified_Transformer_for_Pedestrian_Trajectory_Prediction_ICCV_2023_paper.pdf)
  * [BiFF: Bi-level Future Fusion with Polyline-based Coordinate for Interactive Trajectory Prediction](http://arxiv.org/abs/2306.14161)
  * [Joint Metrics Matter: A Better Standard for Trajectory Forecasting](http://arxiv.org/abs/2305.06292)
  * [Traj-MAE: Masked Autoencoders for Trajectory Prediction](https://openaccess.thecvf.com/content/ICCV2023/papers/Chen_Traj-MAE_Masked_Autoencoders_for_Trajectory_Prediction_ICCV_2023_paper.pdf)
* è½¦é“çº¿
  * [PETRv2: A Unified Framework for 3D Perception from Multi-Camera Images](https://arxiv.org/pdf/2206.01256.pdf)<br>:star:[code](https://github.com/megvii-research/PETR.git)
  * [ADNet: Lane Shape Prediction via Anchor Decomposition](http://arxiv.org/abs/2308.10481v1)<br>:star:[code](https://github.com/)
  * [LATR: 3D Lane Detection from Monocular Images with Transformer](http://arxiv.org/abs/2308.04583v1)<br>:star:[code](https://github.com/JMoonr/LATR)
  * [Recursive Video Lane Detection](http://arxiv.org/abs/2308.11106v1)<br>:star:[code](https://github.com/dongkwonjin/RVLD)
  * [Sparse Point Guided 3D Lane Detection](https://openaccess.thecvf.com/content/ICCV2023/papers/Yao_Sparse_Point_Guided_3D_Lane_Detection_ICCV_2023_paper.pdf)
  * [Generating Dynamic Kernels via Transformers for Lane Detection](https://openaccess.thecvf.com/content/ICCV2023/papers/Chen_Generating_Dynamic_Kernels_via_Transformers_for_Lane_Detection_ICCV_2023_paper.pdf)

<a name="28"/>

## 28.Style Transfer(é£æ ¼è¿ç§»)
* [AesPA-Net: Aesthetic Pattern-Aware Style Transfer Networks](http://arxiv.org/abs/2307.09724v1)<br>:star:[code](https://github.com/Kibeom-Hong/AesPA-Net)
* [Creative Birds: Self-Supervised Single-View 3D Style Transfer](http://arxiv.org/abs/2307.14127v1)<br>:star:[code](https://github.com/wrk226/2D-to-3D-Evolution-Transfer)
* [StyleDiffusion: Controllable Disentangled Style Transfer via Diffusion Models](http://arxiv.org/abs/2308.07863v1)
* [Zero-Shot Contrastive Loss for Text-Guided Diffusion Image Style Transfer](http://arxiv.org/abs/2303.08622)
* [Two Birds, One Stone: A Unified Framework for Joint Learning of Image and Video Style Transfers](http://arxiv.org/abs/2304.11335)

<a name="27"/>

## 27.Self/Semi-Supervised Learning
* å…¨ç›‘ç£
  * [OPERA: Omni-Supervised Representation Learning with Hierarchical Supervisions](http://arxiv.org/abs/2210.05557)
* æ— ç›‘ç£å­¦ä¹ 
  * [NCHO: Unsupervised Learning for Neural 3D Composition of Humans and Objects](http://arxiv.org/abs/2305.14345)
  * [Householder Projector for Unsupervised Latent Semantics Discovery](http://arxiv.org/abs/2307.08012)
* è‡ªç›‘ç£å­¦ä¹ 
  * [Stable and Causal Inference for Discriminative Self-supervised Deep Visual Representations](http://arxiv.org/abs/2308.08321v1)
  * [Tubelet-Contrastive Self-Supervision for Video-Efficient Generalization](http://arxiv.org/abs/2303.11003)
  * [Active Self-Supervised Learning: A Few Low-Cost Relationships Are All You Need](http://arxiv.org/abs/2303.15256)
  * [Geometrized Transformer for Self-Supervised Homography Estimation](https://openaccess.thecvf.com/content/ICCV2023/papers/Liu_Geometrized_Transformer_for_Self-Supervised_Homography_Estimation_ICCV_2023_paper.pdf)
  * [LightDepth: Single-View Depth Self-Supervision from Illumination Decline](https://openaccess.thecvf.com/content/ICCV2023/papers/Rodriguez-Puigvert_LightDepth_Single-View_Depth_Self-Supervision_from_Illumination_Decline_ICCV_2023_paper.pdf)
  * [Contactless Pulse Estimation Leveraging Pseudo Labels and Self-Supervision](https://openaccess.thecvf.com/content/ICCV2023/papers/Li_Contactless_Pulse_Estimation_Leveraging_Pseudo_Labels_and_Self-Supervision_ICCV_2023_paper.pdf)
  * [An Embarrassingly Simple Backdoor Attack on Self-supervised Learning](http://arxiv.org/abs/2210.07346)
  * [Learning by Sorting: Self-supervised Learning with Group Ordering Constraints](http://arxiv.org/abs/2301.02009)
  * [Representation Uncertainty in Self-Supervised Learning as Variational Inference](http://arxiv.org/abs/2203.11437)
  * [L-DAWA: Layer-wise Divergence Aware Weight Aggregation in Federated Self-Supervised Visual Representation Learning](https://openaccess.thecvf.com/content/ICCV2023/papers/Rehman_L-DAWA_Layer-wise_Divergence_Aware_Weight_Aggregation_in_Federated_Self-Supervised_Visual_ICCV_2023_paper.pdf)
  * [Randomized Quantization: A Generic Augmentation for Data Agnostic Self-supervised Learning](http://arxiv.org/abs/2212.08663)
  * [Semantics Meets Temporal Correspondence: Self-supervised Object-centric Learning in Videos](http://arxiv.org/abs/2308.09951v1)<br>:star:[code](https://github.com/shvdiwnkozbw/SMTC)
  * [Self-supervised Learning to Bring Dual Reversed Rolling Shutter Images Alive](http://arxiv.org/abs/2305.19862)
  * [Multi-Label Self-Supervised Learning with Scene Images](http://arxiv.org/abs/2308.03286)
  * [Weakly Supervised Learning of Semantic Correspondence through Cascaded Online Correspondence Refinement](https://openaccess.thecvf.com/content/ICCV2023/papers/Huang_Weakly_Supervised_Learning_of_Semantic_Correspondence_through_Cascaded_Online_Correspondence_ICCV_2023_paper.pdf)
* åŠç›‘ç£å­¦ä¹ 
  * [Shrinking Class Space for Enhanced Certainty in Semi-Supervised Learning](http://arxiv.org/abs/2308.06777v1)<br>:star:[code](https://github.com/LiheYoung/ShrinkMatch)
  * [Enhancing Sample Utilization through Sample Adaptive Augmentation in Semi-Supervised Learning](http://arxiv.org/abs/2309.03598v1)
  * [Semi-Supervised Learning via Weight-Aware Distillation under Class Distribution Mismatch](http://arxiv.org/abs/2308.11874)
  * [Learning Semi-supervised Gaussian Mixture Models for Generalized Category Discovery](http://arxiv.org/abs/2305.06144)
  * [SSB: Simple but Strong Baseline for Boosting Performance of Open-Set Semi-Supervised Learning](https://openaccess.thecvf.com/content/ICCV2023/papers/Fan_SSB_Simple_but_Strong_Baseline_for_Boosting_Performance_of_Open-Set_ICCV_2023_paper.pdf)
  * [The Perils of Learning From Unlabeled Data: Backdoor Attacks on Semi-supervised Learning](http://arxiv.org/abs/2211.00453)
  * [A Soft Nearest-Neighbor Framework for Continual Semi-Supervised Learning](http://arxiv.org/abs/2212.05102)
  * [Towards Semi-supervised Learning with Non-random Missing Labels](http://arxiv.org/abs/2308.08872v1)<br>:star:[code](https://github.com/NJUyued/PRG4SSL-MNAR)
  * [Diverse Cotraining Makes Strong Semi-Supervised Segmentor](http://arxiv.org/abs/2308.09281v1)<br>:star:[code](https://github.com/williamium3000/diverse-cotraining)
  * [Semi-Supervised Learning via Weight-aware Distillation under Class Distribution Mismatch](http://arxiv.org/abs/2308.11874v1)<br>:star:[code](https://github.com/RUC-DWBI-ML/research/tree/main/WAD-master)
  * [IOMatch: Simplifying Open-Set Semi-Supervised Learning with Joint Inliers and Outliers Utilization](http://arxiv.org/abs/2308.13168v1)<br>:star:[code](https://github.com/nukezil/IOMatch)

<a name="26"/>

## 26.Machine Learning(æœºå™¨å­¦ä¹ )
* [Mitigating Adversarial Vulnerability through Causal Parameter Estimation by Adversarial Double Machine Learning](http://arxiv.org/abs/2307.07250)
* Adversarial Learning(å¯¹æŠ—å­¦ä¹ ) 
  * [ACTIVE: Towards Highly Transferable 3D Physical Camouflage for Universal and Robust Vehicle Evasion](http://arxiv.org/abs/2308.07009v1)<br>:star:[code](https://islab-ai.github.io/active-iccv2023/)
  * [Towards Building More Robust Models with Frequency Bias](http://arxiv.org/abs/2307.09763v1)
  * [Backpropagation Path Search On Adversarial Transferability](http://arxiv.org/abs/2308.07625v1)
  * [Enhancing Generalization of Universal Adversarial Perturbation through Gradient Aggregation](http://arxiv.org/abs/2308.06015)
  * [AdaptGuard: Defending Against Universal Attacks for Model Adaptation](http://arxiv.org/abs/2303.10594)
  * é»‘ç›’
    * [CGBA: Curvature-aware Geometric Black-box Attack](https://arxiv.org/abs/2308.03163)<br>:star:[code](https://github.com/Farhamdur/CGBA)
  * å¯¹æŠ—æ ·æœ¬
    * [Downstream-agnostic Adversarial Examples](http://arxiv.org/abs/2307.12280v1)
    * [Benchmarking and Analyzing Robust Point Cloud Recognition: Bag of Tricks for Defending Adversarial Examples](http://arxiv.org/abs/2307.16361v1)<br>:star:[code](https://github.com/qiufan319/benchmark_pc_attack.git)
  * å¯¹æŠ—æ”»å‡»
    * [An Adaptive Model Ensemble Adversarial Attack for Boosting Adversarial Transferability](http://arxiv.org/abs/2308.02897v1)
    * [Unified Adversarial Patch for Cross-Modal Attacks in the Physical World](http://arxiv.org/abs/2307.07859)
    * [Tracing the Origin of Adversarial Attack for Forensic Investigation and Deterrence](http://arxiv.org/abs/2301.01218)
    * [LEA2: A Lightweight Ensemble Adversarial Attack via Non-overlapping Vulnerable Frequency Regions](https://openaccess.thecvf.com/content/ICCV2023/papers/Qian_LEA2_A_Lightweight_Ensemble_Adversarial_Attack_via_Non-overlapping_Vulnerable_Frequency_ICCV_2023_paper.pdf)
    * [RFLA: A Stealthy Reflected Light Adversarial Attack in the Physical World](http://arxiv.org/abs/2307.07653)
    * [Distracting Downpour: Adversarial Weather Attacks for Motion Estimation](http://arxiv.org/abs/2305.06716)
    * [Boosting Adversarial Transferability via Gradient Relevance Attack](https://openaccess.thecvf.com/content/ICCV2023/papers/Zhu_Boosting_Adversarial_Transferability_via_Gradient_Relevance_Attack_ICCV_2023_paper.pdf)
    * [SAGA: Spectral Adversarial Geometric Attack on 3D Meshes](http://arxiv.org/abs/2211.13775)
    * [F&F Attack: Adversarial Attack against Multiple Object Trackers by Inducing False Negatives and False Positives](https://openaccess.thecvf.com/content/ICCV2023/papers/Zhou_FF_Attack_Adversarial_Attack_against_Multiple_Object_Trackers_by_Inducing_ICCV_2023_paper.pdf)
  * å¯¹æŠ—è®­ç»ƒ  
    * [Improving Generalization of Adversarial Training via Robust Critical Fine-Tuning](http://arxiv.org/abs/2308.02533v1)<br>:star:[code](https://github.com/microsoft/robustlearn)  
    * [Adversarial Finetuning with Latent Representation Constraint to Mitigate Accuracy-Robustness Tradeoff](http://arxiv.org/abs/2308.16454v1)
    * [Improving Generalization of Adversarial Training via Robust Critical Fine-Tuning](http://arxiv.org/abs/2308.02533)
    * [Advancing Example Exploitation Can Alleviate Critical Challenges in Adversarial Training](https://openaccess.thecvf.com/content/ICCV2023/papers/Ge_Advancing_Example_Exploitation_Can_Alleviate_Critical_Challenges_in_Adversarial_Training_ICCV_2023_paper.pdf)
    * [Fast Adversarial Training with Smooth Convergence](http://arxiv.org/abs/2308.12857)
    * [TRM-UAP: Enhancing the Transferability of Data-Free Universal Adversarial Perturbation via Truncated Ratio Maximization](https://openaccess.thecvf.com/content/ICCV2023/papers/Liu_TRM-UAP_Enhancing_the_Transferability_of_Data-Free_Universal_Adversarial_Perturbation_via_ICCV_2023_paper.pdf)
  * åé—¨
    * [Beating Backdoor Attack at Its Own Game](http://arxiv.org/abs/2307.15539)
    * [Enhancing Fine-Tuning Based Backdoor Defense with Sharpness-Aware Minimization](http://arxiv.org/abs/2304.11823)
    * [Computation and Data Efficient Backdoor Attacks](https://openaccess.thecvf.com/content/ICCV2023/papers/Wu_Computation_and_Data_Efficient_Backdoor_Attacks_ICCV_2023_paper.pdf)
* Class Incremental Learning(ç±»å¢é‡å­¦ä¹ )
  * [Proxy Anchor-based Unsupervised Learning for Continuous Generalized Category Discovery](http://arxiv.org/abs/2307.10943v1)
  * [Dynamic Residual Classifier for Class Incremental Learning](http://arxiv.org/abs/2308.13305)
  * [First Session Adaptation: A Strong Replay-Free Baseline for Class-Incremental Learning](http://arxiv.org/abs/2303.13199)
  * [Self-Organizing Pathway Expansion for Non-Exemplar Class-Incremental Learning](https://openaccess.thecvf.com/content/ICCV2023/papers/Zhu_Self-Organizing_Pathway_Expansion_for_Non-Exemplar_Class-Incremental_Learning_ICCV_2023_paper.pdf)
  * [Knowledge Restore and Transfer for Multi-Label Class-Incremental Learning](http://arxiv.org/abs/2302.13334)
  * [Audio-Visual Class-Incremental Learning](http://arxiv.org/abs/2308.11073v1)<br>:star:[code](https://github.com/weiguoPian/AV-CIL_ICCV2023)
  * [Masked Autoencoders are Efficient Class Incremental Learners](http://arxiv.org/abs/2308.12510v1)<br>:star:[code](https://github.com/scok30/MAE-CIL)
  * [Heterogeneous Forgetting Compensation for Class-Incremental Learning](http://arxiv.org/abs/2308.03374v1)<br>:star:[code](https://github.com/JiahuaDong/HFC)
  * [Class-Incremental Grouping Network for Continual Audio-Visual Learning](http://arxiv.org/abs/2309.05281v1)<br>:star:[code](https://github.com/stoneMo/CIGN)
  * [Space-time Prompting for Video Class-incremental Learning](https://openaccess.thecvf.com/content/ICCV2023/papers/Pei_Space-time_Prompting_for_Video_Class-incremental_Learning_ICCV_2023_paper.pdf)
* å¤šä»»åŠ¡å­¦ä¹ 
  * [Efficient Controllable Multi-Task Architectures](http://arxiv.org/abs/2308.11744v1)
  * [AdaMV-MoE: Adaptive Multi-Task Vision Mixture-of-Experts](https://openaccess.thecvf.com/content/ICCV2023/papers/Chen_AdaMV-MoE_Adaptive_Multi-Task_Vision_Mixture-of-Experts_ICCV_2023_paper.pdf)
  * [MAS: Towards Resource-Efficient Federated Multiple-Task Learning](http://arxiv.org/abs/2307.11285v1)<br>:thumbsup:[ICCV 2023 | å¦‚ä½•åœ¨èµ„æºå—é™ä¸‹è¿›è¡Œè”é‚¦å¤šä»»åŠ¡å­¦ä¹ ](https://mp.weixin.qq.com/s/FgwkkHcjw-LaduzwpbqAdQ)
  * [FULLER: Unified Multi-modality Multi-task 3D Perception via Multi-level Gradient Calibration](http://arxiv.org/abs/2307.16617)
  * [Vision Transformer Adapters for Generalizable Multitask Learning](http://arxiv.org/abs/2308.12372v1)<br>:star:[code](https://ivrl.github.io/VTAGML)
  * [TaskExpert: Dynamically Assembling Multi-Task Representations with Memorial Mixture-of-Experts](http://arxiv.org/abs/2307.15324v1)<br>:star:[code](https://github.com/prismformore/Multi-Task-Transformer)
  * [Achievement-Based Training Progress Balancing for Multi-Task Learning](https://openaccess.thecvf.com/content/ICCV2023/papers/Yun_Achievement-Based_Training_Progress_Balancing_for_Multi-Task_Learning_ICCV_2023_paper.pdf)
* æŒç»­å­¦ä¹ 
  * [CLR: Channel-wise Lightweight Reprogramming for Continual Learning](http://arxiv.org/abs/2307.11386v1)<br>:star:[code](https://github.com/gyhandy/Channel-wise-Lightweight-Reprogramming)
  * [Wasserstein Expansible Variational Autoencoder for Discriminative and Generative Continual Learning](https://openaccess.thecvf.com/content/ICCV2023/papers/Ye_Wasserstein_Expansible_Variational_Autoencoder_for_Discriminative_and_Generative_Continual_Learning_ICCV_2023_paper.pdf)
  * [Data Augmented Flatness-aware Gradient Projection for Continual Learning](https://openaccess.thecvf.com/content/ICCV2023/papers/Yang_Data_Augmented_Flatness-aware_Gradient_Projection_for_Continual_Learning_ICCV_2023_paper.pdf)
  * [A Unified Continual Learning Framework with General Parameter-Efficient Tuning](http://arxiv.org/abs/2303.10070)
  * [Growing a Brain with Sparsity-Inducing Generation for Continual Learning](https://openaccess.thecvf.com/content/ICCV2023/papers/Jin_Growing_a_Brain_with_Sparsity-Inducing_Generation_for_Continual_Learning_ICCV_2023_paper.pdf)
  * [Towards Realistic Evaluation of Industrial Continual Learning Scenarios with an Emphasis on Energy Consumption and Computational Footprint](https://openaccess.thecvf.com/content/ICCV2023/papers/Chavan_Towards_Realistic_Evaluation_of_Industrial_Continual_Learning_Scenarios_with_an_ICCV_2023_paper.pdf)
  * [ICICLE: Interpretable Class Incremental Continual Learning](https://openaccess.thecvf.com/content/ICCV2023/papers/Rymarczyk_ICICLE_Interpretable_Class_Incremental_Continual_Learning_ICCV_2023_paper.pdf)
  * [CLNeRF: Continual Learning Meets NeRF](http://arxiv.org/abs/2308.14816v1)<br>:star:[code](https://github.com/IntelLabs/CLNeRF)<br>:house:[project](https://youtu.be/nLRt6OoDGq0?si=8yD6k-8MMBJInQPs)
  * [SLCA: Slow Learner with Classifier Alignment for Continual Learning on a Pre-trained Model](http://arxiv.org/abs/2303.05118)
  * [TARGET: Federated Class-Continual Learning via Exemplar-Free Distillation](http://arxiv.org/abs/2303.06937)
  * [Instance and Category Supervision are Alternate Learners for Continual Learning](https://openaccess.thecvf.com/content/ICCV2023/papers/Tian_Instance_and_Category_Supervision_are_Alternate_Learners_for_Continual_Learning_ICCV_2023_paper.pdf)
  * [Exemplar-Free Continual Transformer with Convolutions](http://arxiv.org/abs/2308.11357v1)
  * [Online Prototype Learning for Online Continual Learning](http://arxiv.org/abs/2308.00301v1)<br>:star:[code](https://github.com/weilllllls/OnPro)
  * [CBA: Improving Online Continual Learning via Continual Bias Adaptor](http://arxiv.org/abs/2308.06925v1)
  * [Introducing Language Guidance in Prompt-based Continual Learning](http://arxiv.org/abs/2308.15827v1)
  * [NAPA-VQ: Neighborhood Aware Prototype Augmentation with Vector Quantization for Continual Learning](http://arxiv.org/abs/2308.09297v1)<br>:star:[code](https://github.com/TamashaM/NAPA-VQ.git)
  * [Generating Instance-level Prompts for Rehearsal-free Continual Learning](https://openaccess.thecvf.com/content/ICCV2023/papers/Jung_Generating_Instance-level_Prompts_for_Rehearsal-free_Continual_Learning_ICCV_2023_paper.pdf)
  * [Online Continual Learning on Hierarchical Label Expansion](http://arxiv.org/abs/2308.14374)
  * [Few-shot Continual Infomax Learning](https://openaccess.thecvf.com/content/ICCV2023/papers/Gu_Few-shot_Continual_Infomax_Learning_ICCV_2023_paper.pdf)
  * [NAPA-VQ: Neighborhood-Aware Prototype Augmentation with Vector Quantization for Continual Learning](https://openaccess.thecvf.com/content/ICCV2023/papers/Malepathirana_NAPA-VQ_Neighborhood-Aware_Prototype_Augmentation_with_Vector_Quantization_for_Continual_Learning_ICCV_2023_paper.pdf)
* å¢é‡å­¦ä¹  
  * [When Prompt-based Incremental Learning Does Not Meet Strong Pretraining](http://arxiv.org/abs/2308.10445v1)<br>:star:[code](https://github.com/TOM-tym/APG)
* Federated Learning(è”é‚¦å­¦ä¹ )
  * [When Do Curricula Work in Federated Learning?](http://arxiv.org/abs/2212.12712)
  * [No Fear of Classifier Biases: Neural Collapse Inspired Federated Learning with Synthetic and Fixed Classifier](http://arxiv.org/abs/2303.10058)
  * [Holistic Geometric Feature Learning for Structured Reconstruction](http://arxiv.org/abs/2309.09622)
  * [ProtoFL: Unsupervised Federated Learning via Prototypical Distillation](http://arxiv.org/abs/2307.12450v1)
  * [Multi-Metrics Adaptively Identifies Backdoors in Federated Learning](http://arxiv.org/abs/2303.06601)
  * [zPROBE: Zero Peek Robustness Checks for Federated Learning](http://arxiv.org/abs/2206.12100)
  * [PGFed: Personalize Each Client's Global Objective for Federated Learning](https://openaccess.thecvf.com/content/ICCV2023/papers/Luo_PGFed_Personalize_Each_Clients_Global_Objective_for_Federated_Learning_ICCV_2023_paper.pdf)
  * [Communication-efficient Federated Learning with Single-Step Synthetic Features Compressor for Faster Convergence](http://arxiv.org/abs/2302.13562)
  * [Towards Instance-adaptive Inference for Federated Learning](http://arxiv.org/abs/2308.06051)
  * [Efficient Model Personalization in Federated Learning via Client-Specific Prompt Generation](http://arxiv.org/abs/2308.15367v1)
  * [FedPerfix: Towards Partial Model Personalization of Vision Transformers in Federated Learning](http://arxiv.org/abs/2308.09160v1)
  * [Workie-Talkie: Accelerating Federated Learning by Overlapping Computing and Communications via Contrastive Regularization](https://openaccess.thecvf.com/content/ICCV2023/papers/Chen_Workie-Talkie_Accelerating_Federated_Learning_by_Overlapping_Computing_and_Communications_via_ICCV_2023_paper.pdf)
  * [Generative Gradient Inversion via Over-Parameterized Networks in Federated Learning](https://openaccess.thecvf.com/content/ICCV2023/papers/Zhang_Generative_Gradient_Inversion_via_Over-Parameterized_Networks_in_Federated_Learning_ICCV_2023_paper.pdf)
  * [FRAug: Tackling Federated Learning with Non-IID Features via Representation Augmentation](http://arxiv.org/abs/2205.14900)
  * [Bold but Cautious: Unlocking the Potential of Personalized Federated Learning through Cautiously Aggressive Collaboration](http://arxiv.org/abs/2309.11103)
* Reinforcement Learning(å¼ºåŒ–å­¦ä¹ )
  * [Improving Generalization in Visual Reinforcement Learning via Conflict-aware Gradient Agreement Augmentation](http://arxiv.org/abs/2308.01194v1)
  * [GAIT: Generating Aesthetic Indoor Tours with Deep Reinforcement Learning](https://openaccess.thecvf.com/content/ICCV2023/papers/Xie_GAIT_Generating_Aesthetic_Indoor_Tours_with_Deep_Reinforcement_Learning_ICCV_2023_paper.pdf)
  * [Simoun: Synergizing Interactive Motion-appearance Understanding for Vision-based Reinforcement Learning](https://openaccess.thecvf.com/content/ICCV2023/papers/Huang_Simoun_Synergizing_Interactive_Motion-appearance_Understanding_for_Vision-based_Reinforcement_Learning_ICCV_2023_paper.pdf)
  * [DISeR: Designing Imaging Systems with Reinforcement Learning](http://arxiv.org/abs/2309.13851v1)<br>:star:[code](https://tzofi.github.io/diser)
  * [Learning to Identify Critical States for Reinforcement Learning from Videos](http://arxiv.org/abs/2308.07795v1)<br>:star:[code](https://github.com/AI-Initiative-KAUST/VideoRLCS)
  * [Towards Attack-tolerant Federated Learning via Critical Parameter Analysis](http://arxiv.org/abs/2308.09318)
  * [Stabilizing Visual Reinforcement Learning via Asymmetric Interactive Cooperation](https://openaccess.thecvf.com/content/ICCV2023/papers/Zhai_Stabilizing_Visual_Reinforcement_Learning_via_Asymmetric_Interactive_Cooperation_ICCV_2023_paper.pdf)
* è¿ç§»å­¦ä¹ 
  * [Disposable Transfer Learning for Selective Source Task Unlearning](http://arxiv.org/abs/2308.09971v1)
  * [Distilling from Similar Tasks for Transfer Learning on a Budget](http://arxiv.org/abs/2304.12314)
  * [Building a Winning Team: Selecting Source Model Ensembles using a Submodular Transferability Estimation Approach](http://arxiv.org/abs/2309.02429v1)
  * [Exploring Model Transferability through the Lens of Potential Energy](http://arxiv.org/abs/2308.15074v1)<br>:star:[code](https://github.com/lixiaotong97/PED)
  * [Disentangling Spatial and Temporal Learning for Efficient Image-to-Video Transfer Learning](http://arxiv.org/abs/2309.07911v1)<br>:star:[code](https://github.com/alibaba-mmai-research/DiST)
* å…ƒå­¦ä¹ 
  * [Meta-ZSDETR: Zero-shot DETR with Meta-learning](http://arxiv.org/abs/2308.09540v1)
  * [Enhanced Meta Label Correction for Coping with Label Corruption](http://arxiv.org/abs/2305.12961)
  * [Dark Side Augmentation: Generating Diverse Night Examples for Metric Learning](https://openaccess.thecvf.com/content/ICCV2023/papers/Mohwald_Dark_Side_Augmentation_Generating_Diverse_Night_Examples_for_Metric_Learning_ICCV_2023_paper.pdf)
  * [Learning with Diversity: Self-Expanded Equalization for Better Generalized Deep Metric Learning](https://openaccess.thecvf.com/content/ICCV2023/papers/Yan_Learning_with_Diversity_Self-Expanded_Equalization_for_Better_Generalized_Deep_Metric_ICCV_2023_paper.pdf)
* åº¦é‡å­¦ä¹ 
  * [Generalized Sum Pooling for Metric Learning](http://arxiv.org/abs/2308.09228v1)
  * [HSE: Hybrid Species Embedding for Deep Metric Learning](https://openaccess.thecvf.com/content/ICCV2023/papers/Yang_HSE_Hybrid_Species_Embedding_for_Deep_Metric_Learning_ICCV_2023_paper.pdf)
* å¤šæ¨¡æ€å­¦ä¹ 
  * [Preserving Modality Structure Improves Multi-Modal Learning](http://arxiv.org/abs/2308.13077v1)<br>:star:[code](https://github.com/Swetha5/Multi_Sinkhorn_Knopp)
  * [Distribution-Consistent Modal Recovering for Incomplete Multimodal Learning](https://openaccess.thecvf.com/content/ICCV2023/papers/Wang_Distribution-Consistent_Modal_Recovering_for_Incomplete_Multimodal_Learning_ICCV_2023_paper.pdf)
  * [DG3D: Generating High Quality 3D Textured Shapes by Learning to Discriminate Multi-Modal Diffusion-Renderings](https://openaccess.thecvf.com/content/ICCV2023/papers/Zuo_DG3D_Generating_High_Quality_3D_Textured_Shapes_by_Learning_to_ICCV_2023_paper.pdf)
  * [Practical Membership Inference Attacks Against Large-Scale Multi-Modal Models: A Pilot Study](https://openaccess.thecvf.com/content/ICCV2023/papers/Ko_Practical_Membership_Inference_Attacks_Against_Large-Scale_Multi-Modal_Models_A_Pilot_ICCV_2023_paper.pdf)
* å¯¹æ¯”å­¦ä¹ 
  * [Contrastive Learning Relies More on Spatial Inductive Bias Than Supervised Learning: An Empirical Study](https://openaccess.thecvf.com/content/ICCV2023/papers/Zhong_Contrastive_Learning_Relies_More_on_Spatial_Inductive_Bias_Than_Supervised_ICCV_2023_paper.pdf)
  * [One-Shot Recognition of Any Material Anywhere Using Contrastive Learning with Physics-Based Rendering](http://arxiv.org/abs/2212.00648)
  * [All4One: Symbiotic Neighbour Contrastive Learning via Self-Attention and Redundancy Reduction](http://arxiv.org/abs/2303.09417)
* æœºå™¨é—å¿˜
  * [MUter: Machine Unlearning on Adversarially Trained Models](https://openaccess.thecvf.com/content/ICCV2023/papers/Liu_MUter_Machine_Unlearning_on_Adversarially_Trained_Models_ICCV_2023_paper.pdf)

<a name="25"/>

## 25.Model Compression/Knowledge Distillation/Pruning(æ¨¡å‹å‹ç¼©/çŸ¥è¯†è’¸é¦/å‰ªæ)
* é‡åŒ–
  * [EMQ: Evolving Training-free Proxies for Automated Mixed Precision Quantization](http://arxiv.org/abs/2307.10554v1)
  * [Causal-DFQ: Causality Guided Data-free Network Quantization](http://arxiv.org/abs/2309.13682v1)<br>:star:[code](https://github.com/42Shawn/Causal-DFQ)
  * [DenseShift: Towards Accurate and Efficient Low-Bit Power-of-Two Quantization](http://arxiv.org/abs/2208.09708)
  * [EQ-Net: Elastic Quantization Neural Networks](https://openaccess.thecvf.com/content/ICCV2023/papers/Xu_EQ-Net_Elastic_Quantization_Neural_Networks_ICCV_2023_paper.pdf)
  * [Causal-DFQ: Causality Guided Data-Free Network Quantization](https://openaccess.thecvf.com/content/ICCV2023/papers/Shang_Causal-DFQ_Causality_Guided_Data-Free_Network_Quantization_ICCV_2023_paper.pdf)
* å‰ªæ
  * [Unified Data-Free Compression: Pruning and Quantization without Fine-Tuning](http://arxiv.org/abs/2308.07209v1)
  * [Differentiable Transportation Pruning](http://arxiv.org/abs/2307.08483)
  * [Efficient Joint Optimization of Layer-Adaptive Weight Pruning in Deep Neural Networks](http://arxiv.org/abs/2308.10438)
  * [Towards Fairness-aware Adversarial Network Pruning](https://openaccess.thecvf.com/content/ICCV2023/papers/Zhang_Towards_Fairness-aware_Adversarial_Network_Pruning_ICCV_2023_paper.pdf)
* è½»é‡çº§ç½‘ç»œ
  * [Adaptive Frequency Filters As Efficient Global Token Mixers](http://arxiv.org/abs/2307.14008v1)
* çŸ¥è¯†è’¸é¦
  * [DOT: A Distillation-Oriented Trainer](https://arxiv.org/abs/2307.08436)
  * [Class-relation Knowledge Distillation for Novel Class Discovery](http://arxiv.org/abs/2307.09158)
  * [Beyond the Limitation of Monocular 3D Detector via Knowledge Distillation](https://openaccess.thecvf.com/content/ICCV2023/papers/Yang_Beyond_the_Limitation_of_Monocular_3D_Detector_via_Knowledge_Distillation_ICCV_2023_paper.pdf)
  * [FerKD: Surgical Label Adaptation for Efficient Distillation](https://openaccess.thecvf.com/content/ICCV2023/papers/Shen_FerKD_Surgical_Label_Adaptation_for_Efficient_Distillation_ICCV_2023_paper.pdf)
  * [Masked Autoencoders Are Stronger Knowledge Distillers](https://openaccess.thecvf.com/content/ICCV2023/papers/Lao_Masked_Autoencoders_Are_Stronger_Knowledge_Distillers_ICCV_2023_paper.pdf)
  * [DETRDistill: A Universal Knowledge Distillation Framework for DETR-families](http://arxiv.org/abs/2211.10156)
  * [Distribution Shift Matters for Knowledge Distillation with Webly Collected Images](http://arxiv.org/abs/2307.11469)
  * [Automated Knowledge Distillation via Monte Carlo Tree Search](https://openaccess.thecvf.com/content/ICCV2023/papers/Li_Automated_Knowledge_Distillation_via_Monte_Carlo_Tree_Search_ICCV_2023_paper.pdf)
  * [Cumulative Spatial Knowledge Distillation for Vision Transformers](https://arxiv.org/abs/2307.08500)
  * [Data-free Knowledge Distillation for Fine-grained Visual Categorization](https://openaccess.thecvf.com/content/ICCV2023/papers/Shao_Data-free_Knowledge_Distillation_for_Fine-grained_Visual_Categorization_ICCV_2023_paper.pdf)
  * [Multi-Label Knowledge Distillation](http://arxiv.org/abs/2308.06453v1)<br>:star:[code](https://github.com/penghui-yang/L2D)
  * [DistillBEV: Boosting Multi-Camera 3D Object Detection with Cross-Modal Knowledge Distillation](http://arxiv.org/abs/2309.15109v1)
  * [Alleviating Catastrophic Forgetting of Incremental Object Detection via Within-Class and Between-Class Knowledge Distillation](https://openaccess.thecvf.com/content/ICCV2023/papers/Kang_Alleviating_Catastrophic_Forgetting_of_Incremental_Object_Detection_via_Within-Class_and_ICCV_2023_paper.pdf)
* æ¨¡å‹å‹ç¼©
  * [Lossy and Lossless (L2) Post-training Model Size Compression](https://openaccess.thecvf.com/content/ICCV2023/papers/Shi_Lossy_and_Lossless_L2_Post-training_Model_Size_Compression_ICCV_2023_paper.pdf)

<a name="24"/>

## 24.Few/Zero-Shot Learning/Domain Generalization/Adaptation(å°/é›¶æ ·æœ¬/åŸŸæ³›åŒ–/åŸŸé€‚åº”)
* åŸŸé€‚åº”
  * [Unsupervised Accuracy Estimation of Deep Visual Models using Domain-Adaptive Adversarial Perturbation without Source Samples](http://arxiv.org/abs/2307.10062v1)
  * [Local Context-Aware Active Domain Adaptation](http://arxiv.org/abs/2208.12856)
  * [Similarity Min-Max: Zero-Shot Day-Night Domain Adaptation](https://openaccess.thecvf.com/content/ICCV2023/papers/Luo_Similarity_Min-Max_Zero-Shot_Day-Night_Domain_Adaptation_ICCV_2023_paper.pdf)
  * [Augmenting and Aligning Snippets for Few-Shot Video Domain Adaptation](http://arxiv.org/abs/2303.10451)
  * [Improved Knowledge Transfer for Semi-Supervised Domain Adaptation via Trico Training Strategy](https://openaccess.thecvf.com/content/ICCV2023/papers/Ngo_Improved_Knowledge_Transfer_for_Semi-Supervised_Domain_Adaptation_via_Trico_Training_ICCV_2023_paper.pdf)
  * [Universal Domain Adaptation via Compressive Attention Matching](http://arxiv.org/abs/2304.11862)
  * [GeT: Generative Target Structure Debiasing for Domain Adaptation](http://arxiv.org/abs/2308.10205)
  * [PADCLIP: Pseudo-labeling with Adaptive Debiasing in CLIP for Unsupervised Domain Adaptation](https://openaccess.thecvf.com/content/ICCV2023/papers/Lai_PADCLIP_Pseudo-labeling_with_Adaptive_Debiasing_in_CLIP_for_Unsupervised_Domain_ICCV_2023_paper.pdf)
  * [Homeomorphism Alignment for Unsupervised Domain Adaptation](https://openaccess.thecvf.com/content/ICCV2023/papers/Zhou_Homeomorphism_Alignment_for_Unsupervised_Domain_Adaptation_ICCV_2023_paper.pdf)
  * [Towards Effective Instance Discrimination Contrastive Loss for Unsupervised Domain Adaptation](https://openaccess.thecvf.com/content/ICCV2023/papers/Zhang_Towards_Effective_Instance_Discrimination_Contrastive_Loss_for_Unsupervised_Domain_Adaptation_ICCV_2023_paper.pdf)
  * [Order-preserving Consistency Regularization for Domain Adaptation and Generalization](http://arxiv.org/abs/2309.13258v1)
  * [Complementary Domain Adaptation and Generalization for Unsupervised Continual Domain Shift Learning](http://arxiv.org/abs/2303.15833)
  * [One-Shot Generative Domain Adaptation](http://arxiv.org/abs/2111.09876)
  * [The Unreasonable Effectiveness of Large Language-Vision Models for Source-Free Video Domain Adaptation](http://arxiv.org/abs/2308.09139)
  * [StyleDomain: Efficient and Lightweight Parameterizations of StyleGAN for One-shot and Few-shot Domain Adaptation](http://arxiv.org/abs/2212.10229)
  * [SFHarmony: Source Free Domain Adaptation for Distributed Neuroimaging Analysis](http://arxiv.org/abs/2303.15965)
  * [Domain Adaptive Few-Shot Open-Set Learning](http://arxiv.org/abs/2309.12814)
  * [Confidence-based Visual Dispersal for Few-shot Unsupervised Domain Adaptation](http://arxiv.org/abs/2309.15575v1)<br>:star:[code](https://github.com/Bostoncake/C-VisDiT)
  * [LiDAR-UDA: Self-ensembling Through Time for Unsupervised LiDAR Domain Adaptation](http://arxiv.org/abs/2309.13523v1)<br>:star:[code](https://github.com/JHLee0513/LiDARUDA)
  * [Unsupervised Domain Adaptive Detection with Network Stability Analysis](http://arxiv.org/abs/2308.08182v1)<br>:star:[code](https://github.com/tiankongzhang/NSA)
  * [The Unreasonable Effectiveness of Large Language-Vision Models for Source-free Video Domain Adaptation](http://arxiv.org/abs/2308.09139v1)<br>:star:[code](https://github.com/giaczara/dallv)
  * [Domain-Specificity Inducing Transformers for Source-Free Domain Adaptation](http://arxiv.org/abs/2308.14023v1)<br>:house:[project](http://val.cds.iisc.ac.in/DSiT-SFDA)
  * [DomainAdaptor: A Novel Approach to Test-time Adaptation](http://arxiv.org/abs/2308.10297v1)<br>:star:[code](https://github.com/koncle/DomainAdaptor)
  * [GeT: Generative Target Structure Debiasing for Domain Adaptation](http://arxiv.org/abs/2308.10205v1)<br>:star:[code](https://lulusindazc.github.io/getproject/)
  * [Black-box Unsupervised Domain Adaptation with Bi-directional Atkinson-Shiffrin Memory](http://arxiv.org/abs/2308.13236v1)
  * [Towards Better Robustness against Common Corruptions for Unsupervised Domain Adaptation](https://openaccess.thecvf.com/content/ICCV2023/papers/Gao_Towards_Better_Robustness_against_Common_Corruptions_for_Unsupervised_Domain_Adaptation_ICCV_2023_paper.pdf)
  * [Bidirectional Alignment for Domain Adaptive Detection with Transformers](https://openaccess.thecvf.com/content/ICCV2023/papers/He_Bidirectional_Alignment_for_Domain_Adaptive_Detection_with_Transformers_ICCV_2023_paper.pdf)
  * [Text-Driven Generative Domain Adaptation with Spectral Consistency Regularization](https://openaccess.thecvf.com/content/ICCV2023/papers/Liu_Text-Driven_Generative_Domain_Adaptation_with_Spectral_Consistency_Regularization_ICCV_2023_paper.pdf)
* åŸŸæ³›åŒ–
  * [Flatness-Aware Minimization for Domain Generalization](http://arxiv.org/abs/2307.11108v1)
  * [DomainDrop: Suppressing Domain-Sensitive Channels for Domain Generalization](http://arxiv.org/abs/2308.10285v1)<br>:star:[code](https://github.com/lingeringlight/DomainDrop)
  * [Domain Generalization via Balancing Training Difficulty and Model Capability](http://arxiv.org/abs/2309.00844v1)
  * [Adversarial Bayesian Augmentation for Single-Source Domain Generalization](http://arxiv.org/abs/2307.09520)
  * [PASTA: Proportional Amplitude Spectrum Training Augmentation for Syn-to-Real Domain Generalization](http://arxiv.org/abs/2212.00979)
  * [DandelionNet: Domain Composition with Instance Adaptive Classification for Domain Generalization](https://openaccess.thecvf.com/content/ICCV2023/papers/Hu_DandelionNet_Domain_Composition_with_Instance_Adaptive_Classification_for_Domain_Generalization_ICCV_2023_paper.pdf)
  * [iDAG: Invariant DAG Searching for Domain Generalization](https://openaccess.thecvf.com/content/ICCV2023/papers/Huang_iDAG_Invariant_DAG_Searching_for_Domain_Generalization_ICCV_2023_paper.pdf)
  * [A Sentence Speaks a Thousand Images: Domain Generalization through Distilling CLIP with Language Guidance](http://arxiv.org/abs/2309.12530v1)
  * [Understanding Hessian Alignment for Domain Generalization](http://arxiv.org/abs/2308.11778v1)<br>:star:[code](https://github.com/huawei-noah/Federated-Learning/tree/main/HessianAlignment)
  * [Domain Generalization via Rationale Invariance](http://arxiv.org/abs/2308.11158v1)<br>:star:[code](https://github.com/liangchen527/RIDG)
  * [PromptStyler: Prompt-driven Style Generation for Source-free Domain Generalization](http://arxiv.org/abs/2307.15199v1)<br>:house:[project](https://promptstyler.github.io/)
  * [Generalizable Decision Boundaries: Dualistic Meta-Learning for Open Set Domain Generalization](http://arxiv.org/abs/2308.09391v1)<br>:star:[code](https://github.com/zzwdx/MEDIC)
* é›¶æ ·æœ¬å­¦ä¹ 
  * [Hyperbolic Audio-visual Zero-shot Learning](http://arxiv.org/abs/2308.12558v1)
  * [Continual Zero-Shot Learning through Semantically Guided Generative Random Walks](http://arxiv.org/abs/2308.12366v1)<br>:star:[code](https://github.com/wx-zhang/IGCZSL)
  * [Hierarchical Visual Primitive Experts for Compositional Zero-Shot Learning](http://arxiv.org/abs/2308.04016v1)<br>:star:[code](https://github.com/HanjaeKim98/CoT)
  * [Distilled Reverse Attention Network for Open-world Compositional Zero-Shot Learning](http://arxiv.org/abs/2303.00404)
* å°æ ·æœ¬å­¦ä¹ 
  * [Prototypes-oriented Transductive Few-shot Learning with Conditional Transport](http://arxiv.org/abs/2308.03047v1)
  * [CDFSL-V: Cross-Domain Few-Shot Learning for Videos](http://arxiv.org/abs/2309.03989v1)<br>:star:[code](https://github.com/Sarinda251/CDFSL-V)
  * [Read-only Prompt Optimization for Vision-Language Few-shot Learning](http://arxiv.org/abs/2308.14960v1)<br>:star:[code](https://github.com/mlvlab/RPO)
  * [Task-aware Adaptive Learning for Cross-domain Few-shot Learning](https://openaccess.thecvf.com/content/ICCV2023/papers/Guo_Task-aware_Adaptive_Learning_for_Cross-domain_Few-shot_Learning_ICCV_2023_paper.pdf)
  * [DETA: Denoised Task Adaptation for Few-Shot Learning](http://arxiv.org/abs/2303.06315)

<a name="23"/>

## 23.Optical Flow Estimation(å…‰æµä¼°è®¡)
* [GAFlow: Incorporating Gaussian Attention into Optical Flow](https://openaccess.thecvf.com/content/ICCV2023/papers/Luo_GAFlow_Incorporating_Gaussian_Attention_into_Optical_Flow_ICCV_2023_paper.pdf)
* [SemARFlow: Injecting Semantics into Unsupervised Optical Flow Estimation for Autonomous Driving](http://arxiv.org/abs/2303.06209)
* [Explicit Motion Disentangling for Efficient Optical Flow Estimation]æœªå…¬å¼€
* [VideoFlow: Exploiting Temporal Cues for Multi-frame Optical Flow Estimation](https://arxiv.org/abs/2303.08340)<br>:star:[code](https://github.com/XiaoyuShi97/VideoFlow)<br>:thumbsup:[ICCV2023|æ¸¯ä¸­æ–‡MMLabæå‡ºå¤šå¸§å…‰æµä¼°è®¡æ¨¡å‹VideoFlowï¼Œå……åˆ†æŒ–æ˜æ—¶åºçº¿ç´¢ï¼ŒSintelä¸KITTIæ¦œå•æ’åç¬¬ä¸€](https://mp.weixin.qq.com/s/jsHDk055nSCmkJ8TXch_Lg)
* [MPI-Flow: Learning Realistic Optical Flow with Multiplane Images](http://arxiv.org/abs/2309.06714v1)<br>:star:[code](https://github.com/Sharpiless/MPI-Flow)<br>:thumbsup:[ä»å¤šå¹³é¢å›¾åƒä¸­å­¦ä¹ æ›´çœŸå®çš„å…‰æµ](https://mp.weixin.qq.com/s/IUTfQo6FJoB14oZFyyeVmQ)
* [RPEFlow: Multimodal Fusion of RGB-PointCloud-Event for Joint Optical Flow and Scene Flow Estimation](http://arxiv.org/abs/2309.15082v1)<br>:star:[code](https://github.com/danqu130/RPEFlow)<br>:star:[code](https://npucvr.github.io/RPEFlow)
* [Event-based Temporally Dense Optical Flow Estimation with Sequential Learning](https://openaccess.thecvf.com/content/ICCV2023/papers/Ponghiran_Event-based_Temporally_Dense_Optical_Flow_Estimation_with_Sequential_Learning_ICCV_2023_paper.pdf)
* [TMA: Temporal Motion Aggregation for Event-based Optical Flow](http://arxiv.org/abs/2303.11629)
* [Taming Contrast Maximization for Learning Sequential, Low-latency, Event-based Optical Flow](https://openaccess.thecvf.com/content/ICCV2023/papers/Paredes-Valles_Taming_Contrast_Maximization_for_Learning_Sequential_Low-latency_Event-based_Optical_Flow_ICCV_2023_paper.pdf)

<a name="22"/>

## 22.OCR
* [ChartReader: A Unified Framework for Chart Derendering and Comprehension without Heuristic Rules](http://arxiv.org/abs/2304.02173)
* [Self-Supervised Character-to-Character Distillation for Text Recognition](http://arxiv.org/abs/2211.00288)
* [Vision Grid Transformer for Document Layout Analysis](http://arxiv.org/abs/2308.14978v1)<br>:star:[code](https://github.com/AlibabaResearch/AdvancedLiterateMachinery)
* [CNN based Cuneiform Sign Detection Learned from Annotated 3D Renderings and Mapped Photographs with Illumination Augmentation](http://arxiv.org/abs/2308.11277v1)<br>:house:[project](https://gigamesh.eu)
* [ESTextSpotter: Towards Better Scene Text Spotting with Explicit Synergy in Transformer](http://arxiv.org/abs/2308.10147v1)<br>:star:[code](https://github.com/mxin262/ESTextSpotter)
* [DocTr: Document Transformer for Structured Information Extraction in Documents](http://arxiv.org/abs/2307.07929)
* åœºæ™¯æ–‡æœ¬è¯†åˆ«
  * [LISTER: Neighbor Decoding for Length-Insensitive Scene Text Recognition](http://arxiv.org/abs/2308.12774v1)
* ä¸­æ–‡æ–‡æœ¬è¯†åˆ«
  * [Chinese Text Recognition with A Pre-Trained CLIP-Like Model Through Image-IDS Aligning](http://arxiv.org/abs/2309.01083v1)<br>:star:[code](https://github.com/FudanVI/FudanOCR/tree/main/image-ids-CTR)
* æ–‡æ¡£å›¾åƒæ ¡æ­£
  * [Foreground and Text-lines Aware Document Image Rectification](https://openaccess.thecvf.com/content/ICCV2023/papers/Li_Foreground_and_Text-lines_Aware_Document_Image_Rectification_ICCV_2023_paper.pdf)
* æ–‡æ¡£ç†è§£
  * [Attention Where It Matters: Rethinking Visual Document Understanding with Selective Region Concentration](http://arxiv.org/abs/2309.01131v1)
  * [SCOB: Universal Text Understanding via Character-wise Supervised Contrastive Learning with Online Text Rendering for Bridging Domain Gap](http://arxiv.org/abs/2309.12382v1)<br>:star:[code](https://github.com/naver-ai/scob)
* å­—ä½“ç”Ÿæˆ
  * [Few shot font generation via transferring similarity guided global style and quantization local style](http://arxiv.org/abs/2309.00827v1)<br>:star:[code](https://github.com/awei669/VQ-Font)
* å®ä½“è¯†åˆ«
  * [Open-domain Visual Entity Recognition: Towards Recognizing Millions of Wikipedia Entities](http://arxiv.org/abs/2302.11154)
* æ‰‹å†™æ‰“å°æ–‡æœ¬åˆ†å‰²
  * [Handwritten and Printed Text Segmentation: A Signature Case Study](http://arxiv.org/abs/2307.07887)

<a name="21"/>

## 21.Point Cloud(ç‚¹äº‘)
* [Self-Ordering Point Clouds](http://arxiv.org/abs/2304.00961)
* [Efficient LiDAR Point Cloud Oversegmentation Network](https://openaccess.thecvf.com/content/ICCV2023/papers/Hui_Efficient_LiDAR_Point_Cloud_Oversegmentation_Network_ICCV_2023_paper.pdf)
* [Attention Discriminant Sampling for Point Clouds](https://openaccess.thecvf.com/content/ICCV2023/papers/Hong_Attention_Discriminant_Sampling_for_Point_Clouds_ICCV_2023_paper.pdf)
* [CO-PILOT: Dynamic Top-Down Point Cloud with Conditional Neighborhood Aggregation for Multi-Gigapixel Histopathology Image Representation](https://openaccess.thecvf.com/content/ICCV2023/papers/Nakhli_CO-PILOT_Dynamic_Top-Down_Point_Cloud_with_Conditional_Neighborhood_Aggregation_for_ICCV_2023_paper.pdf)
* [Instance-aware Dynamic Prompt Tuning for Pre-trained Point Cloud Models](https://openaccess.thecvf.com/content/ICCV2023/papers/Zha_Instance-aware_Dynamic_Prompt_Tuning_for_Pre-trained_Point_Cloud_Models_ICCV_2023_paper.pdf)
* [Ponder: Point Cloud Pre-training via Neural Rendering](http://arxiv.org/abs/2301.00157)
* [CO-Net: Learning Multiple Point Cloud Tasks at Once with A Cohesive Network](https://openaccess.thecvf.com/content/ICCV2023/papers/Xie_CO-Net_Learning_Multiple_Point_Cloud_Tasks_at_Once_with_A_ICCV_2023_paper.pdf)
* [SVDFormer: Complementing Point Cloud via Self-view Augmentation and Self-structure Dual-generator](http://arxiv.org/abs/2307.08492)
* [EPiC: Ensemble of Partial Point Clouds for Robust Classification](http://arxiv.org/abs/2303.11419)
* [Point Contrastive Prediction with Semantic Clustering for Self-Supervised Learning on Point Cloud Videos](http://arxiv.org/abs/2308.09247v1)
* [Masked Spatio-Temporal Structure Prediction for Self-supervised Learning on Point Cloud Videos](http://arxiv.org/abs/2308.09245v1)
* [SC3K: Self-supervised and Coherent 3D Keypoints Estimation from Rotated, Noisy, and Decimated Point Cloud Data](http://arxiv.org/abs/2308.05410v1)<br>:star:[code](https://github.com/IITPAVIS/SC3K)
* [2D3D-MATR: 2D-3D Matching Transformer for Detection-free Registration between Images and Point Clouds](http://arxiv.org/abs/2308.05667v1)<br>:star:[code](https://github.com/minhaolee/2D3DMATR)
* [DELFlow: Dense Efficient Learning of Scene Flow for Large-Scale Point Clouds](http://arxiv.org/abs/2308.04383v1)<br>:star:[code](https://github.com/IRMVLab/DELFlow)
* [Sketch and Text Guided Diffusion Model for Colored Point Cloud Generation](http://arxiv.org/abs/2308.02874v1)
* [Clustering based Point Cloud Representation Learning for 3D Analysis](http://arxiv.org/abs/2307.14605v1)<br>:star:[code](https://github.com/FengZicai/Cluster3Dseg/)
* [Take-A-Photo: 3D-to-2D Generative Pre-training of Point Cloud Models](http://arxiv.org/abs/2307.14971v1)<br>:house:[project](https://tap.ivg-research.xyz)<br>:star:[code](https://github.com/wangzy22/TAP)
* [PC-Adapter: Topology-Aware Adapter for Efficient Domain Adaption on Point Clouds with Rectified Pseudo-label](https://openaccess.thecvf.com/content/ICCV2023/papers/Park_PC-Adapter_Topology-Aware_Adapter_for_Efficient_Domain_Adaption_on_Point_Clouds_ICCV_2023_paper.pdf)
* [Invariant Training 2D-3D Joint Hard Samples for Few-Shot Point Cloud Recognition](http://arxiv.org/abs/2308.09694)
* ç‚¹äº‘é…å‡†
  * [Density-invariant Features for Distant Point Cloud Registration](http://arxiv.org/abs/2307.09788v1)
  * [Rethinking Point Cloud Registration as Masking and Reconstruction](https://openaccess.thecvf.com/content/ICCV2023/papers/Chen_Rethinking_Point_Cloud_Registration_as_Masking_and_Reconstruction_ICCV_2023_paper.pdf)
  * [Point-TTA: Test-Time Adaptation for Point Cloud Registration Using Multitask Meta-Auxiliary Learning](https://openaccess.thecvf.com/content/ICCV2023/papers/Hatem_Point-TTA_Test-Time_Adaptation_for_Point_Cloud_Registration_Using_Multitask_Meta-Auxiliary_ICCV_2023_paper.pdf)
  * [SIRA-PCR: Sim-to-Real Adaptation for 3D Point Cloud Registration]æœªå…¬å¼€
  * [PointMBF: A Multi-scale Bidirectional Fusion Network for Unsupervised RGB-D Point Cloud Registration](http://arxiv.org/abs/2308.04782v1)<br>:star:[code](https://github.com/phdymz/PointMBF)
  * [Sample-adaptive Augmentation for Point Cloud Recognition Against Real-world Corruptions](http://arxiv.org/abs/2309.10431v1)<br>:star:[code](https://github.com/Roywangj/AdaptPoint)
  * [AutoSynth: Learning to Generate 3D Training Data for Object Point Cloud Registration](http://arxiv.org/abs/2309.11170v1)
  * [RegFormer: An Efficient Projection-Aware Transformer Network for Large-Scale Point Cloud Registration](http://arxiv.org/abs/2303.12384)
  * [Chasing Clouds: Differentiable Volumetric Rasterisation of Point Clouds as a Highly Efficient and Accurate Loss for Large-Scale Deformable 3D Registration](https://openaccess.thecvf.com/content/ICCV2023/papers/Heinrich_Chasing_Clouds_Differentiable_Volumetric_Rasterisation_of_Point_Clouds_as_a_ICCV_2023_paper.pdf)
* ç‚¹äº‘åˆ†å‰²
  * [See More and Know More: Zero-shot Point Cloud Segmentation via Multi-modal Visual Data](http://arxiv.org/abs/2307.10782v1)
  * [Generalized Few-Shot Point Cloud Segmentation via Geometric Words](http://arxiv.org/abs/2309.11222)
  * [2D-3D Interlaced Transformer for Point Cloud Segmentation with Scene-Level Supervision](https://openaccess.thecvf.com/content/ICCV2023/papers/Yang_2D-3D_Interlaced_Transformer_for_Point_Cloud_Segmentation_with_Scene-Level_Supervision_ICCV_2023_paper.pdf)
  * [ProtoTransfer: Cross-Modal Prototype Transfer for Point Cloud Segmentation](https://openaccess.thecvf.com/content/ICCV2023/papers/Tang_ProtoTransfer_Cross-Modal_Prototype_Transfer_for_Point_Cloud_Segmentation_ICCV_2023_paper.pdf)
  * [Zero-Shot Point Cloud Segmentation by Semantic-Visual Aware Synthesis](https://openaccess.thecvf.com/content/ICCV2023/papers/Yang_Zero-Shot_Point_Cloud_Segmentation_by_Semantic-Visual_Aware_Synthesis_ICCV_2023_paper.pdf)
  * [CPCM: Contextual Point Cloud Modeling for Weakly-supervised Point Cloud Semantic Segmentation](http://arxiv.org/abs/2307.10316v1)
  * [GaPro: Box-Supervised 3D Point Cloud Instance Segmentation Using Gaussian Processes as Pseudo Labelers](http://arxiv.org/abs/2307.13251v1)<br>:star:[code](https://github.com/VinAIResearch/GaPro)
  * [Hierarchical Point-based Active Learning for Semi-supervised Point Cloud Semantic Segmentation](http://arxiv.org/abs/2308.11166v1)<br>:star:[code](https://github.com/SmiletoE/HPAL)
  * [Generalized Few-Shot Point Cloud Segmentation Via Geometric Words](http://arxiv.org/abs/2309.11222v1)<br>:star:[code](https://github.com/Pixie8888/GFS-3DSeg_GWs)
* ç‚¹äº‘è¡¥å…¨
  * [P2C: Self-Supervised Point Cloud Completion from Single Partial Clouds](http://arxiv.org/abs/2307.14726v1)<br>:star:[code](https://github.com/CuiRuikai/Partial2Complete)
  * [Hyperbolic Chamfer Distance for Point Cloud Completion](https://openaccess.thecvf.com/content/ICCV2023/papers/Lin_Hyperbolic_Chamfer_Distance_for_Point_Cloud_Completion_ICCV_2023_paper.pdf)
  * [VAPCNet: Viewpoint-Aware 3D Point Cloud Completion](https://openaccess.thecvf.com/content/ICCV2023/papers/Fu_VAPCNet_Viewpoint-Aware_3D_Point_Cloud_Completion_ICCV_2023_paper.pdf)
* ç‚¹äº‘åˆ†ç±»
  * [CLIP2Point: Transfer CLIP to Point Cloud Classification with Image-Depth Pre-Training](http://arxiv.org/abs/2210.01055)
* 3Dç‚¹äº‘
  * [3DHacker: Spectrum-based Decision Boundary Generation for Hard-label 3D Point Cloud Attack](http://arxiv.org/abs/2308.07546v1)
  * [GridPull: Towards Scalability in Learning Implicit Representations from 3D Point Clouds](http://arxiv.org/abs/2308.13175v1)<br>:star:[code](https://github.com/chenchao15/GridPull)
  * [DiffFacto: Controllable Part-Based 3D Point Cloud Generation with Cross Diffusion](http://arxiv.org/abs/2305.01921)
  * [GeoUDF: Surface Reconstruction from 3D Point Clouds via Geometry-guided Distance Representation](http://arxiv.org/abs/2211.16762)
* 4D ç‚¹äº‘
  * [LeaF: Learning Frames for 4D Point Cloud Sequence Understanding](https://openaccess.thecvf.com/content/ICCV2023/papers/Liu_LeaF_Learning_Frames_for_4D_Point_Cloud_Sequence_Understanding_ICCV_2023_paper.pdf)

<a name="20"/>

## 20.Reid(äººå‘˜é‡è¯†åˆ«/æ­¥æ€è¯†åˆ«/è¡Œäººæ£€æµ‹)
* äººå‘˜æœç´¢
  * [Self-similarity Driven Scale-invariant Learning for Weakly Supervised Person Search](http://arxiv.org/abs/2302.12986)
* Reid
  * [Identity-Seeking Self-Supervised Representation Learning for Generalizable Person Re-identification](http://arxiv.org/abs/2308.08887v1)<br>:star:[code](https://github.com/dcp15/ISR_ICCV2023_Oral)
  * [Part-Aware Transformer for Generalizable Person Re-identification](http://arxiv.org/abs/2308.03322)
  * [Unified Pre-training with Pseudo Texts for Text-To-Image Person Re-identification](http://arxiv.org/abs/2309.01420v1)
  * [Camera-Driven Representation Learning for Unsupervised Domain Adaptive Person Re-identification](http://arxiv.org/abs/2308.11901v1)
  * [Discrepant and Multi-Instance Proxies for Unsupervised Person Re-Identification](https://openaccess.thecvf.com/content/ICCV2023/papers/Zou_Discrepant_and_Multi-Instance_Proxies_for_Unsupervised_Person_Re-Identification_ICCV_2023_paper.pdf)
  * [Learning Clothing and Pose Invariant 3D Shape Representation for Long-Term Person Re-Identification](http://arxiv.org/abs/2308.10658v1)
  * [Identity-Seeking Self-Supervised Representation Learning for Generalizable Person Re-Identification](http://arxiv.org/abs/2308.08887)
  * æ¢è¡£é‡è¯†åˆ«
    * [DeepChange: A Long-Term Person Re-Identification Benchmark with Clothes Change](https://openaccess.thecvf.com/content/ICCV2023/papers/Xu_DeepChange_A_Long-Term_Person_Re-Identification_Benchmark_with_Clothes_Change_ICCV_2023_paper.pdf)
  * å¯è§å…‰çº¢å¤–é‡è¯†åˆ«
    * [Modality Unifying Network for Visible-Infrared Person Re-Identification](http://arxiv.org/abs/2309.06262v1)
    * [Towards Grand Unified Representation Learning for Unsupervised Visible-Infrared Person Re-Identification](https://openaccess.thecvf.com/content/ICCV2023/papers/Yang_Towards_Grand_Unified_Representation_Learning_for_Unsupervised_Visible-Infrared_Person_Re-Identification_ICCV_2023_paper.pdf)
    * [Visible-Infrared Person Re-Identification via Semantic Alignment and Affinity Inference](https://openaccess.thecvf.com/content/ICCV2023/papers/Fang_Visible-Infrared_Person_Re-Identification_via_Semantic_Alignment_and_Affinity_Inference_ICCV_2023_paper.pdf)
    * [Learning Concordant Attention via Target-aware Alignment for Visible-Infrared Person Re-identification](https://openaccess.thecvf.com/content/ICCV2023/papers/Wu_Learning_Concordant_Attention_via_Target-aware_Alignment_for_Visible-Infrared_Person_Re-identification_ICCV_2023_paper.pdf)
  * æ–‡æœ¬-å›¾åƒé‡è¯†åˆ«
    * [Unified Pre-Training with Pseudo Texts for Text-To-Image Person Re-Identification](http://arxiv.org/abs/2309.01420)
* æ­¥æ€è¯†åˆ«
  * [Hierarchical Spatio-Temporal Representation Learning for Gait Recognition](http://arxiv.org/abs/2307.09856v1)
  * [Fine-grained Unsupervised Domain Adaptation for Gait Recognition](https://openaccess.thecvf.com/content/ICCV2023/papers/Ma_Fine-grained_Unsupervised_Domain_Adaptation_for_Gait_Recognition_ICCV_2023_paper.pdf)
  * [DyGait: Exploiting Dynamic Representations for High-performance Gait Recognition](http://arxiv.org/abs/2303.14953)
  * åŸºäºå§¿åŠ¿
    * [GPGait: Generalized Pose-based Gait Recognition](http://arxiv.org/abs/2303.05234) 
  * åŸºäº3Déª¨æ¶
    * [Physics-Augmented Autoencoder for 3D Skeleton-Based Gait Recognition](https://openaccess.thecvf.com/content/ICCV2023/papers/Guo_Physics-Augmented_Autoencoder_for_3D_Skeleton-Based_Gait_Recognition_ICCV_2023_paper.pdf)
* äººç¾¤è®¡æ•°
  * [Calibrating Uncertainty for Semi-Supervised Crowd Counting](http://arxiv.org/abs/2308.09887v1)
  * [Point-Query Quadtree for Crowd Counting, Localization, and More](http://arxiv.org/abs/2308.13814v1)<br>:star:[code](https://github.com/cxliu0/PET)

<a name="19"/>

## 19.UAV/Remote Sensing/Satellite Image(æ— äººæœº/é¥æ„Ÿ/å«æ˜Ÿå›¾åƒ)
* [View Consistent Purification for Accurate Cross-View Localization](http://arxiv.org/abs/2308.08110v1)
* [Class Prior-Free Positive-Unlabeled Learning with Taylor Variational Loss for Hyperspectral Remote Sensing Imagery](http://arxiv.org/abs/2308.15081v1)<br>:star:[code](https://github.com/Hengwei-Zhao96/T-HOneCls)
* é¥æ„Ÿå›¾åƒåˆ†å‰²
  * [Seeing Beyond the Patch: Scale-Adaptive Semantic Segmentation of High-resolution Remote Sensing Imagery based on Reinforcement Learning](https://openaccess.thecvf.com/content/ICCV2023/papers/Liu_Seeing_Beyond_the_Patch_Scale-Adaptive_Semantic_Segmentation_of_High-resolution_Remote_ICCV_2023_paper.pdf)
* é¥æ„Ÿå›¾åƒç†è§£
  * [SatlasPretrain: A Large-Scale Dataset for Remote Sensing Image Understanding](http://arxiv.org/abs/2211.15660)
* æ— äººæœºè·Ÿè¸ª
  * [Adaptive and Background-Aware Vision Transformer for Real-Time UAV Tracking](https://openaccess.thecvf.com/content/ICCV2023/papers/Li_Adaptive_and_Background-Aware_Vision_Transformer_for_Real-Time_UAV_Tracking_ICCV_2023_paper.pdf)
* å˜åŒ–æ£€æµ‹
  * [MapFormer: Boosting Change Detection by Using Pre-change Information](https://openaccess.thecvf.com/content/ICCV2023/papers/Bernhard_MapFormer_Boosting_Change_Detection_by_Using_Pre-change_Information_ICCV_2023_paper.pdf)

<a name="18"/>

## 18.Human Pose Estimation
* [Plausible Uncertainties for Human Pose Regression](https://openaccess.thecvf.com/content/ICCV2023/papers/Bramlage_Plausible_Uncertainties_for_Human_Pose_Regression_ICCV_2023_paper.pdf)
* [BaRe-ESA: A Riemannian Framework for Unregistered Human Body Shapes](https://openaccess.thecvf.com/content/ICCV2023/papers/Hartman_BaRe-ESA_A_Riemannian_Framework_for_Unregistered_Human_Body_Shapes_ICCV_2023_paper.pdf)
* [Algebraically Rigorous Quaternion Framework for the Neural Network Pose Estimation Problem](https://openaccess.thecvf.com/content/ICCV2023/papers/Lin_Algebraically_Rigorous_Quaternion_Framework_for_the_Neural_Network_Pose_Estimation_ICCV_2023_paper.pdf)
* äººä½“å§¿æ€ä¼°è®¡
  * [DiffPose: SpatioTemporal Diffusion Model for Video-Based Human Pose Estimation](http://arxiv.org/abs/2307.16687v1)
  * [SEFD: Learning to Distill Complex Pose and Occlusion](https://openaccess.thecvf.com/content/ICCV2023/papers/Yang_SEFD_Learning_to_Distill_Complex_Pose_and_Occlusion_ICCV_2023_paper.pdf)
  * [DiffPose: Multi-hypothesis Human Pose Estimation using Diffusion Models](http://arxiv.org/abs/2211.16487)
  * [Source-free Domain Adaptive Human Pose Estimation](http://arxiv.org/abs/2308.03202v1)
  * [Prior-guided Source-free Domain Adaptation for Human Pose Estimation](http://arxiv.org/abs/2308.13954v1)
  * [TEMPO: Efficient Multi-View Pose Estimation, Tracking, and Forecasting](http://arxiv.org/abs/2309.07910v1)
  * [MHEntropy: Entropy Meets Multiple Hypotheses for Pose and Shape Recovery](https://openaccess.thecvf.com/content/ICCV2023/papers/Chen_MHEntropy_Entropy_Meets_Multiple_Hypotheses_for_Pose_and_Shape_Recovery_ICCV_2023_paper.pdf)
  * [MixSynthFormer: A Transformer Encoder-like Structure with Mixed Synthetic Self-attention for Efficient Human Pose Estimation](https://openaccess.thecvf.com/content/ICCV2023/papers/Sun_MixSynthFormer_A_Transformer_Encoder-like_Structure_with_Mixed_Synthetic_Self-attention_for_ICCV_2023_paper.pdf)
* å¤šäººå§¿æ€ä¼°è®¡
  * [Group Pose: A Simple Baseline for End-to-End Multi-person Pose Estimation](http://arxiv.org/abs/2308.07313v1)<br>:star:[code](https://github.com/Michel-liu/GroupPose-Paddle)<br>:star:[code](https://github.com/Michel-liu/GroupPose)
* 3Däººä½“å§¿æ€ä¼°è®¡
  * [3D-Aware Neural Body Fitting for Occlusion Robust 3D Human Pose Estimation](http://arxiv.org/abs/2308.10123v1)<br>:star:[code](https://github.com/edz-o/3DNBF)<br>:star:[code](https://3dnbf.github.io/)
  * [Diffusion-Based 3D Human Pose Estimation with Multi-Hypothesis Aggregation](http://arxiv.org/abs/2303.11579)
  * [Global Adaptation Meets Local Generalization: Unsupervised Domain Adaptation for 3D Human Pose Estimation](http://arxiv.org/abs/2303.16456)
  * [PhaseMP: Robust 3D Pose Estimation via Phase-conditioned Human Motion Prior](https://openaccess.thecvf.com/content/ICCV2023/papers/Shi_PhaseMP_Robust_3D_Pose_Estimation_via_Phase-conditioned_Human_Motion_Prior_ICCV_2023_paper.pdf)
  * [Test-time Personalizable Forecasting of 3D Human Poses](https://openaccess.thecvf.com/content/ICCV2023/papers/Cui_Test-time_Personalizable_Forecasting_of_3D_Human_Poses_ICCV_2023_paper.pdf)<br>:thumbsup:[é¢å‘åˆ†å¸ƒå¤–è§’è‰²çš„ä¸ªæ€§åŒ–äººä½“è¿åŠ¨é¢„æµ‹](https://mp.weixin.qq.com/s/IUTfQo6FJoB14oZFyyeVmQ)
  * [Probabilistic Triangulation for Uncalibrated Multi-View 3D Human Pose Estimation](http://arxiv.org/abs/2309.04756)
  * [HopFIR: Hop-wise GraphFormer with Intragroup Joint Refinement for 3D Human Pose Estimation](http://arxiv.org/abs/2302.14581)
  * [Co-Evolution of Pose and Mesh for 3D Human Body Estimation from Video](http://arxiv.org/abs/2308.10305v1)<br>:star:[code](https://kasvii.github.io/PMCE)<br>:star:[code](https://github.com/kasvii/PMCE)
  * [EMDB: The Electromagnetic Database of Global 3D Human Pose and Shape in the Wild](http://arxiv.org/abs/2308.16894v1)<br>:house:[project](https://ait.ethz.ch/emdb)
  * [GLA-GCN: Global-local Adaptive Graph Convolutional Network for 3D Human Pose Estimation from Monocular Video](https://openaccess.thecvf.com/content/ICCV2023/papers/Yu_GLA-GCN_Global-local_Adaptive_Graph_Convolutional_Network_for_3D_Human_Pose_ICCV_2023_paper.pdf)
  * [PoseFix: Correcting 3D Human Poses with Natural Language](http://arxiv.org/abs/2309.08480v1)
  * [Towards Robust and Smooth 3D Multi-Person Pose Estimation from Monocular Videos in the Wild](http://arxiv.org/abs/2309.08644v1)<br>:house:[project](https://www.youtube.com/@potr3d)
* äººä½“å§¿æ€é¢„æµ‹
  * [HDG-ODE: A Hierarchical Continuous-Time Model for Human Pose Forecasting](https://openaccess.thecvf.com/content/ICCV2023/papers/Xing_HDG-ODE_A_Hierarchical_Continuous-Time_Model_for_Human_Pose_Forecasting_ICCV_2023_paper.pdf)
* äººä½“ç½‘æ ¼æ¢å¤
  * [JOTR: 3D Joint Contrastive Learning with Transformers for Occluded Human Mesh Recovery](http://arxiv.org/abs/2307.16377v1)
  * [Zolly: Zoom Focal Length Correctly for Perspective-Distorted Human Mesh Reconstruction](http://arxiv.org/abs/2303.13796)
  * [TORE: Token Reduction for Efficient Human Mesh Recovery with Transformer](http://arxiv.org/abs/2211.10705)
  * [Distribution-Aligned Diffusion for Human Mesh Recovery](http://arxiv.org/abs/2308.13369v1)<br>:star:[code](https://gongjia0208.github.io/HMDiff/)
  * [Cloth2Body: Generating 3D Human Body Mesh from 2D Clothing](https://openaccess.thecvf.com/content/ICCV2023/papers/Dai_Cloth2Body_Generating_3D_Human_Body_Mesh_from_2D_Clothing_ICCV_2023_paper.pdf)
  * [3D Human Mesh Recovery with Sequentially Global Rotation Estimation](https://openaccess.thecvf.com/content/ICCV2023/papers/Wang_3D_Human_Mesh_Recovery_with_Sequentially_Global_Rotation_Estimation_ICCV_2023_paper.pdf)
* å¤šäººç½‘æ ¼æ¢å¤
  * [Coordinate Transformer: Achieving Single-stage Multi-person Mesh Recovery from Videos](http://arxiv.org/abs/2308.10334v1)<br>:star:[code](https://github.com/Li-Hao-yuan/CoordFormer)
* 3Däººä½“æ¢å¤
  * [ReFit: Recurrent Fitting Network for 3D Human Recovery](http://arxiv.org/abs/2308.11184v1)<br>:star:[code](https://yufu-wang.github.io/refit_humans/)
* å§¿åŠ¿è¿ç§»
  * [Weakly-supervised 3D Pose Transfer with Keypoints](http://arxiv.org/abs/2307.13459v1)
  * [WaveIPT: Joint Attention and Flow Alignment in the Wavelet domain for Pose Transfer](https://openaccess.thecvf.com/content/ICCV2023/papers/Ma_WaveIPT_Joint_Attention_and_Flow_Alignment_in_the_Wavelet_domain_ICCV_2023_paper.pdf)
  * [Collecting The Puzzle Pieces: Disentangled Self-Driven Human Pose Transfer by Permuting Textures](http://arxiv.org/abs/2210.01887)
  * [Bidirectionally Deformable Motion Modulation For Video-based Human Pose Transfer](http://arxiv.org/abs/2307.07754)
  * [MAPConNet: Self-supervised 3D Pose Transfer with Mesh and Point Contrastive Learning](http://arxiv.org/abs/2304.13819)
* ä¸‰ç»´äººä½“é‡å»º
  * [Body Knowledge and Uncertainty Modeling for Monocular 3D Human Body Reconstruction](http://arxiv.org/abs/2308.00799v1)Meta
  * [Chupa: Carving 3D Clothed Humans from Skinned Shape Priors using 2D Diffusion Probabilistic Models](http://arxiv.org/abs/2305.11870)
  * [Get3DHuman: Lifting StyleGAN-Human into a 3D Generative Model Using Pixel-Aligned Reconstruction Priors](http://arxiv.org/abs/2302.01162)
* ä¸‰ç»´äººä½“ç½‘æ ¼é‡å»º
  * [Cyclic Test-Time Adaptation on Monocular Video for 3D Human Mesh Reconstruction](http://arxiv.org/abs/2308.06554v1)<br>:star:[code](https://github.com/hygenie1228/CycleAdapt_RELEASE)
* æ‰‹éƒ¨å§¿åŠ¿ä¼°è®¡
  * [Deformer: Dynamic Fusion Transformer for Robust Hand Pose Estimation](http://arxiv.org/abs/2303.04991)
  * [HaMuCo: Hand Pose Estimation via Multiview Collaborative Self-Supervised Learning](http://arxiv.org/abs/2302.00988)
* 3Dæ‰‹éƒ¨å§¿æ€ä¼°è®¡
  * [OCHID-Fi: Occlusion-Robust Hand Pose Estimation in 3D via RF-Vision](http://arxiv.org/abs/2308.10146v1)
  * [RenderIH: A Large-Scale Synthetic Dataset for 3D Interacting Hand Pose Estimation](http://arxiv.org/abs/2309.09301)
  * [HandR2N2: Iterative 3D Hand Pose Estimation Using a Residual Recurrent Neural Network](https://openaccess.thecvf.com/content/ICCV2023/papers/Cheng_HandR2N2_Iterative_3D_Hand_Pose_Estimation_Using_a_Residual_Recurrent_ICCV_2023_paper.pdf)
  * [RenderIH: A Large-scale Synthetic Dataset for 3D Interacting Hand Pose Estimation](http://arxiv.org/abs/2309.09301v1)<br>:star:[code](https://github.com/adwardlee/RenderIH)
* æ‰‹-ç‰©å»ºæ¨¡
  * [CHORD: Category-level Hand-held Object Reconstruction via Shape Deformation](http://arxiv.org/abs/2308.10574v1)<br>:star:[code](https://kailinli.github.io/CHORD)
  * [Dynamic Hyperbolic Attention Network for Fine Hand-object Reconstruction](http://arxiv.org/abs/2309.02965v1)
  * [Reconstructing Interacting Hands with Interaction Prior from Monocular Images](http://arxiv.org/abs/2308.14082v1)<br>:star:[code](https://github.com/binghui-z/InterPrior_pytorch)
* æ‰‹éƒ¨é‡å»º
  * [Spectral Graphormer: Spectral Graph-based Transformer for Egocentric Two-Hand Reconstruction using Multi-View Color Images](http://arxiv.org/abs/2308.11015v1)
  * [Decoupled Iterative Refinement Framework for Interacting Hands Reconstruction from a Single RGB Image](http://arxiv.org/abs/2302.02410)
* æ‰‹åŠ¿ç”Ÿæˆ
  * [LivelySpeaker: Towards Semantic-Aware Co-Speech Gesture Generation](http://arxiv.org/abs/2309.09294v1)
  * [Continual Learning for Personalized Co-speech Gesture Generation](https://openaccess.thecvf.com/content/ICCV2023/papers/Ahuja_Continual_Learning_for_Personalized_Co-speech_Gesture_Generation_ICCV_2023_paper.pdf)
* æ‰‹åŠ¿è¯†åˆ«
  * [Learning Robust Representations with Information Bottleneck and Memory Network for RGB-D-based Gesture Recognition](https://openaccess.thecvf.com/content/ICCV2023/papers/Li_Learning_Robust_Representations_with_Information_Bottleneck_and_Memory_Network_for_ICCV_2023_paper.pdf)
  * [Data-Free Class-Incremental Hand Gesture Recognition](https://openaccess.thecvf.com/content/ICCV2023/papers/Aich_Data-Free_Class-Incremental_Hand_Gesture_Recognition_ICCV_2023_paper.pdf)
* äººä½“åˆæˆ
  * [UnitedHuman: Harnessing Multi-Source Data for High-Resolution Human Generation](http://arxiv.org/abs/2309.14335v1)<br>:star:[code](https://unitedhuman.github.io/)<br>:star:[code](https://github.com/UnitedHuman/UnitedHuman)
  * [3DHumanGAN: 3D-Aware Human Image Generation with 3D Pose Mapping](http://arxiv.org/abs/2212.07378)
* 3D äººä½“è¿åŠ¨ç”Ÿæˆ
  * [Make-An-Animation: Large-Scale Text-conditional 3D Human Motion Generation](https://openaccess.thecvf.com/content/ICCV2023/papers/Azadi_Make-An-Animation_Large-Scale_Text-conditional_3D_Human_Motion_Generation_ICCV_2023_paper.pdf)
  * [ActFormer: A GAN-based Transformer towards General Action-Conditioned 3D Human Motion Generation](http://arxiv.org/abs/2203.07706)
  * [Synthesizing Diverse Human Motions in 3D Indoor Scenes](http://arxiv.org/abs/2305.12411)
  * [TMR: Text-to-Motion Retrieval Using Contrastive 3D Human Motion Synthesis](http://arxiv.org/abs/2305.00976)
  * [Guided Motion Diffusion for Controllable Human Motion Synthesis](https://openaccess.thecvf.com/content/ICCV2023/papers/Karunratanakul_Guided_Motion_Diffusion_for_Controllable_Human_Motion_Synthesis_ICCV_2023_paper.pdf)
  * [MotionBERT: A Unified Perspective on Learning Human Motion Representations](http://arxiv.org/abs/2210.06551)
  * [Fg-T2M: Fine-Grained Text-Driven Human Motion Generation via Diffusion Model](https://openaccess.thecvf.com/content/ICCV2023/papers/Wang_Fg-T2M_Fine-Grained_Text-Driven_Human_Motion_Generation_via_Diffusion_Model_ICCV_2023_paper.pdf)
* äººä½“å§¿æ€å¼‚å¸¸æ£€æµ‹
  * [Normalizing Flows for Human Pose Anomaly Detection](http://arxiv.org/abs/2211.10946)
* èˆè¹ˆç”Ÿæˆ
  * [FineDance: A Fine-grained Choreography Dataset for 3D Full Body Dance Generation](http://arxiv.org/abs/2212.03741)
  * [TM2D: Bimodality Driven 3D Dance Generation via Music-Text Integration](http://arxiv.org/abs/2304.02419)

<a name="17"/>

## 17.Generative Adversarial Network
* [LFS-GAN: Lifelong Few-Shot Image Generation](http://arxiv.org/abs/2308.11917v1)
* [What can Discriminator do? Towards Box-free Ownership Verification of Generative Adversarial Network](http://arxiv.org/abs/2307.15860v1)<br>:star:[code](https://github.com/AbstractTeen/gan_ownership_verification)
* [Improving Diversity in Zero-Shot GAN Adaptation with Semantic Variations](http://arxiv.org/abs/2308.10554v1)
* [LinkGAN: Linking GAN Latents to Pixels for Controllable Image Synthesis](http://arxiv.org/abs/2301.04604)
* [Robust One-Shot Face Video Re-enactment using Hybrid Latent Spaces of StyleGAN2](https://openaccess.thecvf.com/content/ICCV2023/papers/Oorloff_Robust_One-Shot_Face_Video_Re-enactment_using_Hybrid_Latent_Spaces_of_ICCV_2023_paper.pdf)
* [Smoothness Similarity Regularization for Few-Shot GAN Adaptation](http://arxiv.org/abs/2308.09717v1)
* [SIDGAN: High-Resolution Dubbed Video Generation via Shift-Invariant Learning](https://openaccess.thecvf.com/content/ICCV2023/papers/Muaz_SIDGAN_High-Resolution_Dubbed_Video_Generation_via_Shift-Invariant_Learning_ICCV_2023_paper.pdf)
* [Frequency-aware GAN for Adversarial Manipulation Generation](https://openaccess.thecvf.com/content/ICCV2023/papers/Zhu_Frequency-aware_GAN_for_Adversarial_Manipulation_Generation_ICCV_2023_paper.pdf)
* GAN é€†æ˜ å°„
  * [Diverse Inpainting and Editing with GAN Inversion](http://arxiv.org/abs/2307.15033v1)
  * [Make Encoder Great Again in 3D GAN Inversion through Geometry and Occlusion-Aware Encoding](http://arxiv.org/abs/2303.12326)

<a name="16"/>

## 16.Super-Resolution(è¶…åˆ†è¾¨ç‡)
* [Self-Supervised Burst Super-Resolution](https://openaccess.thecvf.com/content/ICCV2023/papers/Bhat_Self-Supervised_Burst_Super-Resolution_ICCV_2023_paper.pdf)
* [Who Are You Referring To? Coreference Resolution In Image Narrations](http://arxiv.org/abs/2211.14563)
* [ESSAformer: Efficient Transformer for Hyperspectral Image Super-resolution](http://arxiv.org/abs/2307.14010)
* [Spatially-Adaptive Feature Modulation for Efficient Image Super-Resolution](http://arxiv.org/abs/2302.13800)
* [CuNeRF: Cube-Based Neural Radiance Field for Zero-Shot Medical Image Arbitrary-Scale Super Resolution](http://arxiv.org/abs/2303.16242)
* [Content-Aware Local GAN for Photo-Realistic Super-Resolution](https://openaccess.thecvf.com/content/ICCV2023/papers/Park_Content-Aware_Local_GAN_for_Photo-Realistic_Super-Resolution_ICCV_2023_paper.pdf)
* [Boosting Single Image Super-Resolution via Partial Channel Shifting](https://openaccess.thecvf.com/content/ICCV2023/papers/Zhang_Boosting_Single_Image_Super-Resolution_via_Partial_Channel_Shifting_ICCV_2023_paper.pdf)
* [Reconstructed Convolution Module Based Look-Up Tables for Efficient Image Super-Resolution](http://arxiv.org/abs/2307.08544)
* [Learning Non-Local Spatial-Angular Correlation for Light Field Image Super-Resolution](http://arxiv.org/abs/2302.08058)
* [Iterative Soft Shrinkage Learning for Efficient Image Super-Resolution](http://arxiv.org/abs/2303.09650)
* [On the Effectiveness of Spectral Discriminators for Perceptual Quality Improvement](http://arxiv.org/abs/2307.12027v1)<br>:star:[code](https://github.com/Luciennnnnnn/DualFormer)
* [Dual Aggregation Transformer for Image Super-Resolution](http://arxiv.org/abs/2308.03364v1)<br>:star:[code](https://github.com/zhengchen1999/DAT)
* [Feature Modulation Transformer: Cross-Refinement of Global Representation via High-Frequency Prior for Image Super-Resolution](http://arxiv.org/abs/2308.05022v1)<br>:star:[code](https://github.com/AVC2-UESTC/CRAFT-SR.git)
* [MetaF2N: Blind Image Super-Resolution by Learning Efficient Model Adaptation from Faces](http://arxiv.org/abs/2309.08113v1)<br>:star:[code](https://github.com/yinzhicun/MetaF2N)
* [HSR-Diff: Hyperspectral Image Super-Resolution via Conditional Diffusion Models](https://openaccess.thecvf.com/content/ICCV2023/papers/Wu_HSR-Diff_Hyperspectral_Image_Super-Resolution_via_Conditional_Diffusion_Models_ICCV_2023_paper.pdf)
* [Lightweight Image Super-Resolution with Superpixel Token Interaction](https://openaccess.thecvf.com/content/ICCV2023/papers/Zhang_Lightweight_Image_Super-Resolution_with_Superpixel_Token_Interaction_ICCV_2023_paper.pdf)
* [MSRA-SR: Image Super-resolution Transformer with Multi-scale Shared Representation Acquisition](https://openaccess.thecvf.com/content/ICCV2023/papers/Zhou_MSRA-SR_Image_Super-resolution_Transformer_with_Multi-scale_Shared_Representation_Acquisition_ICCV_2023_paper.pdf)
* è§†é¢‘è¶…åˆ†è¾¨ç‡
  * [MoTIF: Learning Motion Trajectories with Local Implicit Neural Functions for Continuous Space-Time Video Super-Resolution](http://arxiv.org/abs/2307.07988)
  * [Learning Data-Driven Vector-Quantized Degradation Model for Animation Video Super-Resolution](http://arxiv.org/abs/2303.09826)
* åŸºäºå‚è€ƒçš„è¶…åˆ†è¾¨ç‡
  * [LMR: A Large-Scale Multi-Reference Dataset for Reference-Based Super-Resolution](http://arxiv.org/abs/2303.04970)
* å›¾åƒé‡ç¼©æ”¾
  * [Downscaled Representation Matters: Improving Image Rescaling with Collaborative Downscaled Images](http://arxiv.org/abs/2211.10643)

<a name="15"/>

## 15.Image/Video Retrieval(å›¾åƒ/è§†é¢‘æ£€ç´¢)
* [Zero-Shot Composed Image Retrieval with Textual Inversion](http://arxiv.org/abs/2303.15247)
* [Democratising 2D Sketch to 3D Shape Retrieval Through Pivoting](https://openaccess.thecvf.com/content/ICCV2023/papers/Chowdhury_Democratising_2D_Sketch_to_3D_Shape_Retrieval_Through_Pivoting_ICCV_2023_paper.pdf)
* [Learning Spatial-context-aware Global Visual Feature Representation for Instance Image Retrieval](https://openaccess.thecvf.com/content/ICCV2023/papers/Zhang_Learning_Spatial-context-aware_Global_Visual_Feature_Representation_for_Instance_Image_Retrieval_ICCV_2023_paper.pdf)
* [U-RED: Unsupervised 3D Shape Retrieval and Deformation for Partial Point Clouds](http://arxiv.org/abs/2308.06383v1)
* [DeDrift: Robust Similarity Search under Content Drift](http://arxiv.org/abs/2308.02752v1)
* [FashionNTM: Multi-turn Fashion Image Retrieval via Cascaded Memory](http://arxiv.org/abs/2308.10170v1)<br>:house:[project](https://sites.google.com/eng.ucsd.edu/fashionntm)
* [Coarse-to-Fine: Learning Compact Discriminative Representation for Single-Stage Image Retrieval](http://arxiv.org/abs/2308.04008v1)<br>:star:[code](https://github.com/bassyess/CFCD)
* [Global Features are All You Need for Image Retrieval and Reranking](http://arxiv.org/abs/2308.06954v1)<br>:star:[code](https://github.com/ShihaoShao-GH/SuperGlobal)
* [Unsupervised Feature Representation Learning for Domain-generalized Cross-domain Image Retrieval](https://openaccess.thecvf.com/content/ICCV2023/papers/Hu_Unsupervised_Feature_Representation_Learning_for_Domain-generalized_Cross-domain_Image_Retrieval_ICCV_2023_paper.pdf)
* [Towards Content-based Pixel Retrieval in Revisited Oxford and Paris](http://arxiv.org/abs/2309.05438)
* [Fan-Beam Binarization Difference Projection (FB-BDP): A Novel Local Object Descriptor for Fine-Grained Leaf Image Retrieval](https://openaccess.thecvf.com/content/ICCV2023/papers/Chen_Fan-Beam_Binarization_Difference_Projection_FB-BDP_A_Novel_Local_Object_Descriptor_ICCV_2023_paper.pdf)
* å›¾åƒ-æ–‡æœ¬æ£€ç´¢
  * [LexLIP: Lexicon-Bottlenecked Language-Image Pre-Training for Large-Scale Image-Text Sparse Retrieval](https://openaccess.thecvf.com/content/ICCV2023/papers/Luo_LexLIP_Lexicon-Bottlenecked_Language-Image_Pre-Training_for_Large-Scale_Image-Text_Sparse_Retrieval_ICCV_2023_paper.pdf)
* æ–‡æœ¬-è§†é¢‘æ£€ç´¢
  * [Helping Hands: An Object-Aware Ego-Centric Video Recognition Model](http://arxiv.org/abs/2308.07918v1)
  * [Prompt Switch: Efficient CLIP Adaptation for Text-Video Retrieval](http://arxiv.org/abs/2308.07648v1)
  * [In-Style: Bridging Text and Uncurated Videos with Style Transfer for Text-Video Retrieval](http://arxiv.org/abs/2309.08928v1)<br>:star:[code](https://github.com/ninatu/in_style)
  * [UATVR: Uncertainty-Adaptive Text-Video Retrieval](http://arxiv.org/abs/2301.06309)
  * [Progressive Spatio-Temporal Prototype Matching for Text-Video Retrieval](https://openaccess.thecvf.com/content/ICCV2023/papers/Li_Progressive_Spatio-Temporal_Prototype_Matching_for_Text-Video_Retrieval_ICCV_2023_paper.pdf)
* è§†é¢‘-æ–‡æœ¬æ£€ç´¢  
  * [Multi-event Video-Text Retrieval](http://arxiv.org/abs/2308.11551v1)<br>:star:[code](https://github.com/gengyuanmax/MeVTR)
  * [Unified Coarse-to-Fine Alignment for Video-Text Retrieval](http://arxiv.org/abs/2309.10091v1)<br>:star:[code](https://github.com/Ziyang412/UCoFiA)
* è§†é¢‘æ£€ç´¢
  * [Simple Baselines for Interactive Video Retrieval with Questions and Answers](http://arxiv.org/abs/2308.10402v1)<br>:star:[code](https://github.com/kevinliang888/IVR-QA-baselines)
  * [VADER: Video Alignment Differencing and Retrieval](http://arxiv.org/abs/2303.13193)
  * [Dual Learning with Dynamic Knowledge Distillation for Partially Relevant Video Retrieval](https://openaccess.thecvf.com/content/ICCV2023/papers/Dong_Dual_Learning_with_Dynamic_Knowledge_Distillation_for_Partially_Relevant_Video_ICCV_2023_paper.pdf)

<a name="14"/>

## 14.Image/Video Composition(å›¾åƒ/è§†é¢‘å‹ç¼©)
* å›¾åƒå‹ç¼©
  * [TF-ICON: Diffusion-Based Training-Free Cross-Domain Image Composition](http://arxiv.org/abs/2307.12493v1)<br>:star:[code](https://github.com/Shilin-LU/TF-ICON)
  * [Semantically Structured Image Compression via Irregular Group-Based Decoupling](http://arxiv.org/abs/2305.02586)
  * [COMPASS: High-Efficiency Deep Image Compression with Arbitrary-scale Spatial Scalability](http://arxiv.org/abs/2309.07926v1)
  * [Computationally-Efficient Neural Image Compression with Shallow Decoders](https://openaccess.thecvf.com/content/ICCV2023/papers/Yang_Computationally-Efficient_Neural_Image_Compression_with_Shallow_Decoders_ICCV_2023_paper.pdf)
  * [Dec-Adapter: Exploring Efficient Decoder-Side Adapter for Bridging Screen Content and Natural Image Compression](https://openaccess.thecvf.com/content/ICCV2023/papers/Shen_Dec-Adapter_Exploring_Efficient_Decoder-Side_Adapter_for_Bridging_Screen_Content_and_ICCV_2023_paper.pdf)
  * [TransTIC: Transferring Transformer-based Image Compression from Human Perception to Machine Perception](http://arxiv.org/abs/2306.05085)
* è§†é¢‘å‹ç¼©
  * [Deep Optics for Video Snapshot Compressive Imaging](https://openaccess.thecvf.com/content/ICCV2023/papers/Wang_Deep_Optics_for_Video_Snapshot_Compressive_Imaging_ICCV_2023_paper.pdf)
  * [Unfolding Framework with Prior of Convolution-Transformer Mixture and Uncertainty Estimation for Video Snapshot Compressive Imaging](http://arxiv.org/abs/2306.11316)
  * [Scene Matters: Model-based Deep Video Compression](http://arxiv.org/abs/2303.04557)

<a name="13"/>

## 13.Image Captions(å›¾åƒå­—å¹•)
* [RCA-NOC: Relative Contrastive Alignment for Novel Object Captioning](https://openaccess.thecvf.com/content/ICCV2023/papers/Fan_RCA-NOC_Relative_Contrastive_Alignment_for_Novel_Object_Captioning_ICCV_2023_paper.pdf)
* [Guiding Image Captioning Models Toward More Specific Captions](http://arxiv.org/abs/2307.16686v1)
* [OxfordTVG-HIC: Can Machine Make Humorous Captions from Images?](http://arxiv.org/abs/2307.11636v1)
* [Transferable Decoding with Visual Entities for Zero-Shot Image Captioning](http://arxiv.org/abs/2307.16525v1)<br>:star:[code](https://github.com/FeiElysia/ViECap)
* [Explore and Tell: Embodied Visual Captioning in 3D Environments](http://arxiv.org/abs/2308.10447v1)<br>:star:[code](https://aim3-ruc.github.io/ExploreAndTell)
* [With a Little Help from your own Past: Prototypical Memory Networks for Image Captioning](http://arxiv.org/abs/2308.12383v1)<br>:star:[code](https://github.com/aimagelab/PMA-Net)
* æ›´æ”¹å­—å¹•
  * [Self-supervised Cross-view Representation Reconstruction for Change Captioning](https://openaccess.thecvf.com/content/ICCV2023/papers/Tu_Self-supervised_Cross-view_Representation_Reconstruction_for_Change_Captioning_ICCV_2023_paper.pdf)
* è§†é¢‘å­—å¹•
  * [Accurate and Fast Compressed Video Captioning](http://arxiv.org/abs/2309.12867)
  * [Exploring Group Video Captioning with Efficient Relational Approximation](https://openaccess.thecvf.com/content/ICCV2023/papers/Lin_Exploring_Group_Video_Captioning_with_Efficient_Relational_Approximation_ICCV_2023_paper.pdf)

<a name="12"/>

## 12.Medical Image(åŒ»å­¦å½±åƒ)
* [LIMITR: Leveraging Local Information for Medical Image-Text Representation](http://arxiv.org/abs/2303.11755)
* [LNPL-MIL: Learning from Noisy Pseudo Labels for Promoting Multiple Instance Learning in Whole Slide Image](https://openaccess.thecvf.com/content/ICCV2023/papers/Shao_LNPL-MIL_Learning_from_Noisy_Pseudo_Labels_for_Promoting_Multiple_Instance_ICCV_2023_paper.pdf)
* [A skeletonization algorithm for gradient-based optimization](http://arxiv.org/abs/2309.02527v1)
* [Learning to Distill Global Representation for Sparse-View CT](http://arxiv.org/abs/2308.08463v1)<br>:star:[code](https://github.com/longzilicart/GloReDi)
* [Taxonomy Adaptive Cross-Domain Adaptation in Medical Imaging via Optimization Trajectory Distillation](http://arxiv.org/abs/2307.14709v1)
* [Dual Meta-Learning with Longitudinally Generalized Regularization for One-Shot Brain Tissue Segmentation Across the Human Lifespan](http://arxiv.org/abs/2308.06774v1)<br>:star:[code](https://github.com/ladderlab-xjtu/DuMeta)
* [ConSlide: Asynchronous Hierarchical Interaction Transformer with Breakup-Reorganize Rehearsal for Continual Whole Slide Image Analysis](http://arxiv.org/abs/2308.13324v1)
* [MRM: Masked Relation Modeling for Medical Image Pre-Training with Genetics](https://openaccess.thecvf.com/content/ICCV2023/papers/Yang_MRM_Masked_Relation_Modeling_for_Medical_Image_Pre-Training_with_Genetics_ICCV_2023_paper.pdf)
* åŒ»å­¦å½±åƒé…å‡†
  * [Towards Saner Deep Image Registration](http://arxiv.org/abs/2307.09696v1)<br>:star:[code](https://github.com/tuffr5/Saner-deep-registration)
  * [Preserving Tumor Volumes for Unsupervised Medical Image Registration](http://arxiv.org/abs/2309.10153v1)<br>:star:[code](https://dddraxxx.github.io/Volume-Preserving-Registration/)
* åŒ»å­¦æŠ¥å‘Šç”Ÿæˆ
  * [PRIOR: Prototype Representation Joint Learning from Medical Images and Reports](http://arxiv.org/abs/2307.12577v1)<br>:star:[code](https://github.com/QtacierP/PRIOR)
  * [Unify, Align and Refine: Multi-Level Semantic Alignment for Radiology Report Generation](http://arxiv.org/abs/2303.15932)
* åŒ»å­¦å½±åƒåˆ†å‰²
  * [CauSSL: Causality-inspired Semi-supervised Learning for Medical Image Segmentation](https://openaccess.thecvf.com/content/ICCV2023/papers/Miao_CauSSL_Causality-inspired_Semi-supervised_Learning_for_Medical_Image_Segmentation_ICCV_2023_paper.pdf)
  * [Dynamic Snake Convolution Based on Topological Geometric Constraints for Tubular Structure Segmentation](http://arxiv.org/abs/2307.08388)
  * [UniverSeg: Universal Medical Image Segmentation](http://arxiv.org/abs/2304.06131)
* åˆ‡ç‰‡åˆ†ç±»
  * [Multiple Instance Learning Framework with Masked Hard Instance Mining for Whole Slide Image Classification](http://arxiv.org/abs/2307.15254v1)<br>:star:[code](https://github.com/DearCaat/MHIM-MIL)
* ç»†èƒæ ¸æ£€æµ‹
  * [Affine-Consistent Transformer for Multi-Class Cell Nuclei Detection](https://openaccess.thecvf.com/content/ICCV2023/papers/Huang_Affine-Consistent_Transformer_for_Multi-Class_Cell_Nuclei_Detection_ICCV_2023_paper.pdf)
* X å°„çº¿
  * [MedKLIP: Medical Knowledge Enhanced Language-Image Pre-Training for X-ray Diagnosis](https://openaccess.thecvf.com/content/ICCV2023/papers/Wu_MedKLIP_Medical_Knowledge_Enhanced_Language-Image_Pre-Training_for_X-ray_Diagnosis_ICCV_2023_paper.pdf)
  * [BoMD: Bag of Multi-label Descriptors for Noisy Chest X-ray Classification](https://openaccess.thecvf.com/content/ICCV2023/papers/Chen_BoMD_Bag_of_Multi-label_Descriptors_for_Noisy_Chest_X-ray_Classification_ICCV_2023_paper.pdf)
* MRI 
  * MRI é‡å»º
    * [Decomposition-Based Variational Network for Multi-Contrast MRI Super-Resolution and Reconstruction](https://openaccess.thecvf.com/content/ICCV2023/papers/Lei_Decomposition-Based_Variational_Network_for_Multi-Contrast_MRI_Super-Resolution_and_Reconstruction_ICCV_2023_paper.pdf)
  * MRIè¶…åˆ†è¾¨ç‡
    * [Rethinking Multi-Contrast MRI Super-Resolution: Rectangle-Window Cross-Attention Transformer and Arbitrary-Scale Upsampling](https://openaccess.thecvf.com/content/ICCV2023/papers/Li_Rethinking_Multi-Contrast_MRI_Super-Resolution_Rectangle-Window_Cross-Attention_Transformer_and_Arbitrary-Scale_Upsampling_ICCV_2023_paper.pdf)
* è„‘è‚¿ç˜¤åˆ†å‰²
  * [Scratch Each Other's Back: Incomplete Multi-Modal Brain Tumor Segmentation via Category Aware Group Self-Support Learning](https://openaccess.thecvf.com/content/ICCV2023/papers/Qiu_Scratch_Each_Others_Back_Incomplete_Multi-Modal_Brain_Tumor_Segmentation_via_ICCV_2023_paper.pdf)
  * [Enhancing Modality-Agnostic Representations via Meta-Learning for Brain Tumor Segmentation](http://arxiv.org/abs/2302.04308)
  * [Dual Meta-Learning with Longitudinally Consistent Regularization for One-Shot Brain Tissue Segmentation Across the Human Lifespan](https://openaccess.thecvf.com/content/ICCV2023/papers/Sun_Dual_Meta-Learning_with_Longitudinally_Consistent_Regularization_for_One-Shot_Brain_Tissue_ICCV_2023_paper.pdf)
* å™¨å®˜åˆ†å‰²å’Œè‚¿ç˜¤æ£€æµ‹
  * [CLIP-Driven Universal Model for Organ Segmentation and Tumor Detection](http://arxiv.org/abs/2301.00785)
* CT
  * [Continual Segment: Towards a Single, Unified and Non-forgetting Continual Segmentation Model of 143 Whole-body Organs in CT Scans](https://openaccess.thecvf.com/content/ICCV2023/papers/Ji_Continual_Segment_Towards_a_Single_Unified_and_Non-forgetting_Continual_Segmentation_ICCV_2023_paper.pdf)
  * [DOLCE: A Model-Based Probabilistic Diffusion Framework for Limited-Angle CT Reconstruction](http://arxiv.org/abs/2211.12340)
  * [CancerUniT: Towards a Single Unified Model for Effective Detection, Segmentation, and Diagnosis of Eight Major Cancers Using a Large Collection of CT Scans](https://openaccess.thecvf.com/content/ICCV2023/papers/Chen_CancerUniT_Towards_a_Single_Unified_Model_for_Effective_Detection_Segmentation_ICCV_2023_paper.pdf)

<a name="11"/>

## 11.Image/Video Editing(å›¾åƒ/è§†é¢‘ç¼–è¾‘)
* å›¾åƒç¼–è¾‘
  * [Effective Real Image Editing with Accelerated Iterative Diffusion Inversion](http://arxiv.org/abs/2309.04907v1)
  * [Multimodal Garment Designer: Human-Centric Latent Diffusion Models for Fashion Image Editing](http://arxiv.org/abs/2304.02051)
  * [SKED: Sketch-guided Text-based 3D Editing](http://arxiv.org/abs/2303.10735)
  * [Prompt Tuning Inversion for Text-driven Image Editing Using Diffusion Models](http://arxiv.org/abs/2305.04441)
  * [A Latent Space of Stochastic Diffusion Models for Zero-Shot Image Editing and Guidance](https://openaccess.thecvf.com/content/ICCV2023/papers/Wu_A_Latent_Space_of_Stochastic_Diffusion_Models_for_Zero-Shot_Image_ICCV_2023_paper.pdf)
* è§†é¢‘ç¼–è¾‘
  * [StableVideo: Text-driven Consistency-aware Diffusion Video Editing](http://arxiv.org/abs/2308.09592v1)<br>:star:[code](https://github.com/rese1f/StableVideo)
  * [VidStyleODE: Disentangled Video Editing via StyleGAN and NeuralODEs](http://arxiv.org/abs/2304.06020)
  * [Pix2Video: Video Editing using Image Diffusion](http://arxiv.org/abs/2303.12688)
  * [FateZero: Fusing Attentions for Zero-shot Text-based Video Editing](http://arxiv.org/abs/2303.09535)
* å¤´å‘ç¼–è¾‘
  * [HairCLIPv2: Unifying Hair Editing via Proxy Feature Blending](https://openaccess.thecvf.com/content/ICCV2023/papers/Wei_HairCLIPv2_Unifying_Hair_Editing_via_Proxy_Feature_Blending_ICCV_2023_paper.pdf)

<a name="10"/>

## 10.Image Synthesis(å›¾åƒåˆæˆ)
* [Foreground Object Search by Distilling Composite Image Feature](http://arxiv.org/abs/2308.04990v1)<br>:star:[code](https://github.com/bcmi/Foreground-Object-Search-Dataset-FOSD)
* [Beyond Image Borders: Learning Feature Extrapolation for Unbounded Image Composition](http://arxiv.org/abs/2309.12042)
* å›¾åƒç”Ÿæˆ
  * [MosaiQ: Quantum Generative Adversarial Networks for Image Generation on NISQ Computers](http://arxiv.org/abs/2308.11096v1)
  * [The Euclidean Space is Evil: Hyperbolic Attribute Editing for Few-shot Image Generation](http://arxiv.org/abs/2211.12347)
  * [Generative Multiplane Neural Radiance for 3D-Aware Image Generation](http://arxiv.org/abs/2304.01172)
  * [EGC: Image Generation and Classification via a Diffusion Energy-Based Model](http://arxiv.org/abs/2304.02012)
  * [Personalized Image Generation for Color Vision Deficiency Population](https://openaccess.thecvf.com/content/ICCV2023/papers/Jiang_Personalized_Image_Generation_for_Color_Vision_Deficiency_Population_ICCV_2023_paper.pdf)
  * [3D-aware Image Generation using 2D Diffusion Models](http://arxiv.org/abs/2303.17905)
  * [Ray Conditioning: Trading Photo-consistency for Photo-realism in Multi-view Image Generation](http://arxiv.org/abs/2304.13681)
  * [Both Diverse and Realism Matter: Physical Attribute and Style Alignment for Rainy Image Generation](https://openaccess.thecvf.com/content/ICCV2023/papers/Yu_Both_Diverse_and_Realism_Matter_Physical_Attribute_and_Style_Alignment_ICCV_2023_paper.pdf)
* å›¾åƒåˆæˆ
  * [Conditional 360-degree Image Synthesis for Immersive Indoor Scene Decoration](http://arxiv.org/abs/2307.09621v1)
  * [Steered Diffusion: A Generalized Framework for Plug-and-Play Conditional Image Synthesis](https://openaccess.thecvf.com/content/ICCV2023/papers/Nair_Steered_Diffusion_A_Generalized_Framework_for_Plug-and-Play_Conditional_Image_Synthesis_ICCV_2023_paper.pdf)
  * [BallGAN: 3D-aware Image Synthesis with a Spherical Background](http://arxiv.org/abs/2301.09091)
  * [Perceptual Artifacts Localization for Image Synthesis Tasks](https://openaccess.thecvf.com/content/ICCV2023/papers/Zhang_Perceptual_Artifacts_Localization_for_Image_Synthesis_Tasks_ICCV_2023_paper.pdf)
  * [Masked Diffusion Transformer is a Strong Image Synthesizer](http://arxiv.org/abs/2303.14389)
  * [SideGAN: 3D-Aware Generative Model for Improved Side-View Image Synthesis](http://arxiv.org/abs/2309.10388v1)
  * [VeRi3D: Generative Vertex-based Radiance Fields for 3D Controllable Human Image Synthesis](https://openaccess.thecvf.com/content/ICCV2023/papers/Ji_Anchor_Structure_Regularization_Induced_Multi-view_Subspace_Clustering_via_Enhanced_Tensor_ICCV_2023_paper.pdf)
  * [MasaCtrl: Tuning-Free Mutual Self-Attention Control for Consistent Image Synthesis and Editing](http://arxiv.org/abs/2304.08465)
  * [CHORUS : Learning Canonicalized 3D Human-Object Spatial Relations from Unbounded Synthesized Images](http://arxiv.org/abs/2308.12288)
* æ–‡æœ¬-å›¾åƒåˆæˆ
  * [Dense Text-to-Image Generation with Attention Modulation](http://arxiv.org/abs/2308.12964v1)<br>:star:[code](https://github.com/naver-ai/DenseDiffusion)
  * [DALL-Eval: Probing the Reasoning Skills and Social Biases of Text-to-Image Generation Models](https://openaccess.thecvf.com/content/ICCV2023/papers/Cho_DALL-Eval_Probing_the_Reasoning_Skills_and_Social_Biases_of_Text-to-Image_ICCV_2023_paper.pdf)
  * [ITI-GEN: Inclusive Text-to-Image Generation](http://arxiv.org/abs/2309.05569v1)<br>:star:[code](https://czhang0528.github.io/iti-gen)
  * [MagicFusion: Boosting Text-to-Image Generation Performance by Fusing Diffusion Models](http://arxiv.org/abs/2303.13126)
  * [Localizing Object-Level Shape Variations with Text-to-Image Diffusion Models](http://arxiv.org/abs/2303.11306)
  * [Expressive Text-to-Image Generation with Rich Text](http://arxiv.org/abs/2304.06720)
  * [Evaluating Data Attribution for Text-to-Image Models](http://arxiv.org/abs/2306.09345)
  * [Affective Image Filter: Reflecting Emotions from Text to Images](https://openaccess.thecvf.com/content/ICCV2023/papers/Weng_Affective_Image_Filter_Reflecting_Emotions_from_Text_to_Images_ICCV_2023_paper.pdf)
  * [Editing Implicit Assumptions in Text-to-Image Diffusion Models](http://arxiv.org/abs/2303.08084)
  * [A-STAR: Test-time Attention Segregation and Retention for Text-to-image Synthesis](https://openaccess.thecvf.com/content/ICCV2023/papers/Agarwal_A-STAR_Test-time_Attention_Segregation_and_Retention_for_Text-to-image_Synthesis_ICCV_2023_paper.pdf)
  * [PODIA-3D: Domain Adaptation of 3D Generative Model Across Large Domain Gap Using Pose-Preserved Text-to-Image Diffusion](https://openaccess.thecvf.com/content/ICCV2023/papers/Kim_PODIA-3D_Domain_Adaptation_of_3D_Generative_Model_Across_Large_Domain_ICCV_2023_paper.pdf)
  * [BoxDiff: Text-to-Image Synthesis with Training-Free Box-Constrained Diffusion](http://arxiv.org/abs/2307.10816v1)<br>:star:[code](https://github.com/Sierkinhane/BoxDiff)
   * [Learning to Generate Semantic Layouts for Higher Text-Image Correspondence in Text-to-Image Synthesis](http://arxiv.org/abs/2308.08157v1)<br>:star:[code](https://pmh9960.github.io/research/GCDP)
   * [Unsupervised Compositional Concepts Discovery with Text-to-Image Generative Models](http://arxiv.org/abs/2306.05357)
   * [Rickrolling the Artist: Injecting Backdoors into Text Encoders for Text-to-Image Synthesis](http://arxiv.org/abs/2211.02408)
   * [Text-Conditioned Sampling Framework for Text-to-Image Generation with Masked Generative Models](http://arxiv.org/abs/2304.01515)
   * [Zero-Shot Spatial Layout Conditioning for Text-to-Image Diffusion Models](http://arxiv.org/abs/2306.13754)
   * [Harnessing the Spatial-Temporal Attention of Diffusion Models for High-Fidelity Text-to-Image Synthesis](http://arxiv.org/abs/2304.03869)
* å›¾åƒ-è§†é¢‘ç”Ÿæˆ
  * [Breaking Temporal Consistency: Generating Video Universal Adversarial Perturbations Using Image Models](https://openaccess.thecvf.com/content/ICCV2023/papers/Kim_Breaking_Temporal_Consistency_Generating_Video_Universal_Adversarial_Perturbations_Using_Image_ICCV_2023_paper.pdf)
* æ–‡æœ¬-è§†é¢‘ç”Ÿæˆ
  * [Tune-A-Video: One-Shot Tuning of Image Diffusion Models for Text-to-Video Generation](https://openaccess.thecvf.com/content/ICCV2023/papers/Wu_Tune-A-Video_One-Shot_Tuning_of_Image_Diffusion_Models_for_Text-to-Video_Generation_ICCV_2023_paper.pdf)
* éŸ³é¢‘é©±åŠ¨çš„å›¾åƒç”Ÿæˆ
  * [Generating Realistic Images from In-the-wild Sounds](http://arxiv.org/abs/2309.02405v1)
* X-å›¾åƒç”Ÿæˆ
  * [GlueGen: Plug and Play Multi-modal Encoders for X-to-image Generation](https://openaccess.thecvf.com/content/ICCV2023/papers/Qin_GlueGen_Plug_and_Play_Multi-modal_Encoders_for_X-to-image_Generation_ICCV_2023_paper.pdf)
* æ‰©æ•£
  * [DPM-OT: A New Diffusion Probabilistic Model Based on Optimal Transport](http://arxiv.org/abs/2307.11308v1)<br>:star:[code](https://github.com/cognaclee/DPM-OT)
  * [Diffuse3D: Wide-Angle 3D Photography via Bilateral Diffusion](https://openaccess.thecvf.com/content/ICCV2023/papers/Jiang_Diffuse3D_Wide-Angle_3D_Photography_via_Bilateral_Diffusion_ICCV_2023_paper.pdf)
  * [Diffusion Models as Masked Autoencoders](http://arxiv.org/abs/2304.03283)
  * [Versatile Diffusion: Text, Images and Variations All in One Diffusion Model](http://arxiv.org/abs/2211.08332)
  * [DS-Fusion: Artistic Typography via Discriminated and Stylized Diffusion](https://openaccess.thecvf.com/content/ICCV2023/papers/Tanveer_DS-Fusion_Artistic_Typography_via_Discriminated_and_Stylized_Diffusion_ICCV_2023_paper.pdf)
  * [The Stable Signature: Rooting Watermarks in Latent Diffusion Models](https://openaccess.thecvf.com/content/ICCV2023/papers/Fernandez_The_Stable_Signature_Rooting_Watermarks_in_Latent_Diffusion_Models_ICCV_2023_paper.pdf)
  * [Improving Sample Quality of Diffusion Models Using Self-Attention Guidance](http://arxiv.org/abs/2210.00939)
  * [Ablating Concepts in Text-to-Image Diffusion Models](http://arxiv.org/abs/2303.13516)
  * [Preserve Your Own Correlation: A Noise Prior for Video Diffusion Models](http://arxiv.org/abs/2305.10474)
  * [Score-Based Diffusion Models as Principled Priors for Inverse Imaging](http://arxiv.org/abs/2304.11751)
  * [Diffusion-SDF: Conditional Generative Modeling of Signed Distance Functions](https://openaccess.thecvf.com/content/ICCV2023/papers/Chou_Diffusion-SDF_Conditional_Generative_Modeling_of_Signed_Distance_Functions_ICCV_2023_paper.pdf)
  * [Benchmarking Low-Shot Robustness to Natural Distribution Shifts](http://arxiv.org/abs/2304.11263)
  * [Viewset Diffusion: (0-)Image-Conditioned 3D Generative Models from 2D Data](https://openaccess.thecvf.com/content/ICCV2023/papers/Szymanowicz_Viewset_Diffusion_0-Image-Conditioned_3D_Generative_Models_from_2D_Data_ICCV_2023_paper.pdf)
  * [AutoDiffusion: Training-Free Optimization of Time Steps and Architectures for Automated Diffusion Model Acceleration](http://arxiv.org/abs/2309.10438)
  * [Text2Tex: Text-driven Texture Synthesis via Diffusion Models](http://arxiv.org/abs/2303.11396)
  * [AdvDiffuser: Natural Adversarial Example Synthesis with Diffusion Models](https://openaccess.thecvf.com/content/ICCV2023/papers/Chen_AdvDiffuser_Natural_Adversarial_Example_Synthesis_with_Diffusion_Models_ICCV_2023_paper.pdf)
  * [Q-Diffusion: Quantizing Diffusion Models](https://openaccess.thecvf.com/content/ICCV2023/papers/Li_Q-Diffusion_Quantizing_Diffusion_Models_ICCV_2023_paper.pdf)
  * [Phasic Content Fusing Diffusion Model with Directional Distribution Consistency for Few-Shot Model Adaption](http://arxiv.org/abs/2309.03729v1)<br>:star:[code](https://github.com/sjtuplayer/few-shot-diffusion)
  * [DiffGuard: Semantic Mismatch-Guided Out-of-Distribution Detection using Pre-trained Diffusion Models](http://arxiv.org/abs/2308.07687v1)
  * [DiffDis: Empowering Generative Diffusion Model with Cross-Modal Discrimination Capability](http://arxiv.org/abs/2308.09306v1)
  * [Texture Generation on 3D Meshes with Point-UV Diffusion](http://arxiv.org/abs/2308.10490v1)<br>:star:[code](https://cvmi-lab.github.io/Point-UV-Diffusion)
  * [End-to-End Diffusion Latent Optimization Improves Classifier Guidance](http://arxiv.org/abs/2303.13703)
  * [Stochastic Segmentation with Conditional Categorical Diffusion Models](http://arxiv.org/abs/2303.08888)
  * [DIFFGUARD: Semantic Mismatch-Guided Out-of-Distribution Detection Using Pre-Trained Diffusion Models](http://arxiv.org/abs/2308.07687)
  * [Erasing Concepts from Diffusion Models](http://arxiv.org/abs/2303.07345)
  * [A Complete Recipe for Diffusion Generative Models](https://openaccess.thecvf.com/content/ICCV2023/papers/Pandey_A_Complete_Recipe_for_Diffusion_Generative_Models_ICCV_2023_paper.pdf)
  * [SVDiff: Compact Parameter Space for Diffusion Fine-Tuning](http://arxiv.org/abs/2303.11305)
  * [TexFusion: Synthesizing 3D Textures with Text-Guided Image Diffusion Models](https://openaccess.thecvf.com/content/ICCV2023/papers/Cao_TexFusion_Synthesizing_3D_Textures_with_Text-Guided_Image_Diffusion_Models_ICCV_2023_paper.pdf)
* 3Då½¢çŠ¶ç”Ÿæˆ
  * [3D Semantic Subspace Traverser: Empowering 3D Generative Model with Shape Editing Capability](http://arxiv.org/abs/2307.14051v1)<br>:star:[code](https://github.com/TrepangCat/3D_Semantic_Subspace_Traverser)
  * [SALAD: Part-Level Latent Diffusion for 3D Shape Generation and Manipulation](http://arxiv.org/abs/2303.12236)
  * [ShapeScaffolder: Structure-Aware 3D Shape Generation from Text](https://openaccess.thecvf.com/content/ICCV2023/papers/Tian_ShapeScaffolder_Structure-Aware_3D_Shape_Generation_from_Text_ICCV_2023_paper.pdf)
* æ•…äº‹å¯è§†åŒ–
  * [Story Visualization by Online Text Augmentation with Context Memory](http://arxiv.org/abs/2308.07575v1)
* AIGC
  * [ReMoDiffuse: Retrieval-Augmented Motion Diffusion Model](https://arxiv.org/pdf/2304.01116.pdf)<br>:house:[project](https://mingyuan-zhang.github.io/projects/ReMoDiffuse.html)<br>:thumbsup:[ICCV 2023|é‡å¡‘äººä½“åŠ¨ä½œç”Ÿæˆï¼Œèåˆæ‰©æ•£æ¨¡å‹ä¸æ£€ç´¢ç­–ç•¥çš„æ–°èŒƒå¼ReMoDiffuseæ¥äº†](https://mp.weixin.qq.com/s/IOuqsd0e6tj_qFnxpRPaEA)
* å¸ƒå±€ç”Ÿæˆ
  * [LayoutDiffusion: Improving Graphic Layout Generation by Discrete Diffusion Probabilistic Models](http://arxiv.org/abs/2303.11589)
  * [CC3D: Layout-Conditioned Generation of Compositional 3D Scenes](http://arxiv.org/abs/2303.12074)
  * [A Parse-Then-Place Approach for Generating Graphic Layouts from Textual Descriptions](http://arxiv.org/abs/2308.12700v1)
  * [GlobalMapper: Arbitrary-Shaped Urban Layout Generation](http://arxiv.org/abs/2307.09693v1)
  * [DLT: Conditioned layout generation with Joint Discrete-Continuous Diffusion Layout Transformer](http://arxiv.org/abs/2303.03755)
* æ–‡æœ¬-3D
  * [Fantasia3D: Disentangling Geometry and Appearance for High-quality Text-to-3D Content Creation](http://arxiv.org/abs/2303.13873)

<a name="9"/>

## 9.Image Classification(å›¾åƒåˆ†ç±»)
* [Learning Support and Trivial Prototypes for Interpretable Image Classification](http://arxiv.org/abs/2301.04011)
* [Dynamic Perceiver for Efficient Visual Recognition](http://arxiv.org/abs/2306.11248)
* [Agile Modeling: From Concept to Classifier in Minutes](http://arxiv.org/abs/2302.12948)
* [Waffling Around for Performance: Visual Classification with Random Words and Broad Concepts](http://arxiv.org/abs/2306.07282)
* [A step towards understanding why classification helps regression](http://arxiv.org/abs/2308.10603v1)
* [Image-free Classifier Injection for Zero-Shot Classification](http://arxiv.org/abs/2308.10599v1)<br>:star:[code](https://github.com/ExplainableML/ImageFreeZSL)
* [Label-Free Event-based Object Recognition via Joint Learning with Image Reconstruction from Events](http://arxiv.org/abs/2308.09383v1)<br>:star:[code](https://github.com/Chohoonhee/Ev-LaFOR)
* [ImbSAM: A Closer Look at Sharpness-Aware Minimization in Class-Imbalanced Recognition](http://arxiv.org/abs/2308.07815v1)<br>:star:[code](https://github.com/cool-xuan/Imbalanced_SAM)
* [Learning Concise and Descriptive Attributes for Visual Recognition](http://arxiv.org/abs/2308.03685v1)
* [Get the Best of Both Worlds: Improving Accuracy and Transferability by Grassmann Class Representation](http://arxiv.org/abs/2308.01547v1)<br>:star:[code](https://github.com/innerlee/GCR)
* [What do neural networks learn in image classification? A frequency shortcut perspective](http://arxiv.org/abs/2307.09829v1)
* [Identification of Systematic Errors of Image Classifiers on Rare Subgroups](http://arxiv.org/abs/2303.05072)
* [Better May Not Be Fairer: A Study on Subgroup Discrepancy in Image Classification](http://arxiv.org/abs/2212.08649)
* é›¶æ ·æœ¬å›¾åƒåˆ†ç±»
  * [What Does a Platypus Look Like? Generating Customized Prompts for Zero-Shot Image Classification](http://arxiv.org/abs/2209.03320)
  * [Your Diffusion Model is Secretly a Zero-Shot Classifier](http://arxiv.org/abs/2303.16203)
  * [Image-Free Classifier Injection for Zero-Shot Classification](http://arxiv.org/abs/2308.10599)
* å¤šæ ‡ç­¾å›¾åƒåˆ†ç±»
  * [CDUL: CLIP-Driven Unsupervised Learning for Multi-Label Image Classification](http://arxiv.org/abs/2307.16634v1)
  * [Learning in Imperfect Environment: Multi-Label Classification with Long-Tailed Distribution and Partial Labels](http://arxiv.org/abs/2304.10539)
  * [PatchCT: Aligning Patch Set and Label Set with Conditional Transport for Multi-Label Image Classification](http://arxiv.org/abs/2307.09066)
  * [Scene-Aware Label Graph Learning for Multi-Label Image Classification](https://openaccess.thecvf.com/content/ICCV2023/papers/Zhu_Scene-Aware_Label_Graph_Learning_for_Multi-Label_Image_Classification_ICCV_2023_paper.pdf)
* ç»†ç²’åº¦è¯†åˆ«
  * [Learning Gabor Texture Features for Fine-Grained Recognition](http://arxiv.org/abs/2308.05396v1)
  * [Multi-View Active Fine-Grained Visual Recognition](https://openaccess.thecvf.com/content/ICCV2023/papers/Du_Multi-View_Active_Fine-Grained_Visual_Recognition_ICCV_2023_paper.pdf)
* é•¿å°¾è¯†åˆ«
  * [MDCS: More Diverse Experts with Consistency Self-distillation for Long-tailed Recognition](http://arxiv.org/abs/2308.09922v1)<br>:star:[code](https://github.com/fistyee/MDCS)
  * [Subclass-balancing Contrastive Learning for Long-tailed Recognition](http://arxiv.org/abs/2306.15925)
* é•¿å°¾åˆ†ç±»
  * [AREA: Adaptive Reweighting via Effective Area for Long-Tailed Classification](https://openaccess.thecvf.com/content/ICCV2023/papers/Chen_AREA_Adaptive_Reweighting_via_Effective_Area_for_Long-Tailed_Classification_ICCV_2023_paper.pdf)
* é•¿å°¾å­¦ä¹ 
  * [Local and Global Logit Adjustments for Long-Tailed Learning](https://openaccess.thecvf.com/content/ICCV2023/papers/Tao_Local_and_Global_Logit_Adjustments_for_Long-Tailed_Learning_ICCV_2023_paper.pdf)
  * [Global Balanced Experts for Federated Long-Tailed Learning](https://openaccess.thecvf.com/content/ICCV2023/papers/Zeng_Global_Balanced_Experts_for_Federated_Long-Tailed_Learning_ICCV_2023_paper.pdf)

<a name="8"/>

## 8.Image Segmentation(å›¾åƒåˆ†å‰²)
* [Segment Anything](https://arxiv.org/abs/2304.02643)<br>:house:[project](https://segment-anything.com/)
* [InterFormer: Real-time Interactive Image Segmentation](http://arxiv.org/abs/2304.02942)
* [Zero-guidance Segmentation Using Zero Segment Labels](http://arxiv.org/abs/2303.13396)
* [Video State-Changing Object Segmentation](https://openaccess.thecvf.com/content/ICCV2023/papers/Yu_Video_State-Changing_Object_Segmentation_ICCV_2023_paper.pdf)
* [RbA: Segmenting Unknown Regions Rejected by All](http://arxiv.org/abs/2211.14293)
* [Locating Noise is Halfway Denoising for Semi-Supervised Segmentation](https://openaccess.thecvf.com/content/ICCV2023/papers/Fang_Locating_Noise_is_Halfway_Denoising_for_Semi-Supervised_Segmentation_ICCV_2023_paper.pdf)
* [SEMPART: Self-supervised Multi-resolution Partitioning of Image Semantics](http://arxiv.org/abs/2309.10972)
* [Texture Learning Domain Randomization for Domain Generalized Segmentation](http://arxiv.org/abs/2303.11546)
* [SegGPT: Towards Segmenting Everything in Context](https://openaccess.thecvf.com/content/ICCV2023/papers/Wang_SegGPT_Towards_Segmenting_Everything_in_Context_ICCV_2023_paper.pdf)
* [Coarse-to-Fine Amodal Segmentation with Shape Prior](http://arxiv.org/abs/2308.16825v1)<br>:star:[code](http://jianxgao.github.io/C2F-Seg)
* [Homography Guided Temporal Fusion for Road Line and Marking Segmentation](https://openaccess.thecvf.com/content/ICCV2023/papers/Wang_Homography_Guided_Temporal_Fusion_for_Road_Line_and_Marking_Segmentation_ICCV_2023_paper.pdf)
* [SegPrompt: Boosting Open-world Segmentation via Category-level Prompt Learning](http://arxiv.org/abs/2308.06531v1)<br>:star:[code](https://github.com/aim-uofa/SegPrompt)
* [Multi-interactive Feature Learning and a Full-time Multi-modality Benchmark for Image Fusion and Segmentation](https://arxiv.org/abs/2308.02097)<br>:star:[code](https://github.com/JinyuanLiu-CV/SegMiF)
* [CoinSeg: Contrast Inter- and Intra- Class Representations for Incremental Segmentation](https://openaccess.thecvf.com/content/ICCV2023/papers/Zhang_CoinSeg_Contrast_Inter-_and_Intra-_Class_Representations_for_Incremental_Segmentation_ICCV_2023_paper.pdf)
* [Unmasking Anomalies in Road-Scene Segmentation](http://arxiv.org/abs/2307.13316v1)<br>:star:[code](https://github.com/shyam671/Mask2Anomaly-Unmasking-Anomalies-in-Road-Scene-Segmentation)
* [DynaMITe: Dynamic Query Bootstrapping for Multi-object Interactive Segmentation Transformer](http://arxiv.org/abs/2304.06668)
* [LogicSeg: Parsing Visual Semantics with Neural Logic Learning and Reasoning](http://arxiv.org/abs/2309.13556)
* [SimpleClick: Interactive Image Segmentation with Simple Vision Transformers](http://arxiv.org/abs/2210.11006)
* [UniSeg: A Unified Multi-Modal LiDAR Segmentation Network and the OpenPCSeg Codebase](http://arxiv.org/abs/2309.05573v1)<br>:star:[code](https://github.com/PJLab-ADG/PCSeg)
* [3D Segmentation of Humans in Point Clouds with Synthetic Data](http://arxiv.org/abs/2212.00786)
* [MasQCLIP for Open-Vocabulary Universal Image Segmentation](https://openaccess.thecvf.com/content/ICCV2023/papers/Xu_MasQCLIP_for_Open-Vocabulary_Universal_Image_Segmentation_ICCV_2023_paper.pdf)
* [FreeCOS: Self-Supervised Learning from Fractals and Unlabeled Images for Curvilinear Object Segmentation](http://arxiv.org/abs/2307.07245)
* [Rethinking Range View Representation for LiDAR Segmentation](http://arxiv.org/abs/2303.05367)
* å¼•ç”¨è¡¨è¾¾å¼åˆ†å‰²
  * [Advancing Referring Expression Segmentation Beyond Single Image](http://arxiv.org/abs/2305.12452)
* æŒ‡ä»£å›¾åƒåˆ†å‰²
  * [Bridging Vision and Language Encoders: Parameter-Efficient Tuning for Referring Image Segmentation](http://arxiv.org/abs/2307.11545v1)<br>:star:[code](https://github.com/kkakkkka/ETRIS)
  * [Beyond One-to-One: Rethinking the Referring Image Segmentation](http://arxiv.org/abs/2308.13853v1)<br>:star:[code](https://github.com/toggle1995/RIS-DMMI)
  * [Referring Image Segmentation Using Text Supervision](http://arxiv.org/abs/2308.14575v1)<br>:star:[code](https://github.com/fawnliu/TRIS)
  * [Shatter and Gather: Learning Referring Image Segmentation with Text Supervision](http://arxiv.org/abs/2308.15512v1)
  * [Weakly Supervised Referring Image Segmentation with Intra-Chunk and Inter-Chunk Consistency](https://openaccess.thecvf.com/content/ICCV2023/papers/Lee_Weakly_Supervised_Referring_Image_Segmentation_with_Intra-Chunk_and_Inter-Chunk_Consistency_ICCV_2023_paper.pdf)
* å°æ ·æœ¬åˆ†å‰²
  * [Self-Calibrated Cross Attention Network for Few-Shot Segmentation](http://arxiv.org/abs/2308.09294v1)<br>:star:[code](https://github.com/Sam1224/SCCAN)
* è¯­ä¹‰åˆ†å‰²
  * [A Good Student is Cooperative and Reliable: CNN-Transformer Collaborative Learning for Semantic Segmentation](http://arxiv.org/abs/2307.12574v1)
  * [Adaptive Superpixel for Active Learning in Semantic Segmentation](http://arxiv.org/abs/2303.16817)
  * [Dynamic Token Pruning in Plain Vision Transformers for Semantic Segmentation](http://arxiv.org/abs/2308.01045)
  * [MemorySeg: Online LiDAR Semantic Segmentation with a Latent Memory](https://openaccess.thecvf.com/content/ICCV2023/papers/Li_MemorySeg_Online_LiDAR_Semantic_Segmentation_with_a_Latent_Memory_ICCV_2023_paper.pdf)
  * [Disentangle then Parse: Night-time Semantic Segmentation with Illumination Disentanglement](http://arxiv.org/abs/2307.09362)
  * [DiffuMask: Synthesizing Images with Pixel-level Annotations for Semantic Segmentation Using Diffusion Models](http://arxiv.org/abs/2303.11681)
  * [Boosting Semantic Segmentation from the Perspective of Explicit Class Embeddings](http://arxiv.org/abs/2308.12894)
  * [Informative Data Mining for One-Shot Cross-Domain Semantic Segmentation](http://arxiv.org/abs/2309.14241v1)<br>:star:[code](https://github.com/yxiwang/IDM)
  * [CMDA: Cross-Modality Domain Adaptation for Nighttime Semantic Segmentation](http://arxiv.org/abs/2307.15942v1)<br>:star:[code](https://github.com/XiaRho/CMDA)
  * [To Adapt or Not to Adapt? Real-Time Adaptation for Semantic Segmentation](http://arxiv.org/abs/2307.15063v1)<br>:star:[code](https://marcbotet.github.io/hamlet-web/)
  * [Residual Pattern Learning for Pixel-Wise Out-of-Distribution Detection in Semantic Segmentation](http://arxiv.org/abs/2211.14512)
  * [Look at the Neighbor: Distortion-aware Unsupervised Domain Adaptation for Panoramic Semantic Segmentation](http://arxiv.org/abs/2308.05493v1)
  * [SVQNet: Sparse Voxel-Adjacent Query Network for 4D Spatio-Temporal LiDAR Semantic Segmentation](http://arxiv.org/abs/2308.13323v1)
  * [Preparing the Future for Continual Semantic Segmentation](https://openaccess.thecvf.com/content/ICCV2023/papers/Lin_Preparing_the_Future_for_Continual_Semantic_Segmentation_ICCV_2023_paper.pdf)
  * ç‚¹äº‘è¯­ä¹‰åˆ†å‰²
    * [Retro-FPN: Retrospective Feature Pyramid Network for Point Cloud Semantic Segmentation](http://arxiv.org/abs/2308.09314v1)<br>:star:[code](https://github.com/AllenXiangX/Retro-FPN)
    * [Using a Waffle Iron for Automotive Point Cloud Semantic Segmentation](http://arxiv.org/abs/2301.10100)
    * [Label-Guided Knowledge Distillation for Continual Semantic Segmentation on 2D Images and 3D Point Clouds](https://openaccess.thecvf.com/content/ICCV2023/papers/Yang_Label-Guided_Knowledge_Distillation_for_Continual_Semantic_Segmentation_on_2D_Images_ICCV_2023_paper.pdf)
  * å°æ ·æœ¬è¯­ä¹‰åˆ†å‰²
    * [Prototypical Kernel Learning and Open-set Foreground Perception for Generalized Few-shot Semantic Segmentation](http://arxiv.org/abs/2308.04952v1)
  * æ— ç›‘ç£è¯­ä¹‰åˆ†å‰²
    * [Learning Neural Eigenfunctions for Unsupervised Semantic Segmentation](http://arxiv.org/abs/2304.02841)
  * å¼±ç›‘ç£è¯­ä¹‰åˆ†å‰²
    * [MARS: Model-agnostic Biased Object Removal without Additional Supervision for Weakly-Supervised Semantic Segmentation](http://arxiv.org/abs/2304.09913)
    * [USAGE: A Unified Seed Area Generation Paradigm for Weakly Supervised Semantic Segmentation](http://arxiv.org/abs/2303.07806)
    * [FPR: False Positive Rectification for Weakly Supervised Semantic Segmentation](https://openaccess.thecvf.com/content/ICCV2023/papers/Chen_FPR_False_Positive_Rectification_for_Weakly_Supervised_Semantic_Segmentation_ICCV_2023_paper.pdf)
  * åŠç›‘ç£è¯­ä¹‰åˆ†å‰²
    * [Space Engage: Collaborative Space Supervision for Contrastive-Based Semi-Supervised Semantic Segmentation](http://arxiv.org/abs/2307.09755)
    * [Large-Scale Land Cover Mapping with Fine-Grained Classes via Class-Aware Semi-Supervised Semantic Segmentation](https://openaccess.thecvf.com/content/ICCV2023/papers/Dong_Large-Scale_Land_Cover_Mapping_with_Fine-Grained_Classes_via_Class-Aware_Semi-Supervised_ICCV_2023_paper.pdf)
    * [Semi-Supervised Semantic Segmentation under Label Noise via Diverse Learning Groups](https://openaccess.thecvf.com/content/ICCV2023/papers/Li_Semi-Supervised_Semantic_Segmentation_under_Label_Noise_via_Diverse_Learning_Groups_ICCV_2023_paper.pdf)
    * [Logic-induced Diagnostic Reasoning for Semi-supervised Semantic Segmentation](http://arxiv.org/abs/2308.12595v1)<br>:star:[code](https://github.com/leonnnop/LogicDiag)
    * [Enhanced Soft Label for Semi-Supervised Semantic Segmentation](https://openaccess.thecvf.com/content/ICCV2023/papers/Ma_Enhanced_Soft_Label_for_Semi-Supervised_Semantic_Segmentation_ICCV_2023_paper.pdf)
    * [CFCG: Semi-Supervised Semantic Segmentation via Cross-Fusion and Contour Guidance Supervision](https://openaccess.thecvf.com/content/ICCV2023/papers/Li_CFCG_Semi-Supervised_Semantic_Segmentation_via_Cross-Fusion_and_Contour_Guidance_Supervision_ICCV_2023_paper.pdf)
    * [XNet: Wavelet-Based Low and High Frequency Fusion Networks for Fully- and Semi-Supervised Semantic Segmentation of Biomedical Images](https://openaccess.thecvf.com/content/ICCV2023/papers/Zhou_XNet_Wavelet-Based_Low_and_High_Frequency_Fusion_Networks_for_Fully-_ICCV_2023_paper.pdf)
  * åŸŸé€‚åº”è¯­ä¹‰åˆ†å‰²
    * [Diffusion-based Image Translation with Label Guidance for Domain Adaptive Semantic Segmentation](http://arxiv.org/abs/2308.12350v1)
    * [Focus on Your Target: A Dual Teacher-Student Framework for Domain-Adaptive Semantic Segmentation](http://arxiv.org/abs/2303.09083)
    * [CDAC: Cross-domain Attention Consistency in Transformer for Domain Adaptive Semantic Segmentation](https://openaccess.thecvf.com/content/ICCV2023/papers/Wang_CDAC_Cross-domain_Attention_Consistency_in_Transformer_for_Domain_Adaptive_Semantic_ICCV_2023_paper.pdf)
  * å¼€æ”¾è¯æ±‡è¯­ä¹‰åˆ†å‰²
    * [Exploring Open-Vocabulary Semantic Segmentation from CLIP Vision Encoder Distillation Only](https://openaccess.thecvf.com/content/ICCV2023/papers/Chen_Exploring_Open-Vocabulary_Semantic_Segmentation_from_CLIP_Vision_Encoder_Distillation_Only_ICCV_2023_paper.pdf)
    * [Global Knowledge Calibration for Fast Open-Vocabulary Segmentation](http://arxiv.org/abs/2303.09181)
    * [A Simple Framework for Open-Vocabulary Segmentation and Detection](http://arxiv.org/abs/2303.08131)
    * [Open-Vocabulary Semantic Segmentation with Decoupled One-Pass Network](http://arxiv.org/abs/2304.01198)
  * å¼€æ”¾ä¸–ç•Œè¯­ä¹‰åˆ†å‰²
    * [MixReorg: Cross-Modal Mixed Patch Reorganization is a Good Mask Learner for Open-World Semantic Segmentation](http://arxiv.org/abs/2308.04829)
  * 3D è¯­ä¹‰åˆ†å‰²
    * [BEV-DG: Cross-Modal Learning under Bird's-Eye View for Domain Generalization of 3D Semantic Segmentation](http://arxiv.org/abs/2308.06530v1)
    * [Multi-Modal Continual Test-Time Adaptation for 3D Semantic Segmentation](http://arxiv.org/abs/2303.10457)
* å®ä¾‹åˆ†å‰²
  * [BoxSnake: Polygonal Instance Segmentation with Box Supervision](http://arxiv.org/abs/2303.11630)
  * [MUVA: A New Large-Scale Benchmark for Multi-View Amodal Instance Segmentation in the Shopping Scenario](https://openaccess.thecvf.com/content/ICCV2023/papers/Li_MUVA_A_New_Large-Scale_Benchmark_for_Multi-View_Amodal_Instance_Segmentation_ICCV_2023_paper.pdf)
  * [WaterMask: Instance Segmentation for Underwater Imagery](https://openaccess.thecvf.com/content/ICCV2023/papers/Lian_WaterMask_Instance_Segmentation_for_Underwater_Imagery_ICCV_2023_paper.pdf)
  * 3D å®ä¾‹åˆ†å‰²
    * [Mask-Attention-Free Transformer for 3D Instance Segmentation](http://arxiv.org/abs/2309.01692v1)<br>:star:[code](https://github.com/dvlab-research/Mask-Attention-Free-Transformer)
  * [Query Refinement Transformer for 3D Instance Segmentation](https://openaccess.thecvf.com/content/ICCV2023/papers/Lu_Query_Refinement_Transformer_for_3D_Instance_Segmentation_ICCV_2023_paper.pdf)
  * ç‚¹äº‘å®ä¾‹åˆ†å‰²
    * [Divide and Conquer: 3D Point Cloud Instance Segmentation With Point-Wise Binarization](http://arxiv.org/abs/2207.11209)
  * ç»†èƒå®ä¾‹åˆ†å‰²
    * [Unsupervised Learning of Object-Centric Embeddings for Cell Instance Segmentation in Microscopy Images](https://openaccess.thecvf.com/content/ICCV2023/papers/Wolf_Unsupervised_Learning_of_Object-Centric_Embeddings_for_Cell_Instance_Segmentation_in_ICCV_2023_paper.pdf)
  * åŠç›‘ç£å®ä¾‹åˆ†å‰²
    * [Pseudo-label Alignment for Semi-supervised Instance Segmentation](http://arxiv.org/abs/2308.05359v1)<br>:star:[code](https://github.com/hujiecpp/PAIS)
  * å¼€æ”¾ä¸–ç•Œå®ä¾‹åˆ†å‰²
    * [Exploring Transformers for Open-world Instance Segmentation](http://arxiv.org/abs/2308.04206v1)
  * å¼€æ”¾è¯æ±‡å®ä¾‹åˆ†å‰²
    * [Betrayed by Captions: Joint Caption Grounding and Generation for Open Vocabulary Instance Segmentation](http://arxiv.org/abs/2301.00805)
* å…¨æ™¯åˆ†å‰²
  * [Towards Deeply Unified Depth-aware Panoptic Segmentation with Bi-directional Guidance Learning](http://arxiv.org/abs/2307.14786v1)
  * [A Generalist Framework for Panoptic Segmentation of Images and Videos](http://arxiv.org/abs/2210.06366)
  * [Open-vocabulary Panoptic Segmentation with Embedding Modulation](http://arxiv.org/abs/2303.11324)
  * [4D Panoptic Segmentation as Invariant and Equivariant Field Prediction](http://arxiv.org/abs/2303.15651)
  * [EDAPS: Enhanced Domain-Adaptive Panoptic Segmentation](http://arxiv.org/abs/2304.14291)
  * [Point2Mask: Point-supervised Panoptic Segmentation via Optimal Transport](http://arxiv.org/abs/2308.01779v1)<br>:star:[code](https://github.com/LiWentomng/Point2Mask)
  * [LiDAR-Camera Panoptic Segmentation via Geometry-Consistent and Semantic-Aware Alignment](http://arxiv.org/abs/2308.01686v1)<br>:star:[code](https://github.com/zhangzw12319/lcps.git)
* å¼€æ”¾è¯æ±‡éƒ¨åˆ†åˆ†å‰²
  * [Going Denser with Open-Vocabulary Part Segmentation](http://arxiv.org/abs/2305.11173)
* VIS
  * [CTVIS: Consistent Training for Online Video Instance Segmentation](http://arxiv.org/abs/2307.12616v1)<br>:star:[code](https://github.com/KainingYing/CTVIS)
  * [DVIS: Decoupled Video Instance Segmentation Framework](https://arxiv.org/pdf/2306.03413.pdf)<br>:star:[code](https://github.com/zhang-tao-whu/DVIS)<br>:thumbsup:[ICCV 2023 | å‘æŒ¥offlineæ–¹æ³•çš„æ½œåŠ›ï¼Œæ­¦å¤§&å¿«æ‰‹æå‡ºè§£è€¦åˆçš„è§†é¢‘å®ä¾‹åˆ†å‰²æ¡†æ¶DVIS](https://mp.weixin.qq.com/s/_MlryCfg_rRMZMfgINPwXw)
  * [TCOVIS: Temporally Consistent Online Video Instance Segmentation](http://arxiv.org/abs/2309.11857v1)<br>:star:[code](https://github.com/jun-long-li/TCOVIS)
  * [Towards Open-Vocabulary Video Instance Segmentation](http://arxiv.org/abs/2304.01715)<br>:thumbsup:[é¢å‘å¼€æ”¾ä»»åŠ¡çš„è§†é¢‘å®ä¾‹åˆ†å‰²ï¼ˆ Oral )](https://mp.weixin.qq.com/s/IUTfQo6FJoB14oZFyyeVmQ)
* VOS
  * [Spectrum-guided Multi-granularity Referring Video Object Segmentation](http://arxiv.org/abs/2307.13537v1)<br>:star:[code](https://github.com/bo-miao/SgMg)
  * [Video Object Segmentation-aware Video Frame Interpolation](https://openaccess.thecvf.com/content/ICCV2023/papers/Yoo_Video_Object_Segmentation-aware_Video_Frame_Interpolation_ICCV_2023_paper.pdf)
  * [LVOS: A Benchmark for Long-term Video Object Segmentation](http://arxiv.org/abs/2211.10181)
  * [Robust Referring Video Object Segmentation with Cyclic Structural Consensus](https://openaccess.thecvf.com/content/ICCV2023/papers/Li_Robust_Referring_Video_Object_Segmentation_with_Cyclic_Structural_Consensus_ICCV_2023_paper.pdf)
  * [Unsupervised Video Object Segmentation with Online Adversarial Self-Tuning](https://openaccess.thecvf.com/content/ICCV2023/papers/Su_Unsupervised_Video_Object_Segmentation_with_Online_Adversarial_Self-Tuning_ICCV_2023_paper.pdf)
  * [Alignment Before Aggregation: Trajectory Memory Retrieval Network for Video Object Segmentation](https://openaccess.thecvf.com/content/ICCV2023/papers/Sun_Alignment_Before_Aggregation_Trajectory_Memory_Retrieval_Network_for_Video_Object_ICCV_2023_paper.pdf)
  * [Multi-grained Temporal Prototype Learning for Few-shot Video Object Segmentation](http://arxiv.org/abs/2309.11160v1)<br>:star:[code](https://github.com/nankepan/VIPMT)
  * [Temporal Collection and Distribution for Referring Video Object Segmentation](http://arxiv.org/abs/2309.03473v1)<br>:star:[code](https://toneyaya.github.io/tempcd/)
  * [OnlineRefer: A Simple Online Baseline for Referring Video Object Segmentation](https://arxiv.org/abs/2307.09356)<br>:star:[code](https://github.com/wudongming97/OnlineRefer)
  * [Isomer: Isomerous Transformer for Zero-shot Video Object Segmentation](http://arxiv.org/abs/2308.06693v1)<br>:star:[code](https://github.com/DLUT-yyc/Isomer)
  * [Scalable Video Object Segmentation with Simplified Framework](http://arxiv.org/abs/2308.09903v1)
  * [Learning Cross-Modal Affinity for Referring Video Object Segmentation Targeting Limited Samples](http://arxiv.org/abs/2309.02041v1)<br>:star:[code](https://github.com/hengliusky/Few_shot_RVOS)
* åŠ¨ä½œåˆ†å‰²
  * [How Much Temporal Long-Term Context is Needed for Action Segmentation?](http://arxiv.org/abs/2308.11358v1)
  * [Markov Game Video Augmentation for Action Segmentation](https://openaccess.thecvf.com/content/ICCV2023/papers/Aziere_Markov_Game_Video_Augmentation_for_Action_Segmentation_ICCV_2023_paper.pdf)
  * [Weakly-Supervised Action Segmentation and Unseen Error Detection in Anomalous Instructional Videos](https://openaccess.thecvf.com/content/ICCV2023/papers/Ghoddoosian_Weakly-Supervised_Action_Segmentation_and_Unseen_Error_Detection_in_Anomalous_Instructional_ICCV_2023_paper.pdf)
* äº¤äº’åˆ†å‰²
  * [Multi-granularity Interaction Simulation for Unsupervised Interactive Segmentation](http://arxiv.org/abs/2303.13399)
* åŸºäºæ–‡æœ¬çš„å›¾åƒåˆ†å‰²
  * [LD-ZNet: A Latent Diffusion Approach for Text-Based Image Segmentation](https://openaccess.thecvf.com/content/ICCV2023/papers/PNVR_LD-ZNet_A_Latent_Diffusion_Approach_for_Text-Based_Image_Segmentation_ICCV_2023_paper.pdf)
* åœºæ™¯è§£æ
  * [Re:PolyWorld - A Graph Neural Network for Polygonal Scene Parsing](https://openaccess.thecvf.com/content/ICCV2023/papers/Zorzi_RePolyWorld_-_A_Graph_Neural_Network_for_Polygonal_Scene_Parsing_ICCV_2023_paper.pdf)

<a name="7"/>

## 7.Image Progress(ä½å±‚å›¾åƒå¤„ç†ã€è´¨é‡è¯„ä»·)
* [DRAW: Defending Camera-shooted RAW against Image Manipulation](http://arxiv.org/abs/2307.16418v1)
* [Towards Generic Image Manipulation Detection with Weakly-Supervised Self-Consistency Learning](http://arxiv.org/abs/2309.01246v1)<br>:star:[code](https://github.com/yhZhai/WSCL)
* [Improving Lens Flare Removal with General Purpose Pipeline and Multiple Light Sources Recovery](http://arxiv.org/abs/2308.16460v1)
* å›¾åƒä¿®é¥°
  * [RSFNet: A White-Box Image Retouching Approach using Region-Specific Color Filters](http://arxiv.org/abs/2303.08682)
* å›¾åƒæ¢å¤
  * [Physics-Driven Turbulence Image Restoration with Stochastic Refinement](http://arxiv.org/abs/2307.10603v1)<br>:star:[code](https://github.com/VITA-Group/PiRN)
  * [DiffIR: Efficient Diffusion Model for Image Restoration](http://arxiv.org/abs/2303.09472)
  * [DDS2M: Self-Supervised Denoising Diffusion Spatio-Spectral Model for Hyperspectral Image Restoration](http://arxiv.org/abs/2303.06682)
  * [Under-Display Camera Image Restoration with Scattering Effect](http://arxiv.org/abs/2308.04163v1)<br>:star:[code](https://github.com/NamecantbeNULL/SRUDC)
  * [Learning Image-Adaptive Codebooks for Class-Agnostic Image Restoration](http://arxiv.org/abs/2306.06513)
  * [Fingerprinting Deep Image Restoration Models](https://openaccess.thecvf.com/content/ICCV2023/papers/Quan_Fingerprinting_Deep_Image_Restoration_Models_ICCV_2023_paper.pdf)
  * [Focal Network for Image Restoration](https://openaccess.thecvf.com/content/ICCV2023/papers/Cui_Focal_Network_for_Image_Restoration_ICCV_2023_paper.pdf)
  * [FSI: Frequency and Spatial Interactive Learning for Image Restoration in Under-Display Cameras](https://openaccess.thecvf.com/content/ICCV2023/papers/Liu_FSI_Frequency_and_Spatial_Interactive_Learning_for_Image_Restoration_in_ICCV_2023_paper.pdf)
  * [Self-supervised Monocular Underwater Depth Recovery, Image Restoration, and a Real-sea Video Dataset](https://openaccess.thecvf.com/content/ICCV2023/papers/Varghese_Self-supervised_Monocular_Underwater_Depth_Recovery_Image_Restoration_and_a_Real-sea_ICCV_2023_paper.pdf)
* å›¾åƒ/è§†é¢‘ä¿®å¤
  * [Rethinking Fast Fourier Convolution in Image Inpainting](https://openaccess.thecvf.com/content/ICCV2023/papers/Chu_Rethinking_Fast_Fourier_Convolution_in_Image_Inpainting_ICCV_2023_paper.pdf)
  * [Continuously Masked Transformer for Image Inpainting](https://openaccess.thecvf.com/content/ICCV2023/papers/Ko_Continuously_Masked_Transformer_for_Image_Inpainting_ICCV_2023_paper.pdf)
  * [Semantic-Aware Dynamic Parameter for Video Inpainting Transformer](https://openaccess.thecvf.com/content/ICCV2023/papers/Lee_Semantic-Aware_Dynamic_Parameter_for_Video_Inpainting_Transformer_ICCV_2023_paper.pdf)
  * [MI-GAN: A Simple Baseline for Image Inpainting on Mobile Devices](https://openaccess.thecvf.com/content/ICCV2023/papers/Sargsyan_MI-GAN_A_Simple_Baseline_for_Image_Inpainting_on_Mobile_Devices_ICCV_2023_paper.pdf)
  * [CIRI: Curricular Inactivation for Residue-aware One-shot Video Inpainting](https://openaccess.thecvf.com/content/ICCV2023/papers/Zheng_CIRI_Curricular_Inactivation_for_Residue-aware_One-shot_Video_Inpainting_ICCV_2023_paper.pdf)
* å›¾åƒ/è§†é¢‘å¢å¼º
  * [Coherent Event Guided Low-Light Video Enhancement](https://openaccess.thecvf.com/content/ICCV2023/papers/Liang_Coherent_Event_Guided_Low-Light_Video_Enhancement_ICCV_2023_paper.pdf)
  * [Lighting up NeRF via Unsupervised Decomposition and Enhancement](http://arxiv.org/abs/2307.10664v1)<br>:house:[project](https://whyy.site/paper/llnerf)
  * [Implicit Neural Representation for Cooperative Low-light Image Enhancement](https://arxiv.org/pdf/2303.11722.pdf)<br>:star:[code](https://github.com/Ysz2022/NeRCo)<br>:thumbsup:[ICCV2023 | å°†éšå¼ç¥ç»è¡¨å¾ç”¨äºâ€œä½å…‰å¢å¼ºâ€ï¼ŒåŒ—å¤§å¼ å¥å›¢é˜Ÿæå‡ºNeRCo](https://mp.weixin.qq.com/s/nkalW0aZqkoIZHW5SX91Ag)
  * [Retinexformer: One-stage Retinex-based Transformer for Low-light Image Enhancement](https://arxiv.org/abs/2303.06705)<br>:star:[code](https://github.com/caiyuanhao1998/Retinexformer)<br>:thumbsup:[ICCV 2023 æ¸…åETHæå‡º Retinexformer åˆ·æ–°åä¸‰å¤§æš—å…‰å¢å¼ºæ¦œå•](https://mp.weixin.qq.com/s/NI1-J1JqAT7jV9_HbQ2VxA)
  * [Low-Light Image Enhancement with Illumination-Aware Gamma Correction and Complete Image Modelling Network](http://arxiv.org/abs/2308.08220v1)
  * [Diff-Retinex: Rethinking Low-light Image Enhancement with A Generative Diffusion Model](http://arxiv.org/abs/2308.13164v1)
  * [Empowering Low-Light Image Enhancer through Customized Learnable Priors](http://arxiv.org/abs/2309.01958v1)<br>:star:[code](https://github.com/zheng980629/CUE)
  * [ExposureDiffusion: Learning to Expose for Low-light Image Enhancement](http://arxiv.org/abs/2307.07710)
  * [NIR-assisted Video Enhancement via Unpaired 24-hour Data](https://openaccess.thecvf.com/content/ICCV2023/papers/Niu_NIR-assisted_Video_Enhancement_via_Unpaired_24-hour_Data_ICCV_2023_paper.pdf)
  * [Low-Light Image Enhancement with Multi-Stage Residue Quantization and Brightness-Aware Attention](https://openaccess.thecvf.com/content/ICCV2023/papers/Liu_Low-Light_Image_Enhancement_with_Multi-Stage_Residue_Quantization_and_Brightness-Aware_Attention_ICCV_2023_paper.pdf)
  * [Iterative Prompt Learning for Unsupervised Backlit Image Enhancement](http://arxiv.org/abs/2303.17569)
* å›¾åƒ/è§†é¢‘å»é›¨
  * [Sparse Sampling Transformer with Uncertainty-Driven Ranking for Unified Removal of Raindrops and Rain Streaks](http://arxiv.org/abs/2308.14153v1)
  * [Learning Rain Location Prior for Nighttime Deraining](https://openaccess.thecvf.com/content/ICCV2023/papers/Zhang_Learning_Rain_Location_Prior_for_Nighttime_Deraining_ICCV_2023_paper.pdf)
  * [Unsupervised Video Deraining with An Event Camera](https://openaccess.thecvf.com/content/ICCV2023/papers/Wang_Unsupervised_Video_Deraining_with_An_Event_Camera_ICCV_2023_paper.pdf)
* å›¾åƒå»å™ª
  * [Delta Denoising Score](http://arxiv.org/abs/2304.07090)
  * [Random Sub-Samples Generation for Self-Supervised Real Image Denoising](http://arxiv.org/abs/2307.16825v1)<br>:star:[code](https://github.com/p1y2z3/SDAP)
  * [Denoising Diffusion Autoencoders are Unified Self-supervised Learners](http://arxiv.org/abs/2303.09769)
  * [Noise2Info: Noisy Image to Information of Noise for Self-Supervised Image Denoising](https://openaccess.thecvf.com/content/ICCV2023/papers/Wang_Noise2Info_Noisy_Image_to_Information_of_Noise_for_Self-Supervised_Image_ICCV_2023_paper.pdf)
  * [Iterative Denoiser and Noise Estimator for Self-Supervised Image Denoising](https://openaccess.thecvf.com/content/ICCV2023/papers/Zou_Iterative_Denoiser_and_Noise_Estimator_for_Self-Supervised_Image_Denoising_ICCV_2023_paper.pdf)
  * [Self-supervised Image Denoising with Downsampled Invariance Loss and Conditional Blind-Spot Network](http://arxiv.org/abs/2304.09507)
  * [The Devil is in the Upsampling: Architectural Decisions Made Simpler for Denoising with Deep Image Prior](http://arxiv.org/abs/2304.11409)
  * [Lighting Every Darkness in Two Pairs: A Calibration-Free Pipeline for RAW Denoising](http://arxiv.org/abs/2308.03448v1)<br>:star:[code](https://github.com/Srameo/LED)
  * [Score Priors Guided Deep Variational Inference for Unsupervised Real-World Single Image Denoising](http://arxiv.org/abs/2308.04682v1)
  * [Unsupervised Image Denoising in Real-World Scenarios via Self-Collaboration Parallel Generative Adversarial Branches](http://arxiv.org/abs/2308.06776v1)
  * [Multi-view Self-supervised Disentanglement for General Image Denoising](http://arxiv.org/abs/2309.05049v1)
  * [RED-PSM: Regularization by Denoising of Partially Separable Models for Dynamic Imaging](https://openaccess.thecvf.com/content/ICCV2023/papers/Iskender_RED-PSM_Regularization_by_Denoising_of_Partially_Separable_Models_for_Dynamic_ICCV_2023_paper.pdf)
  * [Hybrid Spectral Denoising Transformer with Guided Attention](http://arxiv.org/abs/2303.09040)
* å›¾åƒå»é›¾
  * [MB-TaylorFormer: Multi-branch Efficient Transformer Expanded by Taylor Formula for Image Dehazing](https://arxiv.org/abs/2308.14036)<br>:star:[code](https://github.com/FVL2020/ICCV-2023-MB-TaylorFormer)<br>:thumbsup:[ICCV2023 | æ›´å¿«ã€æ›´çµæ´»çš„ Transformerå›¾åƒå»é›¾ç½‘ç»œ](https://mp.weixin.qq.com/s/nZ9WFBci0WyeBe1Fyi3rnA)
* å›¾åƒå»é™¤é˜´å½±
  * [Boundary-Aware Divide and Conquer: A Diffusion-Based Solution for Unsupervised Shadow Removal](https://openaccess.thecvf.com/content/ICCV2023/papers/Guo_Boundary-Aware_Divide_and_Conquer_A_Diffusion-Based_Solution_for_Unsupervised_Shadow_ICCV_2023_paper.pdf)
* å›¾åƒ/è§†é¢‘å»æ¨¡ç³Š
  * [Exploring Temporal Frequency Spectrum in Deep Video Deblurring](https://openaccess.thecvf.com/content/ICCV2023/papers/Zhu_Exploring_Temporal_Frequency_Spectrum_in_Deep_Video_Deblurring_ICCV_2023_paper.pdf)
  * [Multi-Scale Residual Low-Pass Filter Network for Image Deblurring](https://openaccess.thecvf.com/content/ICCV2023/papers/Dong_Multi-Scale_Residual_Low-Pass_Filter_Network_for_Image_Deblurring_ICCV_2023_paper.pdf)
  * [Multiscale Structure Guided Diffusion for Image Deblurring](http://arxiv.org/abs/2212.01789)
  * [Single Image Defocus Deblurring via Implicit Neural Inverse Kernels](https://openaccess.thecvf.com/content/ICCV2023/papers/Quan_Single_Image_Defocus_Deblurring_via_Implicit_Neural_Inverse_Kernels_ICCV_2023_paper.pdf)
  * [Non-Coaxial Event-Guided Motion Deblurring with Spatial Alignment](https://openaccess.thecvf.com/content/ICCV2023/papers/Cho_Non-Coaxial_Event-Guided_Motion_Deblurring_with_Spatial_Alignment_ICCV_2023_paper.pdf)
  * [Deep Feature Deblurring Diffusion for Detecting Out-of-Distribution Objects](https://openaccess.thecvf.com/content/ICCV2023/papers/Wu_Deep_Feature_Deblurring_Diffusion_for_Detecting_Out-of-Distribution_Objects_ICCV_2023_paper.pdf)
* å›¾åƒ/è§†é¢‘åŒºæ‘©å°”çº¹
  * [Deep Video Demoireing via Compact Invertible Dyadic Decomposition](https://openaccess.thecvf.com/content/ICCV2023/papers/Quan_Deep_Video_Demoireing_via_Compact_Invertible_Dyadic_Decomposition_ICCV_2023_paper.pdf)
* å»é©¬èµ›å…‹/å»é¬¼å½±
  * [Joint Demosaicing and Deghosting of Time-Varying Exposures for Single-Shot HDR Imaging](https://openaccess.thecvf.com/content/ICCV2023/papers/Kim_Joint_Demosaicing_and_Deghosting_of_Time-Varying_Exposures_for_Single-Shot_HDR_ICCV_2023_paper.pdf)
* è´¨é‡è¯„ä¼°
  * [Test Time Adaptation for Blind Image Quality Assessment](http://arxiv.org/abs/2307.14735v1)
* å›¾åƒå’Œè°åŒ–
  * [Learning Global-aware Kernel for Image Harmonization](https://arxiv.org/pdf/2305.11676.pdf)
  * [Deep Image Harmonization with Learnable Augmentation](http://arxiv.org/abs/2308.00376v1)<br>:star:[code](https://github.com/bcmi/SycoNet-Adaptive-Image-Harmonization)
  * [Deep Image Harmonization with Globally Guided Feature Transformation and Relation Distillation](http://arxiv.org/abs/2308.00356v1)<br>:star:[code](https://github.com/bcmi/Image-Harmonization-Dataset-ccHarmony)
  * [Learning Image Harmonization in the Linear Color Space](https://openaccess.thecvf.com/content/ICCV2023/papers/Xu_Learning_Image_Harmonization_in_the_Linear_Color_Space_ICCV_2023_paper.pdf)
* å›¾åƒæ ¡æ­£
  * [SimFIR: A Simple Framework for Fisheye Image Rectification with Self-supervised Representation Learning](http://arxiv.org/abs/2308.09040v1)
* å›¾åƒæ‹¼æ¥
  * [Parallax-Tolerant Unsupervised Deep Image Stitching](http://arxiv.org/abs/2302.08207)
* å›¾åƒç€è‰²
  * [DDColor: Towards Photo-Realistic Image Colorization via Dual Decoders](http://arxiv.org/abs/2212.11613)
* å›¾åƒ/è§†é¢‘åˆ†è§£
  * [Variational Degeneration to Structural Refinement: A Unified Framework for Superimposed Image Decomposition](https://openaccess.thecvf.com/content/ICCV2023/papers/Li_Variational_Degeneration_to_Structural_Refinement_A_Unified_Framework_for_Superimposed_ICCV_2023_paper.pdf)
  * [uSplit: Image Decomposition for Fluorescence Microscopy](https://openaccess.thecvf.com/content/ICCV2023/papers/Ashesh_uSplit_Image_Decomposition_for_Fluorescence_Microscopy_ICCV_2023_paper.pdf)
  * [Hashing Neural Video Decomposition with Multiplicative Residuals in Space-Time](http://arxiv.org/abs/2309.14022)
* è¿åŠ¨å»æ¨¡ç³Š
  * [Generalizing Event-Based Motion Deblurring in Real-World Scenarios](http://arxiv.org/abs/2308.05932)


<a name="6"/>

## 6.Face(äººè„¸)
* [StyleGANEX: StyleGAN-Based Manipulation Beyond Cropped Aligned Faces](http://arxiv.org/abs/2303.06146)
* [Efficient Region-Aware Neural Radiance Fields for High-Fidelity Talking Portrait Synthesis](http://arxiv.org/abs/2307.09323)
* [UniFace: Unified Cross-Entropy Loss for Deep Face Recognition](https://openaccess.thecvf.com/content/ICCV2023/papers/Zhou_UniFace_Unified_Cross-Entropy_Loss_for_Deep_Face_Recognition_ICCV_2023_paper.pdf)
* [Human-Inspired Facial Sketch Synthesis with Dynamic Adaptation](http://arxiv.org/abs/2309.00216v1)<br>:star:[code](https://github.com/AiArt-HDU/HIDA)
* [Can Language Models Learn to Listen?](http://arxiv.org/abs/2308.10897v1)<br>:house:[project](https://people.eecs.berkeley.edu/~evonne_ng/projects/text2listen/)
* å»è¯†åˆ«
  * [Divide and Conquer: a Two-Step Method for High Quality Face De-identification with Model Explainability](https://openaccess.thecvf.com/content/ICCV2023/papers/Wen_Divide_and_Conquer_a_Two-Step_Method_for_High_Quality_Face_ICCV_2023_paper.pdf)
* äººè„¸æ´»ä½“æ£€æµ‹
  * [Towards Unsupervised Domain Generalization for Face Anti-Spoofing](https://openaccess.thecvf.com/content/ICCV2023/papers/Liu_Towards_Unsupervised_Domain_Generalization_for_Face_Anti-Spoofing_ICCV_2023_paper.pdf)
  * [FLIP: Cross-domain Face Anti-spoofing with Language Guidance](https://openaccess.thecvf.com/content/ICCV2023/papers/Srivatsan_FLIP_Cross-domain_Face_Anti-spoofing_with_Language_Guidance_ICCV_2023_paper.pdf)
* è¯´è¯å¤´åˆæˆ
  * [Implicit Identity Representation Conditioned Memory Compensation Network for Talking Head video Generation](https://arxiv.org/abs/2307.09906)<br>:star:[code](https://github.com/harlanhong/ICCV2023-MCNET)
  * [Efficient Emotional Adaptation for Audio-Driven Talking-Head Generation](http://arxiv.org/abs/2309.04946v1)<br>:star:[code](https://yuangan.github.io/eat/)
  * [Talking Head Generation with Probabilistic Audio-to-Visual Diffusion Priors](http://arxiv.org/abs/2212.04248)
* è¯´è¯äººè„¸åˆæˆ
  * [EMMN: Emotional Motion Memory Network for Audio-driven Emotional Talking Face Generation](https://openaccess.thecvf.com/content/ICCV2023/papers/Tan_EMMN_Emotional_Motion_Memory_Network_for_Audio-driven_Emotional_Talking_Face_ICCV_2023_paper.pdf)
* äººè„¸äº¤æ¢
  * [BlendFace: Re-designing Identity Encoders for Face-Swapping](http://arxiv.org/abs/2307.10854v1)<br>:star:[code](https://github.com/mapooon/BlendFace)<br>:star:[code](https://mapooon.github.io/BlendFacePage/)
* å‡è„¸æ£€æµ‹
  * [Controllable Guide-Space for Generalizable Face Forgery Detection](http://arxiv.org/abs/2307.14039v1)
* äººè„¸å†ç°
  * [HyperReenact: One-Shot Reenactment via Jointly Learning to Refine and Retarget Faces](http://arxiv.org/abs/2307.10797v1)<br>:star:[code](https://github.com/StelaBou/HyperReenact)<br>:star:[code](https://stelabou.github.io/hyperreenact.github.io/)
  * [ToonTalker: Cross-Domain Face Reenactment](http://arxiv.org/abs/2308.12866)
* æ–‡æœ¬é©±åŠ¨çš„äººè„¸å¤„ç†
  * [FaceCLIPNeRF: Text-driven 3D Face Manipulation using Deformable Neural Radiance Fields](http://arxiv.org/abs/2307.11418v1)
* äººè„¸è¡¨æƒ…
  * [GaFET: Learning Geometry-aware Facial Expression Translation from In-The-Wild Images](http://arxiv.org/abs/2308.03413v1)
  * [Latent-OFER: Detect, Mask, and Reconstruct with Latent Vectors for Occluded Facial Expression Recognition](https://openaccess.thecvf.com/content/ICCV2023/papers/Lee_Latent-OFER_Detect_Mask_and_Reconstruct_with_Latent_Vectors_for_Occluded_ICCV_2023_paper.pdf)
  * [A-Net: Landmark-Aware Learning for Reliable Facial Expression Recognition under Label Noise](https://openaccess.thecvf.com/content/ICCV2023/papers/Wu_LA-Net_Landmark-Aware_Learning_for_Reliable_Facial_Expression_Recognition_under_Label_ICCV_2023_paper.pdf)
* äººè„¸è¯†åˆ«
  * [IDiff-Face: Synthetic-based Face Recognition through Fizzy Identity-Conditioned Diffusion Models](http://arxiv.org/abs/2308.04995v1)
  * [Invariant Feature Regularization for Fair Face Recognition](https://openaccess.thecvf.com/content/ICCV2023/papers/Ma_Invariant_Feature_Regularization_for_Fair_Face_Recognition_ICCV_2023_paper.pdf)
  * [Benchmarking Algorithmic Bias in Face Recognition: An Experimental Approach Using Synthetic Faces and Human Evaluation](http://arxiv.org/abs/2308.05441v1)
  * [TransFace: Calibrating Transformer Training for Face Recognition from a Data-Centric Perspective](http://arxiv.org/abs/2308.10133v1)<br>:star:[code](https://github.com/DanJun6737/TransFace)
  * [How to Boost Face Recognition with StyleGAN?](https://openaccess.thecvf.com/content/ICCV2023/papers/Sevastopolskiy_How_to_Boost_Face_Recognition_with_StyleGAN_ICCV_2023_paper.pdf)
  * [Privacy-Preserving Face Recognition Using Random Frequency Components](http://arxiv.org/abs/2308.10461v1)<br>:star:[code](https://github.com/Tencent/TFace)
  * [ICD-Face: Intra-class Compactness Distillation for Face Recognition](https://openaccess.thecvf.com/content/ICCV2023/papers/Yu_ICD-Face_Intra-class_Compactness_Distillation_for_Face_Recognition_ICCV_2023_paper.pdf)
* äººè„¸èšç±»
  * [Face Clustering via Graph Convolutional Networks with Confidence Edges](https://openaccess.thecvf.com/content/ICCV2023/papers/Wu_Face_Clustering_via_Graph_Convolutional_Networks_with_Confidence_Edges_ICCV_2023_paper.pdf)
  * [CLIP-Cluster: CLIP-Guided Attribute Hallucination for Face Clustering](https://openaccess.thecvf.com/content/ICCV2023/papers/Shen_CLIP-Cluster_CLIP-Guided_Attribute_Hallucination_for_Face_Clustering_ICCV_2023_paper.pdf)
* äººè„¸åˆæˆ
  * [LPFF: A Portrait Dataset for Face Generators Across Large Poses](http://arxiv.org/abs/2303.14407)
  * [Preface: A Data-driven Volumetric Prior for Few-shot Ultra High-resolution Face Synthesis](https://openaccess.thecvf.com/content/ICCV2023/papers/Buhler_Preface_A_Data-driven_Volumetric_Prior_for_Few-shot_Ultra_High-resolution_Face_ICCV_2023_paper.pdf)
* 3D äººè„¸
  * [Unpaired Multi-domain Attribute Translation of 3D Facial Shapes with a Square and Symmetric Geometric Map](http://arxiv.org/abs/2308.13245)
  * [ASM: Adaptive Skinning Model for High-Quality 3D Face Modeling](http://arxiv.org/abs/2304.09423)
  * [Relightify: Relightable 3D Faces from a Single Image via Diffusion Models](http://arxiv.org/abs/2305.06077)
  * 3Däººè„¸åˆæˆ
    * [Towards High-Fidelity Text-Guided 3D Face Generation and Manipulation Using only Images](http://arxiv.org/abs/2308.16758v1)
  * 3D äººè„¸åŠ¨ç”»
    * [EmoTalk: Speech-Driven Emotional Disentanglement for 3D Face Animation](http://arxiv.org/abs/2303.11089)
    * [Semi-supervised Speech-driven 3D Facial Animation via Cross-modal Encoding](https://openaccess.thecvf.com/content/ICCV2023/papers/Yang_Semi-supervised_Speech-driven_3D_Facial_Animation_via_Cross-modal_Encoding_ICCV_2023_paper.pdf)
    * [Imitator: Personalized Speech-driven 3D Facial Animation](http://arxiv.org/abs/2301.00023)
  * 3D äººè„¸é‡å»º
    * [Template Inversion Attack against Face Recognition Systems using 3D Face Reconstruction](https://openaccess.thecvf.com/content/ICCV2023/papers/Shahreza_Template_Inversion_Attack_against_Face_Recognition_Systems_using_3D_Face_ICCV_2023_paper.pdf)
    * [HiFace: High-Fidelity 3D Face Reconstruction by Learning Static and Dynamic Details](http://arxiv.org/abs/2303.11225)
* äººè„¸è´¨é‡è¯„ä¼°
  * [Troubleshooting Ethnic Quality Bias with Curriculum Domain Adaptation for Face Image Quality Assessment](https://openaccess.thecvf.com/content/ICCV2023/papers/Ou_Troubleshooting_Ethnic_Quality_Bias_with_Curriculum_Domain_Adaptation_for_Face_ICCV_2023_paper.pdf)
* äººè„¸æ¢å¤
  * [Towards Authentic Face Restoration with Iterative Diffusion Models and Beyond](http://arxiv.org/abs/2307.08996)
* äººè„¸ç¼–è¾‘
  * [Unsupervised Facial Performance Editing via Vector-Quantized StyleGAN Representations](https://openaccess.thecvf.com/content/ICCV2023/papers/Kicanaoglu_Unsupervised_Facial_Performance_Editing_via_Vector-Quantized_StyleGAN_Representations_ICCV_2023_paper.pdf)
  * [Conceptual and Hierarchical Latent Space Decomposition for Face Editing](https://openaccess.thecvf.com/content/ICCV2023/papers/Ozkan_Conceptual_and_Hierarchical_Latent_Space_Decomposition_for_Face_Editing_ICCV_2023_paper.pdf)
  * [RIGID: Recurrent GAN Inversion and Editing of Real Face Videos](http://arxiv.org/abs/2308.06097)
* è¯­éŸ³é©±åŠ¨çš„äººåƒåŠ¨ç”»
  * [SPACE: Speech-driven Portrait Animation with Controllable Expression](http://arxiv.org/abs/2211.09809)
* äººè„¸é‡ç…§æ˜
  * [DiFaReli: Diffusion Face Relighting](http://arxiv.org/abs/2304.09479)

<a name="5"/>

## 5.Biometric Recognition(ç”Ÿç‰©ç‰¹å¾è¯†åˆ«)
* æŒçº¹è¯†åˆ«
  * [RPG-Palm: Realistic Pseudo-data Generation for Palmprint Recognition](https://arxiv.org/abs/2307.14016)<br>:star:[code](https://github.com/RayshenSL/RPG-PALM)

<a name="4"/>

## 4.Object Tracking(ç›®æ ‡è·Ÿè¸ª)
* [Tracking Everything Everywhere All at Once](http://arxiv.org/abs/2306.05422)
* [Humans in 4D: Reconstructing and Tracking Humans with Transformers](http://arxiv.org/abs/2305.20091)
* [Cross-Modal Orthogonal High-Rank Augmentation for RGB-Event Transformer-Trackers](http://arxiv.org/abs/2307.04129)
* [Multiple Planar Object Tracking](https://openaccess.thecvf.com/content/ICCV2023/papers/Zhang_Multiple_Planar_Object_Tracking_ICCV_2023_paper.pdf)
* [End-to-end 3D Tracking with Decoupled Queries](https://openaccess.thecvf.com/content/ICCV2023/papers/Li_End-to-end_3D_Tracking_with_Decoupled_Queries_ICCV_2023_paper.pdf)
* å¤šç›®æ ‡è·Ÿè¸ª
  * [MeMOTR: Long-Term Memory-Augmented Transformer for Multi-Object Tracking](http://arxiv.org/abs/2307.15700v1)<br>:star:[code](https://github.com/MCG-NJU/MeMOTR)
  * [DARTH: Holistic Test-time Adaptation for Multiple Object Tracking](https://openaccess.thecvf.com/content/ICCV2023/papers/Segu_DARTH_Holistic_Test-time_Adaptation_for_Multiple_Object_Tracking_ICCV_2023_paper.pdf)
  * [Uncertainty-aware Unsupervised Multi-Object Tracking](http://arxiv.org/abs/2307.15409v1)
  * [TrackFlow: Multi-Object Tracking with Normalizing Flows](http://arxiv.org/abs/2308.11513v1)
  * [3DMOTFormer: Graph Transformer for Online 3D Multi-Object Tracking](http://arxiv.org/abs/2308.06635v1)<br>:star:[code](https://github.com/dsx0511/3DMOTFormer)
  * [ReST: A Reconfigurable Spatial-Temporal Graph Model for Multi-Camera Multi-Object Tracking](http://arxiv.org/abs/2308.13229v1)
  * [Object-Centric Multiple Object Tracking](http://arxiv.org/abs/2309.00233v1)
  * [Heterogeneous Diversity Driven Active Learning for Multi-Object Tracking](https://openaccess.thecvf.com/content/ICCV2023/papers/Li_Heterogeneous_Diversity_Driven_Active_Learning_for_Multi-Object_Tracking_ICCV_2023_paper.pdf)
* è§†è§‰è·Ÿè¸ª
  * [Exploring Lightweight Hierarchical Vision Transformers for Efficient Visual Tracking](http://arxiv.org/abs/2308.06904v1)
  * [CiteTracker: Correlating Image and Text for Visual Tracking](http://arxiv.org/abs/2308.11322v1)
  * [Robust Object Modeling for Visual Tracking](http://arxiv.org/abs/2308.05140v1)<br>:star:[code](https://github.com/dawnyc/ROMTrack)
  * [PVT++: A Simple End-to-End Latency-Aware Visual Tracking Framework](https://openaccess.thecvf.com/content/ICCV2023/papers/Li_PVT_A_Simple_End-to-End_Latency-Aware_Visual_Tracking_Framework_ICCV_2023_paper.pdf)
  * [Integrating Boxes and Masks: A Multi-Object Framework for Unified Visual Tracking and Segmentation](http://arxiv.org/abs/2308.13266v1)<br>:star:[code](https://github.com/yoxu515/MITS)
  * [Foreground-Background Distribution Modeling Transformer for Visual Object Tracking](https://openaccess.thecvf.com/content/ICCV2023/papers/Yang_Foreground-Background_Distribution_Modeling_Transformer_for_Visual_Object_Tracking_ICCV_2023_paper.pdf)
* 3Dç›®æ ‡è·Ÿè¸ª
  * [Delving into Motion-Aware Matching for Monocular 3D Object Tracking](http://arxiv.org/abs/2308.11607v1)<br>:star:[code](https://github.com/kuanchihhuang/MoMA-M3T)
  * [Synchronize Feature Extracting and Matching: A Single Branch Framework for 3D Object Tracking](http://arxiv.org/abs/2308.12549v1)
  * [MixCycle: Mixup Assisted Semi-Supervised 3D Single Object Tracking with Cycle Consistency](http://arxiv.org/abs/2303.09219)
  * [TrajectoryFormer: 3D Object Tracking Transformer with Predictive Trajectory Hypotheses](http://arxiv.org/abs/2306.05888)
  * [MBPTrack: Improving 3D Point Cloud Tracking with Memory Networks and Box Priors](http://arxiv.org/abs/2303.05071)

<a name="3"/>

## 3.Object Detection(ç›®æ ‡æ£€æµ‹)
* [Deep Equilibrium Object Detection](http://arxiv.org/abs/2308.09564)
* [DIRE for Diffusion-Generated Image Detection]()http://arxiv.org/abs/2303.09295
* [Shift from Texture-bias to Shape-bias: Edge Deformation-based Augmentation for Robust Object Recognition](https://openaccess.thecvf.com/content/ICCV2023/papers/He_Shift_from_Texture-bias_to_Shape-bias_Edge_Deformation-based_Augmentation_for_Robust_ICCV_2023_paper.pdf)
* [Multi-Object Discovery by Low-Dimensional Object Motion](https://openaccess.thecvf.com/content/ICCV2023/papers/Safadoust_Multi-Object_Discovery_by_Low-Dimensional_Object_Motion_ICCV_2023_paper.pdf)
* [RecursiveDet: End-to-End Region-Based Recursive Object Detection](http://arxiv.org/abs/2307.13619)
* [StageInteractor: Query-based Object Detector with Cross-stage Interaction](http://arxiv.org/abs/2304.04978)
* [Anchor-Intermediate Detector: Decoupling and Coupling Bounding Boxes for Accurate Object Detection](https://openaccess.thecvf.com/content/ICCV2023/papers/Lv_Anchor-Intermediate_Detector_Decoupling_and_Coupling_Bounding_Boxes_for_Accurate_Object_ICCV_2023_paper.pdf)
* [Uncertainty-guided Learning for Improving Image Manipulation Detection](https://openaccess.thecvf.com/content/ICCV2023/papers/Ji_Uncertainty-guided_Learning_for_Improving_Image_Manipulation_Detection_ICCV_2023_paper.pdf)
* [From Chaos Comes Order: Ordering Event Representations for Object Recognition and Detection](https://openaccess.thecvf.com/content/ICCV2023/papers/Zubic_From_Chaos_Comes_Order_Ordering_Event_Representations_for_Object_Recognition_ICCV_2023_paper.pdf)
* [Unleashing Vanilla Vision Transformer with Masked Image Modeling for Object Detection](http://arxiv.org/abs/2204.02964)
* [FemtoDet: An Object Detection Baseline for Energy Versus Performance Tradeoffs](http://arxiv.org/abs/2301.06719)
* [Periodically Exchange Teacher-Student for Source-Free Object Detection](https://openaccess.thecvf.com/content/ICCV2023/papers/Liu_Periodically_Exchange_Teacher-Student_for_Source-Free_Object_Detection_ICCV_2023_paper.pdf)
* [CoTDet: Affordance Knowledge Prompting for Task Driven Object Detection](http://arxiv.org/abs/2309.01093v1)
* [A Dynamic Dual-Processing Object Detection Framework Inspired by the Brain's Recognition Mechanism](https://openaccess.thecvf.com/content/ICCV2023/papers/Zhang_A_Dynamic_Dual-Processing_Object_Detection_Framework_Inspired_by_the_Brains_ICCV_2023_paper.pdf)
* [ASAG: Building Strong One-Decoder-Layer Sparse Detectors via Adaptive Sparse Anchor Generation](http://arxiv.org/abs/2308.09242v1)<br>:star:[code](https://github.com/iSEE-Laboratory/ASAG)
* [FeatEnHancer: Enhancing Hierarchical Features for Object Detection and Beyond Under Low-Light Vision](http://arxiv.org/abs/2308.03594v1)
* [FB-BEV: BEV Representation from Forward-Backward View Transformations](https://arxiv.org/abs/2308.02236)<br>:star:[code](https://github.com/NVlabs/FB-BEV)
* [DETR Doesn't Need Multi-Scale or Locality Design](http://arxiv.org/abs/2308.01904v1)<br>:star:[code](https://github.com/impiga/Plain-DETR)
* [RecursiveDet: End-to-End Region-based Recursive Object Detection](http://arxiv.org/abs/2307.13619v1)<br>:star:[code](https://github.com/bravezzzzzz/RecursiveDet)
* [Less is More: Focus Attention for Efficient DETR](http://arxiv.org/abs/2307.12612v1)<br>:star:[code](https://github.com/huawei-noah/noah-research/tree/master/Focus-DETR)<br>:house:[project](https://gitee.com/mindspore/models/tree/master/research/cv/Focus-DETR)
* [Spatial Self-Distillation for Object Detection with Inaccurate Bounding Boxes](http://arxiv.org/abs/2307.12101v1)<br>:star:[code](https://github.com/ucas-vg/PointTinyBenchmark/tree/SSD-Det)
* [AlignDet: Aligning Pre-training and Fine-tuning in Object Detection](http://arxiv.org/abs/2307.11077v1)<br>:star:[code](https://liming-ai.github.io/AlignDet)
* [Augmented Box Replay: Overcoming Foreground Shift for Incremental Object Detection](http://arxiv.org/abs/2307.12427v1)
* [Deep Directly-Trained Spiking Neural Networks for Object Detection](http://arxiv.org/abs/2307.11411v1)
* [Cascade-DETR: Delving into High-Quality Universal Object Detection](http://arxiv.org/abs/2307.11035v1)<br>:star:[code](https://github.com/SysCV/cascade-detr)
* [Object-aware Gaze Target Detection](http://arxiv.org/abs/2307.09662v1)<br>:star:[code](https://github.com/francescotonini/object-aware-gaze-target-detection)
* [FS-DETR: Few-Shot DEtection TRansformer with Prompting and without Re-Training](https://openaccess.thecvf.com/content/ICCV2023/papers/Bulat_FS-DETR_Few-Shot_DEtection_TRansformer_with_Prompting_and_without_Re-Training_ICCV_2023_paper.pdf)
* [Adaptive Rotated Convolution for Rotated Object Detection](http://arxiv.org/abs/2303.07820)
* 3D OD
  * [PG-RCNN: Semantic Surface Point Generation for 3D Object Detection](http://arxiv.org/abs/2307.12637v1)<br>:star:[code](https://github.com/quotation2520/PG-RCNN)
  * [GACE: Geometry Aware Confidence Enhancement for Black-Box 3D Object Detectors on LiDAR-Data](https://openaccess.thecvf.com/content/ICCV2023/papers/Schinagl_GACE_Geometry_Aware_Confidence_Enhancement_for_Black-Box_3D_Object_Detectors_ICCV_2023_paper.pdf)
  * [Once Detected, Never Lost: Surpassing Human Performance in Offline LiDAR based 3D Object Detection](http://arxiv.org/abs/2304.12315)
  * [Ada3D : Exploiting the Spatial Redundancy with Adaptive Inference for Efficient 3D Object Detection](http://arxiv.org/abs/2307.08209)
  * [Revisiting Domain-Adaptive 3D Object Detection by Reliable, Diverse and Class-balanced Pseudo-Labeling](http://arxiv.org/abs/2307.07944)
  * [Clusterformer: Cluster-based Transformer for 3D Object Detection in Point Clouds](https://openaccess.thecvf.com/content/ICCV2023/papers/Pei_Clusterformer_Cluster-based_Transformer_for_3D_Object_Detection_in_Point_Clouds_ICCV_2023_paper.pdf)
  * [Object as Query: Lifting Any 2D Object Detector to 3D Detection](http://arxiv.org/abs/2301.02364)
  * [MetaBEV: Solving Sensor Failures for 3D Detection and Map Segmentation](https://openaccess.thecvf.com/content/ICCV2023/papers/Ge_MetaBEV_Solving_Sensor_Failures_for_3D_Detection_and_Map_Segmentation_ICCV_2023_paper.pdf)
  * [SupFusion: Supervised LiDAR-Camera Fusion for 3D Object Detection](http://arxiv.org/abs/2309.07084v1)
  * [UpCycling: Semi-supervised 3D Object Detection without Sharing Raw-level Unlabeled Scenes](http://arxiv.org/abs/2211.11950)
  * [Pixel-Aligned Recurrent Queries for Multi-View 3D Object Detection](https://openaccess.thecvf.com/content/ICCV2023/papers/Xie_Pixel-Aligned_Recurrent_Queries_for_Multi-View_3D_Object_Detection_ICCV_2023_paper.pdf)
  * [Monocular 3D Object Detection with Bounding Box Denoising in 3D by Perceiver](http://arxiv.org/abs/2304.01289)
  * [MonoNeRD: NeRF-like Representations for Monocular 3D Object Detection](http://arxiv.org/abs/2308.09421)
  * [MonoDETR: Depth-guided Transformer for Monocular 3D Object Detection](http://arxiv.org/abs/2203.13310)
  * [Learning from Noisy Data for Semi-Supervised 3D Object Detection](https://openaccess.thecvf.com/content/ICCV2023/papers/Chen_Learning_from_Noisy_Data_for_Semi-Supervised_3D_Object_Detection_ICCV_2023_paper.pdf)
  * [DetZero: Rethinking Offboard 3D Object Detection with Long-term Sequential Point Clouds](http://arxiv.org/abs/2306.06023)
  * [SparseFusion: Fusing Multi-Modal Sparse Representations for Multi-Sensor 3D Object Detection](http://arxiv.org/abs/2304.14340)
  * [SA-BEV: Generating Semantic-Aware Bird's-Eye-View Feature for Multi-view 3D Object Detection](https://openaccess.thecvf.com/content/ICCV2023/papers/Zhang_SA-BEV_Generating_Semantic-Aware_Birds-Eye-View_Feature_for_Multi-view_3D_Object_Detection_ICCV_2023_paper.pdf)
  * [KECOR: Kernel Coding Rate Maximization for Active 3D Object Detection](http://arxiv.org/abs/2307.07942)
  * [ObjectFusion: Multi-modal 3D Object Detection with Object-Centric Fusion](https://openaccess.thecvf.com/content/ICCV2023/papers/Cai_ObjectFusion_Multi-modal_3D_Object_Detection_with_Object-Centric_Fusion_ICCV_2023_paper.pdf)
  * [Predict to Detect: Prediction-guided 3D Object Detection using Sequential Images](http://arxiv.org/abs/2306.08528)
  * [A Simple Vision Transformer for Weakly Semi-supervised 3D Object Detection](https://openaccess.thecvf.com/content/ICCV2023/papers/Zhang_A_Simple_Vision_Transformer_for_Weakly_Semi-supervised_3D_Object_Detection_ICCV_2023_paper.pdf)
  * [A Fast Unified System for 3D Object Detection and Tracking](https://openaccess.thecvf.com/content/ICCV2023/papers/Heitzinger_A_Fast_Unified_System_for_3D_Object_Detection_and_Tracking_ICCV_2023_paper.pdf)
  * [3DPPE: 3D Point Positional Encoding for Transformer-based Multi-Camera 3D Object Detection](https://openaccess.thecvf.com/content/ICCV2023/papers/Shu_3DPPE_3D_Point_Positional_Encoding_for_Transformer-based_Multi-Camera_3D_Object_ICCV_2023_paper.pdf)
  * [PARTNER: Level up the Polar Representation for LiDAR 3D Object Detection](http://arxiv.org/abs/2308.03982v1)
  * [SHIFT3D: Synthesizing Hard Inputs For Tricking 3D Detectors](http://arxiv.org/abs/2309.05810v1)
  * [ImGeoNet: Image-induced Geometry-aware Voxel Representation for Multi-view 3D Object Detection](http://arxiv.org/abs/2308.09098v1)<br>:star:[code](https://ttaoretw.github.io/imgeonet/)
  * [MonoNeRD: NeRF-like Representations for Monocular 3D Object Detection](http://arxiv.org/abs/2308.09421v1)<br>:star:[code](https://github.com/cskkxjk/MonoNeRD)
  * [SparseBEV: High-Performance Sparse 3D Object Detection from Multi-Camera Videos](http://arxiv.org/abs/2308.09244v1)<br>:star:[code](https://github.com/MCG-NJU/SparseBEV)
  * [QD-BEV : Quantization-aware View-guided Distillation for Multi-view 3D Object Detection](http://arxiv.org/abs/2308.10515v1)
  * [Representation Disparity-aware Distillation for 3D Object Detection](http://arxiv.org/abs/2308.10308v1)
  * [GPA-3D: Geometry-aware Prototype Alignment for Unsupervised Domain Adaptive 3D Object Detection from Point Clouds](http://arxiv.org/abs/2308.08140v1)<br>:star:[code](https://github.com/Liz66666/GPA3D)
  * [FocalFormer3D : Focusing on Hard Instance for 3D Object Detection](http://arxiv.org/abs/2308.04556v1)<br>:star:[code](https://github.com/NVlabs/FocalFormer3D)
  * [NeRF-Det: Learning Geometry-Aware Volumetric Representation for Multi-View 3D Object Detection](http://arxiv.org/abs/2307.14620v1)<br>:star:[code](https://github.com/facebookresearch/NeRF-Det)
  * [Exploring Object-Centric Temporal Modeling for Efficient Multi-View 3D Object Detection](https://arxiv.org/pdf/2303.11926.pdf)<br>:star:[code](https://github.com/exiawsh/StreamPETR)
  * [Cross Modal Transformer: Towards Fast and Robust 3D Object Detection](https://arxiv.org/pdf/2301.01283.pdf)<br>:star:[code](https://github.com/junjie18/CMT)
  * [CoIn: Contrastive Instance Feature Mining for Outdoor 3D Object Detection with Very Limited Annotations](https://openaccess.thecvf.com/content/ICCV2023/papers/Xia_CoIn_Contrastive_Instance_Feature_Mining_for_Outdoor_3D_Object_Detection_ICCV_2023_paper.pdf)
  * [GraphAlign: Enhancing Accurate Feature Alignment by Graph matching for Multi-Modal 3D Object Detection](https://openaccess.thecvf.com/content/ICCV2023/papers/Song_GraphAlign_Enhancing_Accurate_Feature_Alignment_by_Graph_matching_for_Multi-Modal_ICCV_2023_paper.pdf)
* æ–‡æœ¬é©±åŠ¨çš„ç›®æ ‡æ£€æµ‹
  * [Unsupervised Prompt Tuning for Text-Driven Object Detection](https://openaccess.thecvf.com/content/ICCV2023/papers/He_Unsupervised_Prompt_Tuning_for_Text-Driven_Object_Detection_ICCV_2023_paper.pdf)
* å¼€æ”¾è¯æ±‡ç›®æ ‡æ£€æµ‹
  * [EdaDet: Open-Vocabulary Object Detection Using Early Dense Alignment](http://arxiv.org/abs/2309.01151v1)<br>:star:[code](https://chengshiest.github.io/edadet)
  * [Open-Vocabulary Object Detection With an Open Corpus](https://openaccess.thecvf.com/content/ICCV2023/papers/Wang_Open-Vocabulary_Object_Detection_With_an_Open_Corpus_ICCV_2023_paper.pdf)
  * [Distilling DETR with Visual-Linguistic Knowledge for Open-Vocabulary Object Detection](https://openaccess.thecvf.com/content/ICCV2023/papers/Li_Distilling_DETR_with_Visual-Linguistic_Knowledge_for_Open-Vocabulary_Object_Detection_ICCV_2023_paper.pdf)
* ç«¯åˆ°ç«¯ç›®æ ‡æ£€æµ‹
  * [Decoupled DETR: Spatially Disentangling Localization and Classification for Improved End-to-End Object Detection](https://openaccess.thecvf.com/content/ICCV2023/papers/Zhang_Decoupled_DETR_Spatially_Disentangling_Localization_and_Classification_for_Improved_End-to-End_ICCV_2023_paper.pdf)
* åŸŸé€‚åº”ç›®æ ‡æ£€æµ‹
  * [Masked Retraining Teacher-Student Framework for Domain Adaptive Object Detection](https://openaccess.thecvf.com/content/ICCV2023/papers/Zhao_Masked_Retraining_Teacher-Student_Framework_for_Domain_Adaptive_Object_Detection_ICCV_2023_paper.pdf)
  * [CSDA: Learning Category-Scale Joint Feature for Domain Adaptive Object Detection](https://openaccess.thecvf.com/content/ICCV2023/papers/Gao_CSDA_Learning_Category-Scale_Joint_Feature_for_Domain_Adaptive_Object_Detection_ICCV_2023_paper.pdf)
* å¼±ç›‘ç£ç›®æ ‡æ£€æµ‹
  * [ALWOD: Active Learning for Weakly-Supervised Object Detection](http://arxiv.org/abs/2309.07914v1)<br>:star:[code](https://github.com/seqam-lab/ALWOD)
* åŠç›‘ç£ç›®æ ‡æ£€æµ‹
* [Gradient-based Sampling for Class Imbalanced Semi-supervised Object Detection](https://openaccess.thecvf.com/content/ICCV2023/papers/Li_Gradient-based_Sampling_for_Class_Imbalanced_Semi-supervised_Object_Detection_ICCV_2023_paper.pdf)
* å°æ ·æœ¬ç›®æ ‡æ£€æµ‹
  * [s-Adaptive Decoupled Prototype for Few-Shot Object Detection](https://openaccess.thecvf.com/content/ICCV2023/papers/Du_s-Adaptive_Decoupled_Prototype_for_Few-Shot_Object_Detection_ICCV_2023_paper.pdf)
* å¯†é›†ç›®æ ‡æ£€æµ‹
  * [Bridging Cross-task Protocol Inconsistency for Distillation in Dense Object Detection](http://arxiv.org/abs/2308.14286v1)<br>:star:[code](https://github.com/TinyTigerPan/BCKD)
* è§†é¢‘ç›®æ ‡æ£€æµ‹
  * [Objects do not disappear: Video object detection by single-frame object location anticipation](http://arxiv.org/abs/2308.04770v1)<br>:star:[code](https://github.com/L-KID/Videoobject-detection-by-location-anticipation)
  * [Identity-Consistent Aggregation for Video Object Detection](http://arxiv.org/abs/2308.07737v1)
* é•¿å°¾ç›®æ ‡æ£€æµ‹
  * [Boosting Long-tailed Object Detection via Step-wise Learning on Smooth-tail Data](http://arxiv.org/abs/2305.12833)
* å¼€é›†ç›®æ ‡æ£€æµ‹
  * [Novel Scenes & Classes: Towards Adaptive Open-set Object Detection](https://openaccess.thecvf.com/content/ICCV2023/papers/Li_Novel_Scenes__Classes_Towards_Adaptive_Open-set_Object_Detection_ICCV_2023_paper.pdf)
* å°ç›®æ ‡æ£€æµ‹
  * [Small Object Detection via Coarse-to-fine Proposal Generation and Imitation Learning](http://arxiv.org/abs/2308.09534v1)<br>:star:[code](https://github.com/shaunyuan22/CFINet)
  * [Monte Carlo Linear Clustering with Single-Point Supervision is Enough for Infrared Small Target Detection](http://arxiv.org/abs/2304.04442)
* ç›®æ ‡å®šä½
  * 3D OL
    * [EgoLoc: Revisiting 3D Object Localization from Egocentric Videos with Visual Queries](http://arxiv.org/abs/2212.06969)
  * æ— ç›‘ç£ç›®æ ‡å®šä½
    * [Unsupervised Object Localization with Representer Point Selection](http://arxiv.org/abs/2309.04172v1)
  * å¼±ç›‘ç£ç›®æ ‡å®šä½
    * [Category-aware Allocation Transformer for Weakly Supervised Object Localization](https://openaccess.thecvf.com/content/ICCV2023/papers/Chen_Category-aware_Allocation_Transformer_for_Weakly_Supervised_Object_Localization_ICCV_2023_paper.pdf)
    * [Generative Prompt Model for Weakly Supervised Object Localization](http://arxiv.org/abs/2307.09756v1)<br>:star:[code](https://github.com/callsys/GenPromp)
    * [Spatial-Aware Token for Weakly Supervised Object Localization](http://arxiv.org/abs/2303.10438)
  * å¼€æ”¾è¯æ±‡ç›®æ ‡å®šä½
    * [Unsupervised Open-Vocabulary Object Localization in Videos](http://arxiv.org/abs/2309.09858v1)  
* å½±å­æ£€æµ‹
  * [SILT: Shadow-aware Iterative Label Tuning for Learning to Detect Shadows from Noisy Labels](http://arxiv.org/abs/2308.12064v1)
  * [Adaptive Illumination Mapping for Shadow Detection in Raw Images](https://openaccess.thecvf.com/content/ICCV2023/papers/Sun_Adaptive_Illumination_Mapping_for_Shadow_Detection_in_Raw_Images_ICCV_2023_paper.pdf)

<a name="2"/>

## 2.3D(ä¸‰ç»´é‡å»º\ä¸‰ç»´è§†è§‰)
* [ATT3D: Amortized Text-to-3D Object Synthesis](http://arxiv.org/abs/2306.07349)
* [Zero-1-to-3: Zero-shot One Image to 3D Object](https://openaccess.thecvf.com/content/ICCV2023/papers/Liu_Zero-1-to-3_Zero-shot_One_Image_to_3D_Object_ICCV_2023_paper.pdf)
* [Robo3D: Towards Robust and Reliable 3D Perception against Corruptions](http://arxiv.org/abs/2303.17597)
* [AG3D: Learning to Generate 3D Avatars from 2D Image Collections](http://arxiv.org/abs/2305.02312)
* [Semantify: Simplifying the Control of 3D Morphable Models Using CLIP](http://arxiv.org/abs/2308.07415)
* [Vox-E: Text-Guided Voxel Editing of 3D Objects](https://openaccess.thecvf.com/content/ICCV2023/papers/Sella_Vox-E_Text-Guided_Voxel_Editing_of_3D_Objects_ICCV_2023_paper.pdf)
* [Tiled Multiplane Images for Practical 3D Photography](http://arxiv.org/abs/2309.14291v1)
* [DeFormer: Integrating Transformers with Deformable Models for 3D Shape Abstraction from a Single Image](http://arxiv.org/abs/2309.12594v1)
* [HoloFusion: Towards Photo-realistic 3D Generative Modeling](http://arxiv.org/abs/2308.14244v1)<br>:star:[code](https://holodiffusion.github.io/holofusion)
* [PlaneRecTR: Unified Query learning for 3D Plane Recovery from a Single View](http://arxiv.org/abs/2307.13756v1)<br>:house:[project](https://youtu.be/YBB7totHGJg)<br>:star:[code](https://github.com/SJingjia/PlaneRecTR)
* [OmnimatteRF: Robust Omnimatte with 3D Background Modeling](http://arxiv.org/abs/2309.07749v1)<br>:star:[code](https://omnimatte-rf.github.io/)
* [Learning Versatile 3D Shape Generation with Improved Auto-regressive Models](https://openaccess.thecvf.com/content/ICCV2023/papers/Luo_Learning_Versatile_3D_Shape_Generation_with_Improved_Auto-regressive_Models_ICCV_2023_paper.pdf)
* [GeoMIM: Towards Better 3D Knowledge Transfer via Masked Image Modeling for Multi-view 3D Understanding](http://arxiv.org/abs/2303.11325)
* ä¸‰ç»´é‡å»º
  * [Metric3D: Towards Zero-shot Metric 3D Prediction from A Single Image](http://arxiv.org/abs/2307.10984v1)<br>:star:[code](https://github.com/YvanYin/Metric3D)
  * [Batch-based Model Registration for Fast 3D Sherd Reconstruction](http://arxiv.org/abs/2211.06897)
  * [UMIFormer: Mining the Correlations between Similar Tokens for Multi-View 3D Reconstruction](http://arxiv.org/abs/2302.13987)
  * [FineRecon: Depth-aware Feed-forward Network for Detailed 3D Reconstruction](http://arxiv.org/abs/2304.01480)
  * [LIST: Learning Implicitly from Spatial Transformers for Single-View 3D Reconstruction](http://arxiv.org/abs/2307.12194v1)
  * [Neural-PBIR Reconstruction of Shape, Material, and Illumination](http://arxiv.org/abs/2304.13445)
  * [Single-Stage Diffusion NeRF: A Unified Approach to 3D Generation and Reconstruction](http://arxiv.org/abs/2304.06714)
  * [LivePose: Online 3D Reconstruction from Monocular Video with Dynamic Camera Poses](http://arxiv.org/abs/2304.00054)
  * [Long-Range Grouping Transformer for Multi-View 3D Reconstruction](http://arxiv.org/abs/2308.08724v1)<br>:star:[code](https://github.com/LiyingCV/Long-Range-Grouping-Transformer)
  * [Doppelgangers: Learning to Disambiguate Images of Similar Structures](http://arxiv.org/abs/2309.02420v1)<br>:star:[code](http://doppelgangers-3d.github.io/)
  * [Deformable Model-Driven Neural Rendering for High-Fidelity 3D Reconstruction of Human Heads Under Low-View Settings](http://arxiv.org/abs/2303.13855)
  * [Iterative Superquadric Recomposition of 3D Objects from Multiple Views](http://arxiv.org/abs/2309.02102v1)<br>:star:[code](https://github.com/ExplainableML/ISCO)
  * [CHORUS: Learning Canonicalized 3D Human-Object Spatial Relations from Unbounded Synthesized Images](http://arxiv.org/abs/2308.12288v1)<br>:star:[code](https://jellyheadandrew.github.io/projects/chorus)
  * [PlankAssembly: Robust 3D Reconstruction from Three Orthographic Views with Learnt Shape Programs](http://arxiv.org/abs/2308.05744v1)<br>:star:[code](https://manycore-research.github.io/PlankAssembly)
  * [Coordinate Quantized Neural Implicit Representations for Multi-view Reconstruction](http://arxiv.org/abs/2308.11025v1)<br>:star:[code](https://github.com/MachinePerceptionLab/CQ-NIR)
  * [Root Pose Decomposition Towards Generic Non-rigid 3D Reconstruction with Monocular Videos](http://arxiv.org/abs/2308.10089v1)<br>:star:[code](https://rpd-share.github.io)
  * [R3D3: Dense 3D Reconstruction of Dynamic Scenes from Multiple Cameras](http://arxiv.org/abs/2308.14713v1)<br>:house:[project](https://www.vis.xyz/pub/r3d3/)
  * ä¸‰ç»´åœºæ™¯é‡å»º
    * [FrozenRecon: Pose-free 3D Scene Reconstruction with Frozen Depth Models](http://arxiv.org/abs/2308.05733v1)<br>:star:[code](https://aim-uofa.github.io/FrozenRecon/)
    * [Spacetime Surface Regularization for Neural Dynamic Scene Reconstruction](https://openaccess.thecvf.com/content/ICCV2023/papers/Choe_Spacetime_Surface_Regularization_for_Neural_Dynamic_Scene_Reconstruction_ICCV_2023_paper.pdf)
    * [DG-Recon: Depth-Guided Neural 3D Scene Reconstruction](https://openaccess.thecvf.com/content/ICCV2023/papers/Ju_DG-Recon_Depth-Guided_Neural_3D_Scene_Reconstruction_ICCV_2023_paper.pdf)
* æ·±åº¦ä¼°è®¡
  * [MAMo: Leveraging Memory and Attention for Monocular Video Depth Estimation](http://arxiv.org/abs/2307.14336v1)
  * [Single Depth-image 3D Reflection Symmetry and Shape Prediction](https://openaccess.thecvf.com/content/ICCV2023/papers/Zhang_Single_Depth-image_3D_Reflection_Symmetry_and_Shape_Prediction_ICCV_2023_paper.pdf)
  * [SlaBins: Fisheye Depth Estimation using Slanted Bins on Road Environments](https://openaccess.thecvf.com/content/ICCV2023/papers/Lee_SlaBins_Fisheye_Depth_Estimation_using_Slanted_Bins_on_Road_Environments_ICCV_2023_paper.pdf)
  * [3D Distillation: Improving Self-Supervised Monocular Depth Estimation on Reflective Surfaces](https://openaccess.thecvf.com/content/ICCV2023/papers/Shi_3D_Distillation_Improving_Self-Supervised_Monocular_Depth_Estimation_on_Reflective_Surfaces_ICCV_2023_paper.pdf)
  * [EGformer: Equirectangular Geometry-biased Transformer for 360 Depth Estimation](http://arxiv.org/abs/2304.07803)
  * [DPS-Net: Deep Polarimetric Stereo Depth Estimation](https://openaccess.thecvf.com/content/ICCV2023/papers/Tian_DPS-Net_Deep_Polarimetric_Stereo_Depth_Estimation_ICCV_2023_paper.pdf)
  * [Robust Geometry-Preserving Depth Estimation Using Differentiable Rendering](http://arxiv.org/abs/2309.09724)
  * [Towards Zero-Shot Scale-Aware Monocular Depth Estimation](http://arxiv.org/abs/2306.17253)
  * [Indoor Depth Recovery Based on Deep Unfolding with Non-Local Prior](https://openaccess.thecvf.com/content/ICCV2023/papers/Dai_Indoor_Depth_Recovery_Based_on_Deep_Unfolding_with_Non-Local_Prior_ICCV_2023_paper.pdf)
  * [GasMono: Geometry-Aided Self-Supervised Monocular Depth Estimation for Indoor Scenes](https://openaccess.thecvf.com/content/ICCV2023/papers/Zhao_GasMono_Geometry-Aided_Self-Supervised_Monocular_Depth_Estimation_for_Indoor_Scenes_ICCV_2023_paper.pdf)
  * [NDDepth: Normal-Distance Assisted Monocular Depth Estimation](http://arxiv.org/abs/2309.10592v1)
  * [V-FUSE: Volumetric Depth Map Fusion with Long-Range Constraints](http://arxiv.org/abs/2308.08715v1)
  * [Robust Monocular Depth Estimation under Challenging Conditions](http://arxiv.org/abs/2308.09711v1)<br>:star:[code](https://md4all.github.io)
  * [Self-Supervised Monocular Depth Estimation by Direction-aware Cumulative Convolution Network](http://arxiv.org/abs/2308.05605v1)
  * [Self-supervised Monocular Depth Estimation: Let's Talk About The Weather](https://openaccess.thecvf.com/content/ICCV2023/papers/Saunders_Self-supervised_Monocular_Depth_Estimation_Lets_Talk_About_The_Weather_ICCV_2023_paper.pdf)
  * [Calibrating Panoramic Depth Estimation for Practical Localization and Mapping](http://arxiv.org/abs/2308.14005v1)
  * [Two-in-One Depth: Bridging the Gap Between Monocular and Binocular Self-supervised Depth Estimation](http://arxiv.org/abs/2309.00933v1)<br>:star:[code](https://github.com/ZM-Zhou/TiO-Depth_pytorch)
  * [Robust Geometry-Preserving Depth Estimation Using Differentiable Rendering](http://arxiv.org/abs/2309.09724v1)
  * [GEDepth: Ground Embedding for Monocular Depth Estimation](http://arxiv.org/abs/2309.09975v1)
* æ·±åº¦è¡¥å…¨  
  * [AGG-Net: Attention Guided Gated-convolutional Network for Depth Image Completion](http://arxiv.org/abs/2309.01624v1)
  * [LRRU: Long-short Range Recurrent Updating Networks for Depth Completion](https://openaccess.thecvf.com/content/ICCV2023/papers/Wang_LRRU_Long-short_Range_Recurrent_Updating_Networks_for_Depth_Completion_ICCV_2023_paper.pdf)
  * [Aggregating Feature Point Cloud for Depth Completion](https://openaccess.thecvf.com/content/ICCV2023/papers/Yu_Aggregating_Feature_Point_Cloud_for_Depth_Completion_ICCV_2023_paper.pdf)
* Stereo Matching
  * [Uncertainty Guided Adaptive Warping for Robust and Efficient Stereo Matching](http://arxiv.org/abs/2307.14071v1)
  * [Learning Depth Estimation for Transparent and Mirror Surfaces](http://arxiv.org/abs/2307.15052v1)<br>:star:[code](https://cvlab-unibo.github.io/Depth4ToM)
  * [ELFNet: Evidential Local-global Fusion for Stereo Matching](http://arxiv.org/abs/2308.00728v1)<br>:star:[code](https://github.com/jimmy19991222/ELFNet)
  * [Parameterized Cost Volume for Stereo Matching](https://openaccess.thecvf.com/content/ICCV2023/papers/Zeng_Parameterized_Cost_Volume_for_Stereo_Matching_ICCV_2023_paper.pdf)
* ä¸‰ç»´ç”Ÿæˆ
  * 3Då½¢çŠ¶ç”Ÿæˆ
    * [Learning Versatile 3D Shape Generation with Improved AR Models](https://arxiv.org/pdf/2303.14700.pdf)
  * æ–‡æœ¬-3Dç”Ÿæˆ
    * [DreamBooth3D: Subject-Driven Text-to-3D Generation](http://arxiv.org/abs/2303.13508)
* MVS
  * [Hierarchical Prior Mining for Non-local Multi-View Stereo](http://arxiv.org/abs/2303.09758)
  * [S-VolSDF: Sparse Multi-View Stereo Regularization of Neural Implicit Surfaces](https://openaccess.thecvf.com/content/ICCV2023/papers/Wu_S-VolSDF_Sparse_Multi-View_Stereo_Regularization_of_Neural_Implicit_Surfaces_ICCV_2023_paper.pdf)
  * [When Epipolar Constraint Meets Non-Local Operators in Multi-View Stereo](https://openaccess.thecvf.com/content/ICCV2023/papers/Liu_When_Epipolar_Constraint_Meets_Non-Local_Operators_in_Multi-View_Stereo_ICCV_2023_paper.pdf)
* è¡¨é¢é‡å»º
  * [Structure-Aware Surface Reconstruction via Primitive Assembly](https://openaccess.thecvf.com/content/ICCV2023/papers/Jiang_Structure-Aware_Surface_Reconstruction_via_Primitive_Assembly_ICCV_2023_paper.pdf)
  * [C2F2NeUS: Cascade Cost Frustum Fusion for High Fidelity and Generalizable Neural Surface Reconstruction](http://arxiv.org/abs/2306.10003)
  * [NeuS2: Fast Learning of Neural Implicit Surfaces for Multi-view Reconstruction](http://arxiv.org/abs/2212.05231)

<a name="1"/>

## 1.å…¶å®ƒ(others)
* [Poincare ResNet](http://arxiv.org/abs/2303.14027)
* [Dataset Quantization](http://arxiv.org/abs/2308.10524)
* [Scene as Occupancy](http://arxiv.org/abs/2306.02851)
* [Attentive Mask CLIP](http://arxiv.org/abs/2212.08653)
* [Generalized Differentiable RANSAC](http://arxiv.org/abs/2212.13185)
* [Neural Implicit Surface Evolution](http://arxiv.org/abs/2201.09636)
* [Gender Artifacts in Visual Datasets](http://arxiv.org/abs/2206.09191)
* [Towards Models that Can See and Read](http://arxiv.org/abs/2301.07389)
* [Convex Decomposition of Indoor Scenes](http://arxiv.org/abs/2307.04246)
* [Navigating to Objects Specified by Images](http://arxiv.org/abs/2304.01192)
* [Quality Diversity for Visual Pre-Training](https://openaccess.thecvf.com/content/ICCV2023/papers/Chavhan_Quality_Diversity_for_Visual_Pre-Training_ICCV_2023_paper.pdf)
* [Towards Multi-Layered 3D Garments Animation](http://arxiv.org/abs/2305.10418)
* [XiNet: Efficient Neural Networks for tinyML](https://openaccess.thecvf.com/content/ICCV2023/papers/Ancilotto_XiNet_Efficient_Neural_Networks_for_tinyML_ICCV_2023_paper.pdf)
* [Simulating Fluids in Real-World Still Images](http://arxiv.org/abs/2204.11335)
* [Bayesian Optimization Meets Self-Distillation](http://arxiv.org/abs/2304.12666)
* [Sentence Attention Blocks for Answer Grounding](http://arxiv.org/abs/2309.11593)
* [Do DALL-E and Flamingo Understand Each Other?](https://openaccess.thecvf.com/content/ICCV2023/papers/Li_Do_DALL-E_and_Flamingo_Understand_Each_Other_ICCV_2023_paper.pdf)
* [Computational 3D Imaging with Position Sensors](https://openaccess.thecvf.com/content/ICCV2023/papers/Klotz_Computational_3D_Imaging_with_Position_Sensors_ICCV_2023_paper.pdf)
* [ClusT3: Information Invariant Test-Time Training](https://openaccess.thecvf.com/content/ICCV2023/papers/Hakim_ClusT3_Information_Invariant_Test-Time_Training_ICCV_2023_paper.pdf)
* [DETR Does Not Need Multi-Scale or Locality Design](https://openaccess.thecvf.com/content/ICCV2023/papers/Lin_DETR_Does_Not_Need_Multi-Scale_or_Locality_Design_ICCV_2023_paper.pdf)
* [Multi-Directional Subspace Editing in Style-Space](http://arxiv.org/abs/2211.11825)
* [Re-ReND: Real-Time Rendering of NeRFs across Devices](https://openaccess.thecvf.com/content/ICCV2023/papers/Rojas_Re-ReND_Real-Time_Rendering_of_NeRFs_across_Devices_ICCV_2023_paper.pdf)
* [Curvature-Aware Training for Coordinate Networks](http://arxiv.org/abs/2305.08552)
* [Document Understanding Dataset and Evaluation (DUDE)](http://arxiv.org/abs/2305.08455)
* [Mesh2Tex: Generating Mesh Textures from Image Queries](http://arxiv.org/abs/2304.05868)
* [Rosetta Neurons: Mining the Common Units in a Model Zoo](http://arxiv.org/abs/2306.09346)
* [General Planar Motion from a Pair of 3D Correspondences](https://openaccess.thecvf.com/content/ICCV2023/papers/Dibene_General_Planar_Motion_from_a_Pair_of_3D_Correspondences_ICCV_2023_paper.pdf)
* [Label-Noise Learning with Intrinsically Long-Tailed Data](http://arxiv.org/abs/2208.09833)
* [3D VR Sketch Guided 3D Shape Prototyping and Exploration](http://arxiv.org/abs/2306.10830)
* [Sensitivity-Aware Visual Parameter-Efficient Fine-Tuning](http://arxiv.org/abs/2303.08566)
* [Transparent Shape from a Single View Polarization Image](http://arxiv.org/abs/2204.06331)
* [Role-Aware Interaction Generation from Textual Description](https://openaccess.thecvf.com/content/ICCV2023/papers/Tanaka_Role-Aware_Interaction_Generation_from_Textual_Description_ICCV_2023_paper.pdf)
* [DFA3D: 3D Deformable Attention For 2D-to-3D Feature Lifting](http://arxiv.org/abs/2307.12972)
* [Grounding 3D Object Affordance from 2D Interactions in Images](http://arxiv.org/abs/2303.10437)
* [FBLNet: FeedBack Loop Network for Driver Attention Prediction](http://arxiv.org/abs/2212.02096)
* [INSTA-BNN: Binary Neural Network with INSTAnce-aware Threshold](https://openaccess.thecvf.com/content/ICCV2023/papers/Lee_INSTA-BNN_Binary_Neural_Network_with_INSTAnce-aware_Threshold_ICCV_2023_paper.pdf)
* [Deep Incubation: Training Large Models by Divide-and-Conquering](http://arxiv.org/abs/2212.04129)
* [Learning to Learn: How to Continuously Teach Humans and Machines](http://arxiv.org/abs/2211.15470)
* [SpinCam: High-Speed Imaging via a Rotating Point-Spread Function](https://openaccess.thecvf.com/content/ICCV2023/papers/Chan_SpinCam_High-Speed_Imaging_via_a_Rotating_Point-Spread_Function_ICCV_2023_paper.pdf)
* [BANSAC: A Dynamic BAyesian Network for Adaptive SAmple Consensus](http://arxiv.org/abs/2309.08690)
* [What Can Simple Arithmetic Operations Do for Temporal Modeling?](http://arxiv.org/abs/2307.08908)
* [Sample4Geo: Hard Negative Sampling For Cross-View Geo-Localisation](http://arxiv.org/abs/2303.11851)
* [Cross-view Semantic Alignment for Livestreaming Product Recognition](http://arxiv.org/abs/2308.04912)
* [Geometric Viewpoint Learning with Hyper-Rays and Harmonics Encoding](https://openaccess.thecvf.com/content/ICCV2023/papers/Min_Geometric_Viewpoint_Learning_with_Hyper-Rays_and_Harmonics_Encoding_ICCV_2023_paper.pdf)
* [Segmenting Known Objects and Unseen Unknowns without Prior Knowledge](http://arxiv.org/abs/2209.05407)
* [Learning Fine-Grained Features for Pixel-Wise Video Correspondences](http://arxiv.org/abs/2308.03040)
* [Graphics2RAW: Mapping Computer Graphics Images to Sensor RAW Images](https://openaccess.thecvf.com/content/ICCV2023/papers/Seo_Graphics2RAW_Mapping_Computer_Graphics_Images_to_Sensor_RAW_Images_ICCV_2023_paper.pdf)
* [LoCUS: Learning Multiscale 3D-consistent Features from Posed Images](https://openaccess.thecvf.com/content/ICCV2023/papers/Kloepfer_LoCUS_Learning_Multiscale_3D-consistent_Features_from_Posed_Images_ICCV_2023_paper.pdf)
* [DreamTeacher: Pretraining Image Backbones with Deep Generative Models](http://arxiv.org/abs/2307.07487)
* [Parametric Information Maximization for Generalized Category Discovery](http://arxiv.org/abs/2212.00334)
* [Sat2Density: Faithful Density Learning from Satellite-Ground Image Pairs](http://arxiv.org/abs/2303.14672)
* [Neural Collage Transfer: Artistic Reconstruction via Material Manipulation](https://openaccess.thecvf.com/content/ICCV2023/papers/Lee_Neural_Collage_Transfer_Artistic_Reconstruction_via_Material_Manipulation_ICCV_2023_paper.pdf)
* [Adaptive Spiral Layers for Efficient 3D Representation Learning on Meshes](https://openaccess.thecvf.com/content/ICCV2023/papers/Babiloni_Adaptive_Spiral_Layers_for_Efficient_3D_Representation_Learning_on_Meshes_ICCV_2023_paper.pdf)
* [Efficient Unified Demosaicing for Bayer and Non-Bayer Patterned Image Sensors](http://arxiv.org/abs/2307.10667)
* [Is Imitation All You Need? Generalized Decision-Making with Dual-Phase Training](http://arxiv.org/abs/2307.07909)
* [Not All Features Matter: Enhancing Few-shot CLIP with Adaptive Prior Refinement](http://arxiv.org/abs/2304.01195)
* [Communication-Efficient Vertical Federated Learning with Limited Overlapping Samples](http://arxiv.org/abs/2303.16270)
* [Improving Representation Learning for Histopathologic Images with Cluster Constraints](https://openaccess.thecvf.com/content/ICCV2023/papers/Wu_Improving_Representation_Learning_for_Histopathologic_Images_with_Cluster_Constraints_ICCV_2023_paper.pdf)
* [EfficientTrain: Exploring Generalized Curriculum Learning for Training Visual Backbones](http://arxiv.org/abs/2211.09703)
* [Mitigating and Evaluating Static Bias of Action Representations in the Background and the Foreground](http://arxiv.org/abs/2211.12883)
* [S-TREK: Sequential Translation and Rotation Equivariant Keypoints for Local Feature Extraction](https://openaccess.thecvf.com/content/ICCV2023/papers/Santellani_S-TREK_Sequential_Translation_and_Rotation_Equivariant_Keypoints_for_Local_Feature_ICCV_2023_paper.pdf)
* [Evaluation and Improvement of Interpretability for Self-Explainable Part-Prototype Networks](http://arxiv.org/abs/2212.05946)
* [Video Adverse-Weather-Component Suppression Network via Weather Messenger and Adversarial Backpropagation](http://arxiv.org/abs/2309.13700)
* [A 5-Point Minimal Solver for Event Camera Relative Motion Estimation](https://openaccess.thecvf.com/content/ICCV2023/papers/Gao_A_5-Point_Minimal_Solver_for_Event_Camera_Relative_Motion_Estimation_ICCV_2023_paper.pdf)
* [Sample-wise Label Confidence Incorporation for Learning with Noisy Labels](https://openaccess.thecvf.com/content/ICCV2023/papers/Ahn_Sample-wise_Label_Confidence_Incorporation_for_Learning_with_Noisy_Labels_ICCV_2023_paper.pdf)
* [MatrixVT: Efficient Multi-Camera to BEV Transformation for 3D Perception](http://arxiv.org/abs/2211.10593)
* [FCCNs: Fully Complex-valued Convolutional Networks using Complex-valued Color Model and Loss Function](https://openaccess.thecvf.com/content/ICCV2023/papers/Yadav_FCCNs_Fully_Complex-valued_Convolutional_Networks_using_Complex-valued_Color_Model_and_ICCV_2023_paper.pdf)
* [Name Your Colour For the Task: Artificially Discover Colour Naming via Colour Quantisation Transformer](http://arxiv.org/abs/2212.03434)
* [Unleashing the Potential of Spiking Neural Networks with Dynamic Confidence](https://openaccess.thecvf.com/content/ICCV2023/papers/Li_Unleashing_the_Potential_of_Spiking_Neural_Networks_with_Dynamic_Confidence_ICCV_2023_paper.pdf)
* [Fast Globally Optimal Surface Normal Estimation from an Affine Correspondence](https://openaccess.thecvf.com/content/ICCV2023/papers/Hajder_Fast_Globally_Optimal_Surface_Normal_Estimation_from_an_Affine_Correspondence_ICCV_2023_paper.pdf)
* [Fast and Accurate Transferability Measurement by Evaluating Intra-class Feature Variance](http://arxiv.org/abs/2308.05986)
* [TiDy-PSFs: Computational Imaging with Time-Averaged Dynamic Point-Spread-Functions](https://openaccess.thecvf.com/content/ICCV2023/papers/Shah_TiDy-PSFs_Computational_Imaging_with_Time-Averaged_Dynamic_Point-Spread-Functions_ICCV_2023_paper.pdf)
* [All in Tokens: Unifying Output Space of Visual Tasks via Soft Token](http://arxiv.org/abs/2301.02229)
* [A Unified Framework for Robustness on Diverse Sampling Errors](https://openaccess.thecvf.com/content/ICCV2023/papers/Jeon_A_Unified_Framework_for_Robustness_on_Diverse_Sampling_Errors_ICCV_2023_paper.pdf)
* [VQ3D: Learning a 3D-Aware Generative Model on ImageNet](http://arxiv.org/abs/2302.06833)
* [Learning to Ground Instructional Articles in Videos through Narrations](http://arxiv.org/abs/2306.03802)
* [Window-Based Early-Exit Cascades for Uncertainty Estimation: When Deep Ensembles are More Efficient than Single Models](http://arxiv.org/abs/2303.08010)
* [Multimodal Optimal Transport-based Co-Attention Transformer with Global Structure Consistency for Survival Prediction](http://arxiv.org/abs/2306.08330)
* [PADDLES: Phase-Amplitude Spectrum Disentangled Early Stopping for Learning with Noisy Labels](http://arxiv.org/abs/2212.03462)
* [Temporal-Coded Spiking Neural Networks with Dynamic Firing Threshold: Learning with Event-Driven Backpropagation](https://openaccess.thecvf.com/content/ICCV2023/papers/Wei_Temporal-Coded_Spiking_Neural_Networks_with_Dynamic_Firing_Threshold_Learning_with_ICCV_2023_paper.pdf)
* [ICL-D3IE: In-Context Learning with Diverse Demonstrations Updating for Document Information Extraction](https://openaccess.thecvf.com/content/ICCV2023/papers/He_ICL-D3IE_In-Context_Learning_with_Diverse_Demonstrations_Updating_for_Document_Information_ICCV_2023_paper.pdf)
* [Translating Images to Road Network: A Non-Autoregressive Sequence-to-Sequence Approach](https://openaccess.thecvf.com/content/ICCV2023/papers/Lu_Translating_Images_to_Road_Network_A_Non-Autoregressive_Sequence-to-Sequence_Approach_ICCV_2023_paper.pdf)
* [CHAMPAGNE: Learning Real-world Conversation from Large-Scale Web Videos](http://arxiv.org/abs/2303.09713)
* [AssetField: Assets Mining and Reconfiguration in Ground Feature Plane Representation](http://arxiv.org/abs/2303.13953)
* [NLOS-NeuS: Non-line-of-sight Neural Implicit Surface](https://openaccess.thecvf.com/content/ICCV2023/papers/Fujimura_NLOS-NeuS_Non-line-of-sight_Neural_Implicit_Surface_ICCV_2023_paper.pdf)
* [XVO: Generalized Visual Odometry via Cross-Modal Self-Training](https://openaccess.thecvf.com/content/ICCV2023/papers/Lai_XVO_Generalized_Visual_Odometry_via_Cross-Modal_Self-Training_ICCV_2023_paper.pdf)
* [ASIC: Aligning Sparse in-the-wild Image Collections](http://arxiv.org/abs/2303.16201)
* [PointCLIP V2: Prompting CLIP and GPT for Powerful 3D Open-world Learning](http://arxiv.org/abs/2211.11682)
* [MATE: Masked Autoencoders are Online 3D Test-Time Learners](http://arxiv.org/abs/2211.11432)
* [SYENet: A Simple Yet Effective Network for Multiple Low-Level Vision Tasks with Real-Time Performance on Mobile Device](http://arxiv.org/abs/2308.08137)
* [Misalign, Contrast then Distill: Rethinking Misalignments in Language-Image Pre-training](https://openaccess.thecvf.com/content/ICCV2023/papers/Kim_Misalign_Contrast_then_Distill_Rethinking_Misalignments_in_Language-Image_Pre-training_ICCV_2023_paper.pdf)
* [Domain Specified Optimization for Deployment Authorization](https://openaccess.thecvf.com/content/ICCV2023/papers/Wang_Domain_Specified_Optimization_for_Deployment_Authorization_ICCV_2023_paper.pdf)
* [PRANC: Pseudo RAndom Networks for Compacting Deep Models](http://arxiv.org/abs/2206.08464)
* [Neural Haircut: Prior-Guided Strand-Based Hair Reconstruction](http://arxiv.org/abs/2306.05872)
* [NaviNeRF: NeRF-based 3D Representation Disentanglement by Latent Semantic Navigation](http://arxiv.org/abs/2304.11342)
* [Building Bridge Across the Time: Disruption and Restoration of Murals In the Wild](https://openaccess.thecvf.com/content/ICCV2023/papers/Shao_Building_Bridge_Across_the_Time_Disruption_and_Restoration_of_Murals_ICCV_2023_paper.pdf)
* [MoreauGrad: Sparse and Robust Interpretation of Neural Networks via Moreau Envelope](http://arxiv.org/abs/2302.05294)
* [One-shot Implicit Animatable Avatars with Model-based Priors](http://arxiv.org/abs/2212.02469)<br>:thumbsup:[åŸºäºæ¨¡å‹å…ˆéªŒçš„éšå¼å¯åŠ¨æ•°å­—äººå•æ ·æœ¬å­¦ä¹ ](https://mp.weixin.qq.com/s/IUTfQo6FJoB14oZFyyeVmQ)
* [RecRecNet: Rectangling Rectified Wide-Angle Images by Thin-Plate Spline Model and DoF-based Curriculum Learning](http://arxiv.org/abs/2301.01661)
* [Luminance-aware Color Transform for Multiple Exposure Correction](https://openaccess.thecvf.com/content/ICCV2023/papers/Baek_Luminance-aware_Color_Transform_for_Multiple_Exposure_Correction_ICCV_2023_paper.pdf)
* [Segment Every Reference Object in Spatial and Temporal Spaces](https://openaccess.thecvf.com/content/ICCV2023/papers/Wu_Segment_Every_Reference_Object_in_Spatial_and_Temporal_Spaces_ICCV_2023_paper.pdf)
* [Surface Normal Clustering for Implicit Representation of Manhattan Scenes](http://arxiv.org/abs/2212.01331)
* [RankMatch: Fostering Confidence and Consistency in Learning with Noisy Labels](https://openaccess.thecvf.com/content/ICCV2023/papers/Zhang_RankMatch_Fostering_Confidence_and_Consistency_in_Learning_with_Noisy_Labels_ICCV_2023_paper.pdf)
* [Hiding Visual Information via Obfuscating Adversarial Perturbations](http://arxiv.org/abs/2209.15304)
* [SHACIRA: Scalable HAsh-grid Compression for Implicit Neural Representations](https://openaccess.thecvf.com/content/ICCV2023/papers/Girish_SHACIRA_Scalable_HAsh-grid_Compression_for_Implicit_Neural_Representations_ICCV_2023_paper.pdf)
* [VoroMesh: Learning Watertight Surface Meshes with Voronoi Diagrams](http://arxiv.org/abs/2308.14616)
* [UniDexGrasp++: Improving Dexterous Grasping Policy Learning via Geometry-Aware Curriculum and Iterative Generalist-Specialist Learning](https://openaccess.thecvf.com/content/ICCV2023/papers/Wan_UniDexGrasp_Improving_Dexterous_Grasping_Policy_Learning_via_Geometry-Aware_Curriculum_and_ICCV_2023_paper.pdf)
* [Will Large-scale Generative Models Corrupt Future Datasets?](http://arxiv.org/abs/2211.08095)
* [DREAM: Efficient Dataset Distillation by Representative Matching](http://arxiv.org/abs/2302.14416)
* [Inverse Compositional Learning for Weakly-supervised Relation Grounding](https://openaccess.thecvf.com/content/ICCV2023/papers/Li_Inverse_Compositional_Learning_for_Weakly-supervised_Relation_Grounding_ICCV_2023_paper.pdf)
* [Adaptive Similarity Bootstrapping for Self-Distillation Based Representation Learning](https://openaccess.thecvf.com/content/ICCV2023/papers/Lebailly_Adaptive_Similarity_Bootstrapping_for_Self-Distillation_Based_Representation_Learning_ICCV_2023_paper.pdf)
* [ParCNetV2: Oversized Kernel with Enhanced Attention](http://arxiv.org/abs/2211.07157)
* [Efficiently Robustify Pre-Trained Models](http://arxiv.org/abs/2309.07499)
* [DIME-FM : DIstilling Multimodal and Efficient Foundation Models](https://openaccess.thecvf.com/content/ICCV2023/papers/Sun_DIME-FM__DIstilling_Multimodal_and_Efficient_Foundation_Models_ICCV_2023_paper.pdf)
* [Overcoming Forgetting Catastrophe in Quantization-Aware Training](https://openaccess.thecvf.com/content/ICCV2023/papers/Chen_Overcoming_Forgetting_Catastrophe_in_Quantization-Aware_Training_ICCV_2023_paper.pdf)
* [Adverse Weather Removal with Codebook Priors](https://openaccess.thecvf.com/content/ICCV2023/papers/Ye_Adverse_Weather_Removal_with_Codebook_Priors_ICCV_2023_paper.pdf)
* [MAP: Towards Balanced Generalization of IID and OOD through Model-Agnostic Adapters](https://openaccess.thecvf.com/content/ICCV2023/papers/Zhang_MAP_Towards_Balanced_Generalization_of_IID_and_OOD_through_Model-Agnostic_ICCV_2023_paper.pdf)
* [Instruct-NeRF2NeRF: Editing 3D Scenes with Instructions](https://openaccess.thecvf.com/content/ICCV2023/papers/Haque_Instruct-NeRF2NeRF_Editing_3D_Scenes_with_Instructions_ICCV_2023_paper.pdf)
* [COOL-CHIC: Coordinate-based Low Complexity Hierarchical Image Codec](https://openaccess.thecvf.com/content/ICCV2023/papers/Ladune_COOL-CHIC_Coordinate-based_Low_Complexity_Hierarchical_Image_Codec_ICCV_2023_paper.pdf)
* [Chop & Learn: Recognizing and Generating Object-State Compositions](http://arxiv.org/abs/2309.14339)
* [CORE: Cooperative Reconstruction for Multi-Agent Perception](http://arxiv.org/abs/2307.11514)
* [Gramian Attention Heads are Strong yet Efficient Vision Learners](https://openaccess.thecvf.com/content/ICCV2023/papers/Ryu_Gramian_Attention_Heads_are_Strong_yet_Efficient_Vision_Learners_ICCV_2023_paper.pdf)
* [Improving Zero-Shot Generalization for CLIP with Synthesized Prompts](http://arxiv.org/abs/2307.07397)
* [E3Sym: Leveraging E(3) Invariance for Unsupervised 3D Planar Reflective Symmetry Detection](https://openaccess.thecvf.com/content/ICCV2023/papers/Li_E3Sym_Leveraging_E3_Invariance_for_Unsupervised_3D_Planar_Reflective_Symmetry_ICCV_2023_paper.pdf)
* [Towards Improved Input Masking for Convolutional Neural Networks](http://arxiv.org/abs/2211.14646)
* [SIGMA: Scale-Invariant Global Sparse Shape Matching](http://arxiv.org/abs/2308.08393)
* [Pluralistic Aging Diffusion Autoencoder](http://arxiv.org/abs/2303.11086)
* [PEANUT: Predicting and Navigating to Unseen Targets](http://arxiv.org/abs/2212.02497)
* [ModelGiF: Gradient Fields for Model Functional Distance](http://arxiv.org/abs/2309.11013)
* [Robust Evaluation of Diffusion-Based Adversarial Purification](http://arxiv.org/abs/2303.09051)
* [R-Pred: Two-Stage Motion Prediction Via Tube-Query Attention-Based Trajectory Refinement](https://openaccess.thecvf.com/content/ICCV2023/papers/Choi_R-Pred_Two-Stage_Motion_Prediction_Via_Tube-Query_Attention-Based_Trajectory_Refinement_ICCV_2023_paper.pdf)
* [DynamicISP: Dynamically Controlled Image Signal Processor for Image Recognition](http://arxiv.org/abs/2211.01146)
* [AutoReP: Automatic ReLU Replacement for Fast Private Network Inference](http://arxiv.org/abs/2308.10134)
* [Candidate-aware Selective Disambiguation Based On Normalized Entropy for Instance-dependent Partial-label Learning](https://openaccess.thecvf.com/content/ICCV2023/papers/He_Candidate-aware_Selective_Disambiguation_Based_On_Normalized_Entropy_for_Instance-dependent_Partial-label_ICCV_2023_paper.pdf)
* [COPILOT: Human-Environment Collision Prediction and Localization from Egocentric Videos](http://arxiv.org/abs/2210.01781)
* [Convolutional Networks with Oriented 1D Kernels](https://openaccess.thecvf.com/content/ICCV2023/papers/Kirchmeyer_Convolutional_Networks_with_Oriented_1D_Kernels_ICCV_2023_paper.pdf)
* [SurfsUP: Learning Fluid Simulation for Novel Surfaces](http://arxiv.org/abs/2304.06197)
* [How Far Pre-trained Models Are from Neural Collapse on the Target Dataset Informs their Transferability](https://openaccess.thecvf.com/content/ICCV2023/papers/Wang_How_Far_Pre-trained_Models_Are_from_Neural_Collapse_on_the_ICCV_2023_paper.pdf)
* [SpaceEvo: Hardware-Friendly Search Space Design for Efficient INT8 Inference](http://arxiv.org/abs/2303.08308)
* [Designing Phase Masks for Under-Display Cameras](https://openaccess.thecvf.com/content/ICCV2023/papers/Yang_Designing_Phase_Masks_for_Under-Display_Cameras_ICCV_2023_paper.pdf)
* [GePSAn: Generative Procedure Step Anticipation in Cooking Videos](https://openaccess.thecvf.com/content/ICCV2023/papers/Abdelsalam_GePSAn_Generative_Procedure_Step_Anticipation_in_Cooking_Videos_ICCV_2023_paper.pdf)
* [Online Clustered Codebook](https://openaccess.thecvf.com/content/ICCV2023/papers/Kicanaoglu_Unsupervised_Facial_Performance_Editing_via_Vector-Quantized_StyleGAN_Representations_ICCV_2023_paper.pdf)
* [Source-free Depth for Object Pop-out](http://arxiv.org/abs/2212.05370)
* [Cross-Modal Translation and Alignment for Survival Analysis](http://arxiv.org/abs/2309.12855)
* [Partition Speeds Up Learning Implicit Neural Representations Based on Exponential-Increase Hypothesis](https://openaccess.thecvf.com/content/ICCV2023/papers/Liu_Partition_Speeds_Up_Learning_Implicit_Neural_Representations_Based_on_Exponential-Increase_ICCV_2023_paper.pdf)
* [Towards High-Quality Specular Highlight Removal by Leveraging Large-Scale Synthetic Data](http://arxiv.org/abs/2309.06302)
* [Corrupting Neuron Explanations of Deep Visual Features](https://openaccess.thecvf.com/content/ICCV2023/papers/Srivastava_Corrupting_Neuron_Explanations_of_Deep_Visual_Features_ICCV_2023_paper.pdf)
* [Editable Image Geometric Abstraction via Neural Primitive Assembly](https://openaccess.thecvf.com/content/ICCV2023/papers/Chen_Editable_Image_Geometric_Abstraction_via_Neural_Primitive_Assembly_ICCV_2023_paper.pdf)
* [DETRs with Collaborative Hybrid Assignments Training](http://arxiv.org/abs/2211.12860)
* [D3G: Exploring Gaussian Prior for Temporal Sentence Grounding with Glance Annotation](http://arxiv.org/abs/2308.04197)
* [OccFormer: Dual-path Transformer for Vision-based 3D Semantic Occupancy Prediction](http://arxiv.org/abs/2304.05316)
* [Group DETR: Fast DETR Training with Group-Wise One-to-Many Assignment](http://arxiv.org/abs/2207.13085)
* [LightGlue: Local Feature Matching at Light Speed](http://arxiv.org/abs/2306.13643)
* [Minimal Solutions to Generalized Three-View Relative Pose Problem](https://openaccess.thecvf.com/content/ICCV2023/papers/Ding_Minimal_Solutions_to_Generalized_Three-View_Relative_Pose_Problem_ICCV_2023_paper.pdf)
* [RICO: Regularizing the Unobservable for Indoor Compositional Reconstruction](http://arxiv.org/abs/2303.08605)
* [DCPB: Deformable Convolution Based on the Poincare Ball for Top-view Fisheye Cameras](https://openaccess.thecvf.com/content/ICCV2023/papers/Wei_DCPB_Deformable_Convolution_Based_on_the_Poincare_Ball_for_Top-view_ICCV_2023_paper.pdf)
* [Improving Lens Flare Removal with General-Purpose Pipeline and Multiple Light Sources Recovery](http://arxiv.org/abs/2308.16460)
* [CAFA: Class-Aware Feature Alignment for Test-Time Adaptation](http://arxiv.org/abs/2206.00205)
* [EverLight: Indoor-Outdoor Editable HDR Lighting Estimation](http://arxiv.org/abs/2304.13207)
* [Scale-MAE: A Scale-Aware Masked Autoencoder for Multiscale Geospatial Representation Learning](https://openaccess.thecvf.com/content/ICCV2023/papers/Reed_Scale-MAE_A_Scale-Aware_Masked_Autoencoder_for_Multiscale_Geospatial_Representation_Learning_ICCV_2023_paper.pdf)
* [Self-regulating Prompts: Foundational Model Adaptation without Forgetting](http://arxiv.org/abs/2307.06948)
* [MotionDeltaCNN: Sparse CNN Inference of Frame Differences in Moving Camera Videos with Spherical Buffers and Padded Convolutions](https://openaccess.thecvf.com/content/ICCV2023/papers/Parger_MotionDeltaCNN_Sparse_CNN_Inference_of_Frame_Differences_in_Moving_Camera_ICCV_2023_paper.pdf)
* [CRN: Camera Radar Net for Accurate, Robust, Efficient 3D Perception](http://arxiv.org/abs/2304.00670)
* [Efficient Diffusion Training via Min-SNR Weighting Strategy](http://arxiv.org/abs/2303.09556)
* [Shape Analysis of Euclidean Curves under Frenet-Serret Framework](https://openaccess.thecvf.com/content/ICCV2023/papers/Chassat_Shape_Analysis_of_Euclidean_Curves_under_Frenet-Serret_Framework_ICCV_2023_paper.pdf)
* [Learning Shape Primitives via Implicit Convexity Regularization](https://openaccess.thecvf.com/content/ICCV2023/papers/Huang_Learning_Shape_Primitives_via_Implicit_Convexity_Regularization_ICCV_2023_paper.pdf)
* [Exploring the Sim2Real Gap Using Digital Twins](https://openaccess.thecvf.com/content/ICCV2023/papers/Sudhakar_Exploring_the_Sim2Real_Gap_Using_Digital_Twins_ICCV_2023_paper.pdf)
* [Compatibility of Fundamental Matrices for Complete Viewing Graphs](https://openaccess.thecvf.com/content/ICCV2023/papers/Bratelund_Compatibility_of_Fundamental_Matrices_for_Complete_Viewing_Graphs_ICCV_2023_paper.pdf)
* [ContactGen: Generative Contact Modeling for Grasp Generation](https://openaccess.thecvf.com/content/ICCV2023/papers/Liu_ContactGen_Generative_Contact_Modeling_for_Grasp_Generation_ICCV_2023_paper.pdf)
* [Parametric Classification for Generalized Category Discovery: A Baseline Study](http://arxiv.org/abs/2211.11727)
* [SeiT: Storage-Efficient Vision Training with Tokens Using 1% of Pixel Storage](http://arxiv.org/abs/2303.11114)
* [Not All Steps are Created Equal: Selective Diffusion Distillation for Image Manipulation](http://arxiv.org/abs/2307.08448)
* [ENVIDR: Implicit Differentiable Renderer with Neural Environment Lighting](http://arxiv.org/abs/2303.13022)
* [Foreground-Background Separation through Concept Distillation from Generative Image Foundation Models](http://arxiv.org/abs/2212.14306)
* [Beyond Single Path Integrated Gradients for Reliable Input Attribution via Randomized Path Sampling](https://openaccess.thecvf.com/content/ICCV2023/papers/Jeon_Beyond_Single_Path_Integrated_Gradients_for_Reliable_Input_Attribution_via_ICCV_2023_paper.pdf)
* [CaPhy: Capturing Physical Properties for Animatable Human Avatars](http://arxiv.org/abs/2308.05925)
* [ICE-NeRF: Interactive Color Editing of NeRFs via Decomposition-Aware Weight Optimization](https://openaccess.thecvf.com/content/ICCV2023/papers/Lee_ICE-NeRF_Interactive_Color_Editing_of_NeRFs_via_Decomposition-Aware_Weight_Optimization_ICCV_2023_paper.pdf)
* [Toward Multi-Granularity Decision-Making: Explicit Visual Reasoning with Hierarchical Knowledge](https://openaccess.thecvf.com/content/ICCV2023/papers/Zhang_Toward_Multi-Granularity_Decision-Making_Explicit_Visual_Reasoning_with_Hierarchical_Knowledge_ICCV_2023_paper.pdf)
* [Breaking The Limits of Text-conditioned 3D Motion Synthesis with Elaborative Descriptions](https://openaccess.thecvf.com/content/ICCV2023/papers/Qian_Breaking_The_Limits_of_Text-conditioned_3D_Motion_Synthesis_with_Elaborative_ICCV_2023_paper.pdf)
* [AutoAD II: The Sequel - Who, When, and What in Movie Audio Description](https://openaccess.thecvf.com/content/ICCV2023/papers/Han_AutoAD_II_The_Sequel_-_Who_When_and_What_in_ICCV_2023_paper.pdf)
* [Bring Clipart to Life](https://openaccess.thecvf.com/content/ICCV2023/papers/Zhao_Bring_Clipart_to_Life_ICCV_2023_paper.pdf)
* [Few-Shot Dataset Distillation via Translative Pre-Training](https://openaccess.thecvf.com/content/ICCV2023/papers/Liu_Few-Shot_Dataset_Distillation_via_Translative_Pre-Training_ICCV_2023_paper.pdf)
* [Controllable Visual-Tactile Synthesis](http://arxiv.org/abs/2305.03051)
* [MolGrapher: Graph-based Visual Recognition of Chemical Structures](http://arxiv.org/abs/2308.12234)
* [The Effectiveness of MAE Pre-Pretraining for Billion-Scale Pretraining](http://arxiv.org/abs/2303.13496)
* [Towards Zero Domain Gap: A Comprehensive Study of Realistic LiDAR Simulation for Autonomy Testing](https://openaccess.thecvf.com/content/ICCV2023/papers/Manivasagam_Towards_Zero_Domain_Gap_A_Comprehensive_Study_of_Realistic_LiDAR_ICCV_2023_paper.pdf)
* [Learning Hierarchical Features with Joint Latent Space Energy-Based Prior](https://openaccess.thecvf.com/content/ICCV2023/papers/Cui_Learning_Hierarchical_Features_with_Joint_Latent_Space_Energy-Based_Prior_ICCV_2023_paper.pdf)
* [Regularized Primitive Graph Learning for Unified Vector Mapping](https://openaccess.thecvf.com/content/ICCV2023/papers/Wang_Regularized_Primitive_Graph_Learning_for_Unified_Vector_Mapping_ICCV_2023_paper.pdf)
* [Saliency Regularization for Self-Training with Partial Annotations](https://openaccess.thecvf.com/content/ICCV2023/papers/Wang_Saliency_Regularization_for_Self-Training_with_Partial_Annotations_ICCV_2023_paper.pdf)
* [Ord2Seq: Regarding Ordinal Regression as Label Sequence Prediction](https://openaccess.thecvf.com/content/ICCV2023/papers/Wang_Ord2Seq_Regarding_Ordinal_Regression_as_Label_Sequence_Prediction_ICCV_2023_paper.pdf)
* [ViM: Vision Middleware for Unified Downstream Transferring](http://arxiv.org/abs/2303.06911)
* [RANA: Relightable Articulated Neural Avatars](https://openaccess.thecvf.com/content/ICCV2023/papers/Iqbal_RANA_Relightable_Articulated_Neural_Avatars_ICCV_2023_paper.pdf)
* [Surface Extraction from Neural Unsigned Distance Fields](http://arxiv.org/abs/2309.08878)
* [Towards Nonlinear-Motion-Aware and Occlusion-Robust Rolling Shutter Correction](http://arxiv.org/abs/2303.18125)
* [Perpetual Humanoid Control for Real-time Simulated Avatars](https://openaccess.thecvf.com/content/ICCV2023/papers/Luo_Perpetual_Humanoid_Control_for_Real-time_Simulated_Avatars_ICCV_2023_paper.pdf)
* [Efficient Deep Space Filling Curve](https://openaccess.thecvf.com/content/ICCV2023/papers/Chen_Efficient_Deep_Space_Filling_Curve_ICCV_2023_paper.pdf)
* [Pixel-Wise Contrastive Distillation](http://arxiv.org/abs/2211.00218)
* [Anchor Structure Regularization Induced Multi-view Subspace Clustering via Enhanced Tensor Rank Minimization](https://openaccess.thecvf.com/content/ICCV2023/papers/Ji_Anchor_Structure_Regularization_Induced_Multi-view_Subspace_Clustering_via_Enhanced_Tensor_ICCV_2023_paper.pdf)
* [FACTS: First Amplify Correlations and Then Slice to Discover Bias](https://openaccess.thecvf.com/content/ICCV2023/papers/Yenamandra_FACTS_First_Amplify_Correlations_and_Then_Slice_to_Discover_Bias_ICCV_2023_paper.pdf)
* [Pairwise Similarity Learning is SimPLE](https://openaccess.thecvf.com/content/ICCV2023/papers/Wen_Pairwise_Similarity_Learning_is_SimPLE_ICCV_2023_paper.pdf)
* [PanFlowNet: A Flow-Based Deep Network for Pan-Sharpening](http://arxiv.org/abs/2305.07774)
* [Visual Explanations via Iterated Integrated Attributions](https://openaccess.thecvf.com/content/ICCV2023/papers/Barkan_Visual_Explanations_via_Iterated_Integrated_Attributions_ICCV_2023_paper.pdf)
* [Make-It-3D: High-fidelity 3D Creation from A Single Image with Diffusion Prior](https://openaccess.thecvf.com/content/ICCV2023/papers/Tang_Make-It-3D_High-fidelity_3D_Creation_from_A_Single_Image_with_Diffusion_ICCV_2023_paper.pdf)
* [Fully Attentional Networks with Self-emerging Token Labeling](https://openaccess.thecvf.com/content/ICCV2023/papers/Zhao_Fully_Attentional_Networks_with_Self-emerging_Token_Labeling_ICCV_2023_paper.pdf)
* [DMNet: Delaunay Meshing Network for 3D Shape Representation](https://openaccess.thecvf.com/content/ICCV2023/papers/Zhang_DMNet_Delaunay_Meshing_Network_for_3D_Shape_Representation_ICCV_2023_paper.pdf)
* [Eulerian Single-Photon Vision](https://openaccess.thecvf.com/content/ICCV2023/papers/Gupta_Eulerian_Single-Photon_Vision_ICCV_2023_paper.pdf)
* [Adaptive Calibrator Ensemble: Navigating Test Set Difficulty in Out-of-Distribution Scenarios](https://openaccess.thecvf.com/content/ICCV2023/papers/Zou_Adaptive_Calibrator_Ensemble_Navigating_Test_Set_Difficulty_in_Out-of-Distribution_Scenarios_ICCV_2023_paper.pdf)
* [Efficient Converted Spiking Neural Network for 3D and 2D Classification](https://openaccess.thecvf.com/content/ICCV2023/papers/Lan_Efficient_Converted_Spiking_Neural_Network_for_3D_and_2D_Classification_ICCV_2023_paper.pdf)
* [Sequential Texts Driven Cohesive Motions Synthesis with Natural Transitions](https://openaccess.thecvf.com/content/ICCV2023/papers/Li_Sequential_Texts_Driven_Cohesive_Motions_Synthesis_with_Natural_Transitions_ICCV_2023_paper.pdf)
* [Locomotion-Action-Manipulation: Synthesizing Human-Scene Interactions in Complex 3D Environments](https://openaccess.thecvf.com/content/ICCV2023/papers/Lee_Locomotion-Action-Manipulation_Synthesizing_Human-Scene_Interactions_in_Complex_3D_Environments_ICCV_2023_paper.pdf)
* [Human Preference Score: Better Aligning Text-to-Image Models with Human Preference](http://arxiv.org/abs/2303.14420)
* [D-IF: Uncertainty-aware Human Digitization via Implicit Distribution Field](https://openaccess.thecvf.com/content/ICCV2023/papers/Yang_D-IF_Uncertainty-aware_Human_Digitization_via_Implicit_Distribution_Field_ICCV_2023_paper.pdf)
* [Unsupervised Manifold Linearizing and Clustering](http://arxiv.org/abs/2301.01805)
* [P1AC: Revisiting Absolute Pose From a Single Affine Correspondence](http://arxiv.org/abs/2011.08790)
* [Mining bias-target Alignment from Voronoi Cells](http://arxiv.org/abs/2305.03691)
* [Exploring the Benefits of Visual Prompting in Differential Privacy](http://arxiv.org/abs/2303.12247)
* [The Victim and The Beneficiary: Exploiting a Poisoned Model to Train a Clean Model on Poisoned Data](https://openaccess.thecvf.com/content/ICCV2023/papers/Zhu_The_Victim_and_The_Beneficiary_Exploiting_a_Poisoned_Model_to_ICCV_2023_paper.pdf)
* [End2End Multi-View Feature Matching with Differentiable Pose Optimization](https://openaccess.thecvf.com/content/ICCV2023/papers/Roessle_End2End_Multi-View_Feature_Matching_with_Differentiable_Pose_Optimization_ICCV_2023_paper.pdf)
* [Task Agnostic Restoration of Natural Video Dynamics](http://arxiv.org/abs/2206.03753)
* [GPFL: Simultaneously Learning Global and Personalized Feature Information for Personalized Federated Learning](http://arxiv.org/abs/2308.10279)
* [Scene Graph Contrastive Learning for Embodied Navigation](https://openaccess.thecvf.com/content/ICCV2023/papers/Singh_Scene_Graph_Contrastive_Learning_for_Embodied_Navigation_ICCV_2023_paper.pdf)
* [Mastering Spatial Graph Prediction of Road Networks](http://arxiv.org/abs/2210.00828)
* [ETran: Energy-Based Transferability Estimation](http://arxiv.org/abs/2308.02027)
* [Inverse Problem Regularization with Hierarchical Variational Autoencoders](http://arxiv.org/abs/2303.11217)
* [Learning a Room with the Occ-SDF Hybrid: Signed Distance Function Mingled with Occupancy Aids Scene Representation](http://arxiv.org/abs/2303.09152)
* [Context-Aware Planning and Environment-Aware Memory for Instruction Following Embodied Agents](http://arxiv.org/abs/2308.07241)
* [Minimal Solutions to Uncalibrated Two-view Geometry with Known Epipoles](https://openaccess.thecvf.com/content/ICCV2023/papers/Nakano_Minimal_Solutions_to_Uncalibrated_Two-view_Geometry_with_Known_Epipoles_ICCV_2023_paper.pdf)
* [Deep Geometry-Aware Camera Self-Calibration from Video](https://openaccess.thecvf.com/content/ICCV2023/papers/Hagemann_Deep_Geometry-Aware_Camera_Self-Calibration_from_Video_ICCV_2023_paper.pdf)
* [Feature Proliferation -- the "Cancer" in StyleGAN and its Treatments](https://openaccess.thecvf.com/content/ICCV2023/papers/Song_Feature_Proliferation_--_the_Cancer_in_StyleGAN_and_its_Treatments_ICCV_2023_paper.pdf)
* [Adaptive Testing of Computer Vision Models](http://arxiv.org/abs/2212.02774)
* [Semantic Attention Flow Fields for Monocular Dynamic Scene Decomposition](https://openaccess.thecvf.com/content/ICCV2023/papers/Liang_Semantic_Attention_Flow_Fields_for_Monocular_Dynamic_Scene_Decomposition_ICCV_2023_paper.pdf)
* [Time-to-Contact Map by Joint Estimation of Up-to-Scale Inverse Depth and Global Motion using a Single Event Camera](https://openaccess.thecvf.com/content/ICCV2023/papers/Nunes_Time-to-Contact_Map_by_Joint_Estimation_of_Up-to-Scale_Inverse_Depth_and_ICCV_2023_paper.pdf)
* [Cross-modal Latent Space Alignment for Image to Avatar Translation](https://openaccess.thecvf.com/content/ICCV2023/papers/de_Guevara_Cross-modal_Latent_Space_Alignment_for_Image_to_Avatar_Translation_ICCV_2023_paper.pdf)
* [Partition-And-Debias: Agnostic Biases Mitigation via a Mixture of Biases-Specific Experts](https://openaccess.thecvf.com/content/ICCV2023/papers/Li_Partition-And-Debias_Agnostic_Biases_Mitigation_via_a_Mixture_of_Biases-Specific_Experts_ICCV_2023_paper.pdf)
* [Learning to Transform for Generalizable Instance-wise Invariance](https://openaccess.thecvf.com/content/ICCV2023/papers/Singhal_Learning_to_Transform_for_Generalizable_Instance-wise_Invariance_ICCV_2023_paper.pdf)
* [DeePoint: Visual Pointing Recognition and Direction Estimation](http://arxiv.org/abs/2304.06977)
* [Sigmoid Loss for Language Image Pre-Training](http://arxiv.org/abs/2303.15343)
* [Tracking by 3D Model Estimation of Unknown Objects in Videos](http://arxiv.org/abs/2304.06419)
* [Physically-Plausible Illumination Distribution Estimation](https://openaccess.thecvf.com/content/ICCV2023/papers/Ershov_Physically-Plausible_Illumination_Distribution_Estimation_ICCV_2023_paper.pdf)
* [Exploiting Proximity-Aware Tasks for Embodied Social Navigation](http://arxiv.org/abs/2212.00767)
* [SkeletonMAE: Graph-based Masked Autoencoder for Skeleton Sequence Pre-training](http://arxiv.org/abs/2307.08476)
* [Studying How to Efficiently and Effectively Guide Models with Explanations](https://openaccess.thecvf.com/content/ICCV2023/papers/Rao_Studying_How_to_Efficiently_and_Effectively_Guide_Models_with_Explanations_ICCV_2023_paper.pdf)
* [NeuRBF: A Neural Fields Representation with Adaptive Radial Basis Functions](http://arxiv.org/abs/2309.15426v1)<br>:star:[code](https://oppo-us-research.github.io/NeuRBF-website/)
* [DECO: Dense Estimation of 3D Human-Scene Contact In The Wild](http://arxiv.org/abs/2309.15273v1)<br>:house:[project](https://deco.is.tue.mpg.de)
* [PHRIT: Parametric Hand Representation with Implicit Template](http://arxiv.org/abs/2309.14916v1)
* [Improving Unsupervised Visual Program Inference with Code Rewriting Families](http://arxiv.org/abs/2309.14972v1)<br>:star:[code](https://bardofcodes.github.io/coref/)
* [Generating Visual Scenes from Touch](http://arxiv.org/abs/2309.15117v1)<br>:star:[code](https://fredfyyang.github.io/vision-from-touch/)
* [LOGICSEG: Parsing Visual Semantics with Neural Logic Learning and Reasoning](http://arxiv.org/abs/2309.13556v1)<br>:star:[code](https://github.com/lingorX/LogicSeg/)
* [Automatic Animation of Hair Blowing in Still Portrait Photos](http://arxiv.org/abs/2309.14207v1)<br>:star:[code](https://nevergiveu.github.io/AutomaticHairBlowing/)
* [Cross-Modal Translation and Alignment for Survival Analysis](http://arxiv.org/abs/2309.12855v1)
* [Contrastive Pseudo Learning for Open-World DeepFake Attribution](http://arxiv.org/abs/2309.11132v1)
* [Dense 2D-3D Indoor Prediction with Sound via Aligned Cross-Modal Distillation](http://arxiv.org/abs/2309.11081v1)
* [Segmentation of Tubular Structures Using Iterative Training with Tailored Samples](http://arxiv.org/abs/2309.08727v1)
* [Active Stereo Without Pattern Projector](http://arxiv.org/abs/2309.12315v1)<br>:star:[code](https://vppstereo.github.io)<br>:star:[code](https://github.com/bartn8/vppstereo)
* [TinyCLIP: CLIP Distillation via Affinity Mimicking and Weight Inheritance](http://arxiv.org/abs/2309.12314v1)<br>:house:[project](https://aka.ms/tinyclip)
* [BANSAC: A dynamic BAyesian Network for adaptive SAmple Consensus](http://arxiv.org/abs/2309.08690v1)
* [Tree-Structured Shading Decomposition](http://arxiv.org/abs/2309.07122v1)<br>:house:[project](https://chen-geng.com/inv-shade-trees)
* [Keep It SimPool: Who Said Supervised Transformers Suffer from Attention Deficit?](http://arxiv.org/abs/2309.06891v1)<br>:star:[code](https://github.com/billpsomas/simpool)
* [Beyond Skin Tone: A Multidimensional Measure of Apparent Skin Color](http://arxiv.org/abs/2309.05148v1)
* [Multi3DRefer: Grounding Text Description to Multiple 3D Objects](http://arxiv.org/abs/2309.05251v1)
* [Panoramas from Photons](http://arxiv.org/abs/2309.03811v1)<br>:house:[project](https://wisionlab.com/project/panoramas-from-photons/)
* [SimNP: Learning Self-Similarity Priors Between Neural Points](http://arxiv.org/abs/2309.03809v1)
* [Multi-label affordance mapping from egocentric vision](http://arxiv.org/abs/2309.02120v1)
* [SoDaCam: Software-defined Cameras via Single-Photon Imaging](http://arxiv.org/abs/2309.00066v1)<br>:house:[project](https://wisionlab.com/project/sodacam/)
* [PivotNet: Vectorized Pivot Learning for End-to-end HD Map Construction](http://arxiv.org/abs/2308.16477v1)
* [Active Neural Mapping](http://arxiv.org/abs/2308.16246v1)<br>:star:[code](https://zikeyan.github.io/active-INR/index.html)
* [Reconstructing Groups of People with Hypergraph Relational Reasoning](http://arxiv.org/abs/2308.15844v1)<br>:star:[code](https://github.com/boycehbz/GroupRec)
* [Learning to Upsample by Learning to Sample](http://arxiv.org/abs/2308.15085v1)<br>:star:[code](https://github.com/tiny-smart/dysample)
* [Efficient Discovery and Effective Evaluation of Visual Perceptual Similarity: A Benchmark and Beyond](http://arxiv.org/abs/2308.14753v1)
* [S-TREK: Sequential Translation and Rotation Equivariant Keypoints for local feature extraction](http://arxiv.org/abs/2308.14598v1)
* [Unaligned 2D to 3D Translation with Conditional Vector-Quantized Code Diffusion using Transformers](http://arxiv.org/abs/2308.14152v1)
* [4D Myocardium Reconstruction with Decoupled Motion and Shape Model](http://arxiv.org/abs/2308.14083v1)
* [Hierarchical Contrastive Learning for Pattern-Generalizable Image Corruption Detection](http://arxiv.org/abs/2308.14061v1)<br>:star:[code](https://github.com/xyfJASON/HCL)
* [LDL: Line Distance Functions for Panoramic Localization](http://arxiv.org/abs/2308.13989v1)<br>:star:[code](https://github.com/82magnolia/panoramic-localization)
* [Generalized Lightness Adaptation with Channel Selective Normalization](http://arxiv.org/abs/2308.13783v1)<br>:star:[code](https://github.com/mdyao/CSNorm/)<br>:star:[code](https://github.com/mdyao/CSNorm)
* [MST-compression: Compressing and Accelerating Binary Neural Networks with Minimum Spanning Tree](http://arxiv.org/abs/2308.13735v1)
* [Late Stopping: Avoiding Confidently Learning from Mislabeled Examples](http://arxiv.org/abs/2308.13862v1)
* [Motion-Guided Masking for Spatiotemporal Representation Learning](http://arxiv.org/abs/2308.12962v1)
* [Self-supervised Learning of Implicit Shape Representation with Dense Correspondence for Deformable Objects](http://arxiv.org/abs/2308.12590v1)<br>:star:[code](https://iscas3dv.github.io/deformshape)
* [SUMMIT: Source-Free Adaptation of Uni-Modal Models to Multi-Modal Targets](http://arxiv.org/abs/2308.11880v1)<br>:star:[code](https://github.com/csimo005/SUMMIT)
* [DR-Tune: Improving Fine-tuning of Pretrained Visual Models by Distribution Regularization with Semantic Calibration](http://arxiv.org/abs/2308.12058v1)<br>:star:[code](https://github.com/weeknan/DR-Tune)
* [RankMixup: Ranking-Based Mixup Training for Network Calibration](http://arxiv.org/abs/2308.11990v1)
* [ACLS: Adaptive and Conditional Label Smoothing for Network Calibration](http://arxiv.org/abs/2308.11911v1)
* [Enhancing NeRF akin to Enhancing LLMs: Generalizable NeRF Transformer with Mixture-of-View-Experts](http://arxiv.org/abs/2308.11793v1)<br>:star:[code](https://github.com/VITA-Group/GNT-MOVE)
* [SPANet: Frequency-balancing Token Mixer using Spectral Pooling Aggregation Modulation](http://arxiv.org/abs/2308.11568v1)<br>:star:[code](https://doranlyong.github.io/projects/spanet/)
* [Learning a More Continuous Zero Level Set in Unsigned Distance Fields through Level Set Projection](http://arxiv.org/abs/2308.11441v1)<br>:star:[code](https://github.com/junshengzhou/LevelSetUDF)
* [CAME: Contrastive Automated Model Evaluation](http://arxiv.org/abs/2308.11111v1)
* [LDP-Feat: Image Features with Local Differential Privacy](http://arxiv.org/abs/2308.11223v1)
* [DiffCloth: Diffusion Based Garment Synthesis and Manipulation via Structural Cross-modal Semantic Alignment](http://arxiv.org/abs/2308.11206v1)
* [Diffusion Model as Representation Learner](http://arxiv.org/abs/2308.10916v1)<br>:star:[code](https://github.com/Adamdad/Repfusion)
* [MetaGCD: Learning to Continually Learn in Generalized Category Discovery](http://arxiv.org/abs/2308.11063v1)
* [Few-Shot Physically-Aware Articulated Mesh Generation via Hierarchical Deformation](http://arxiv.org/abs/2308.10898v1)<br>:star:[code](https://meowuu7.github.io/few-arti-obj-gen)
* [Improving Adversarial Robustness of Masked Autoencoders via Test-time Frequency-domain Prompting](http://arxiv.org/abs/2308.10315v1)<br>:star:[code](https://github.com/shikiw/RobustMAE)
* [Robust Mixture-of-Expert Training for Convolutional Neural Networks](http://arxiv.org/abs/2308.10110v1)<br>:star:[code](https://github.com/OPTML-Group/Robust-MoE-CNN)
* [Single Image Reflection Separation via Component Synergy](http://arxiv.org/abs/2308.10027v1)<br>:star:[code](https://github.com/mingcv/DSRNet)
* [Partition-and-Debias: Agnostic Biases Mitigation via A Mixture of Biases-Specific Experts](http://arxiv.org/abs/2308.10005v1)<br>:star:[code](https://github.com/Jiaxuan-Li/PnD)
* [On the Robustness of Open-World Test-Time Training: Self-Training with Dynamic Prototype Expansion](http://arxiv.org/abs/2308.09942v1)<br>:star:[code](https://github.com/Yushu-Li/OWTTT)
* [Understanding Self-attention Mechanism via Dynamical System Perspective](http://arxiv.org/abs/2308.09939v1)
* [A Theory of Topological Derivatives for Inverse Rendering of Geometry](http://arxiv.org/abs/2308.09865v1)<br>:star:[code](https://ishit.github.io/td/)
* [X-VoE: Measuring eXplanatory Violation of Expectation in Physical Events](http://arxiv.org/abs/2308.10441v1)<br>:house:[project](https://pku.ai/publication/intuitive2023iccv/)
* [ClothesNet: An Information-Rich 3D Garment Model Repository with Simulated Clothes Environment](http://arxiv.org/abs/2308.09987v1)
* [Leveraging Intrinsic Properties for Non-Rigid Garment Alignment](http://arxiv.org/abs/2308.09519v1)<br>:star:[code](https://jsnln.github.io/iccv2023_intrinsic/index.html)
* [Event-Guided Procedure Planning from Instructional Videos with Text Supervision](http://arxiv.org/abs/2308.08885v1)
* [Spatially and Spectrally Consistent Deep Functional Maps](http://arxiv.org/abs/2308.08871v1)<br>:star:[code](https://github.com/rqhuang88/Spatiallyand-Spectrally-Consistent-Deep-Functional-Maps)
* [Realistic Full-Body Tracking from Sparse Observations via Joint-Level Modeling](http://arxiv.org/abs/2308.08855v1)<br>:star:[code](https://zxz267.github.io/AvatarJLM)
* [Label Shift Adapter for Test-Time Adaptation under Covariate and Label Shifts](http://arxiv.org/abs/2308.08810v1)
* [ALIP: Adaptive Language-Image Pre-training with Synthetic Caption](http://arxiv.org/abs/2308.08428v1)<br>:star:[code](https://github.com/deepglint/ALIP)
* [Membrane Potential Batch Normalization for Spiking Neural Networks](http://arxiv.org/abs/2308.08359v1)<br>:star:[code](https://github.com/yfguo91/MPBN)
* [OmniZoomer: Learning to Move and Zoom in on Sphere at High-Resolution](http://arxiv.org/abs/2308.08114v1)<br>:star:[code](http://vlislab22.github.io/OmniZoomer/)
* [Inherent Redundancy in Spiking Neural Networks](http://arxiv.org/abs/2308.08227v1)<br>:star:[code](https://github.com/BICLab/ASA-SNN)
* [One-bit Flip is All You Need: When Bit-flip Attack Meets Model Training](http://arxiv.org/abs/2308.07934v1)<br>:star:[code](https://github.com/jianshuod/TBA)
* [ObjectSDF++: Improved Object-Compositional Neural Implicit Surfaces](http://arxiv.org/abs/2308.07868v1)<br>:star:[code](https://qianyiwu.github.io/objectsdf++)<br>:star:[code](https://github.com/QianyiWu/objectsdf_plus)
* [UniTR: A Unified and Efficient Multi-Modal Transformer for Bird's-Eye-View Representation](http://arxiv.org/abs/2308.07732v1)<br>:star:[code](https://github.com/Haiyang-W/UniTR)
* [PARIS: Part-level Reconstruction and Motion Analysis for Articulated Objects](http://arxiv.org/abs/2308.07391v1)<br>:star:[code](https://3dlg-hcvc.github.io/paris/)<br>:house:[project](https://youtu.be/tDSrROPCgUc)
* [Boosting Multi-modal Model Performance with Adaptive Gradient Modulation](http://arxiv.org/abs/2308.07686v1)<br>:star:[code](https://github.com/lihong2303/AGM_ICCV2023)
* [Towards Open-Set Test-Time Adaptation Utilizing the Wisdom of Crowds in Entropy Minimization](http://arxiv.org/abs/2308.06879v1)
* [RMP-Loss: Regularizing Membrane Potential Distribution for Spiking Neural Networks](http://arxiv.org/abs/2308.06787v1)
* [Estimator Meets Equilibrium Perspective: A Rectified Straight Through Estimator for Binary Neural Networks Training](http://arxiv.org/abs/2308.06689v1)
* [GIFD: A Generative Gradient Inversion Method with Feature Domain Optimization](http://arxiv.org/abs/2308.04699v1)
* [Prune Spatio-temporal Tokens by Semantic-aware Temporal Accumulation](http://arxiv.org/abs/2308.04549v1)<br>:star:[code](https://github.com/Mark12Ding/STA)
* [TIJO: Trigger Inversion with Joint Optimization for Defending Multimodal Backdoored Models](http://arxiv.org/abs/2308.03906v1)<br>:star:[code](https://github.com/SRI-CSL/TIJO)
* [3D Motion Magnification: Visualizing Subtle Motions with Time Varying Radiance Fields](http://arxiv.org/abs/2308.03757v1)<br>:star:[code](https://3d-motion-magnification.github.io)
* [Improving Pixel-based MIM by Reducing Wasted Modeling Capability](http://arxiv.org/abs/2308.00261v1)<br>:star:[code](https://github.com/open-mmlab/mmpretrain)
* [Revisiting the Parameter Efficiency of Adapters from the Perspective of Precision Redundancy](http://arxiv.org/abs/2307.16867v1)<br>:star:[code](https://github.com/JieShibo/PETL-ViT)
* [Towards General Low-Light Raw Noise Synthesis and Modeling](http://arxiv.org/abs/2307.16508v1)<br>:star:[code](https://github.com/fengzhang427/LRD)
* [Supervised Homography Learning with Realistic Dataset Generation](http://arxiv.org/abs/2307.15353v1)<br>:star:[code](https://github.com/megvii-research/RealSH)
* [Dynamic PlenOctree for Adaptive Sampling Refinement in Explicit NeRF](http://arxiv.org/abs/2307.15333v1)<br>:house:[project](https://vlislab22.github.io/DOT)
* [Spatio-Temporal Domain Awareness for Multi-Agent Collaborative Perception](http://arxiv.org/abs/2307.13929v1)
* [Conditional Cross Attention Network for Multi-Space Embedding without Entanglement in Only a SINGLE Network](http://arxiv.org/abs/2307.13254v1)
* [Rethinking Data Distillation: Do Not Overlook Calibration](http://arxiv.org/abs/2307.12463v1)
* [Kick Back & Relax: Learning to Reconstruct the World by Watching SlowTV](http://arxiv.org/abs/2307.10713v1)<br>:star:[code](https://github.com/jspenmar/slowtv_monodepth)
* [Towards Viewpoint-Invariant Visual Recognition via Adversarial Training](http://arxiv.org/abs/2307.10235v1)
* [Tuning Pre-trained Model via Moment Probing](http://arxiv.org/abs/2307.11342v1)<br>:star:[code](https://github.com/mingzeG/Moment-Probing)
* [Replay: Multi-modal Multi-view Acted Videos for Casual Holography](https://arxiv.org/abs/2307.12067)
* [TextManiA: Enriching Visual Feature by Text-driven Manifold Augmentation](http://arxiv.org/abs/2307.14611v1)
* [Rethinking Mobile Block for Efficient Attention-based Models](https://arxiv.org/pdf/2301.01146.pdf)
* [RLSAC: Reinforcement Learning enhanced Sample Consensus for End-to-End Robust Estimation](http://arxiv.org/abs/2308.05318v1)<br>:star:[code](https://github.com/IRMVLab/RLSAC)
* [Cross-Domain Product Representation Learning for Rich-Content E-Commerce](http://arxiv.org/abs/2308.05550v1)
* [Interaction-aware Joint Attention Estimation Using People Attributes](http://arxiv.org/abs/2308.05382v1)<br>:house:[project](https://anonymous.4open.science/r/anonymized_codes-ECA4)


### æ‰«ç CVå›å¾®ä¿¡(æ³¨æ˜ï¼špaper)å…¥å¾®ä¿¡äº¤æµç¾¤ï¼š
![9475fa20fd5e95235d9fa23ae9587a2](https://user-images.githubusercontent.com/62801906/156720309-de92964f-a6da-464a-b21f-cfb270c13e27.png)
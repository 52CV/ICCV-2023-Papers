# ICCV-2023-Papers
![Alt text](af0c53186833a908a200f58867b6dcf.png)

## 官网链接：https://iccv2023.thecvf.com/

### 研讨会:bell:：2023 年 10 月 2 日至 3 日<br>
### 主会:bell:：2023 年 10 月 4 日至 6 日

## 历年综述论文分类汇总戳这里↘️[CV-Surveys](https://github.com/52CV/CV-Surveys)施工中~~~~~~~~~~

## 2023 年论文分类汇总戳这里
↘️[CVPR-2023-Papers](https://github.com/52CV/CVPR-2023-Papers)
↘️[WACV-2023-Papers](https://github.com/52CV/WACV-2023-Papers)
↘️[ICCV-2023-Papers](https://github.com/52CV/ICCV-2023-Papers)

## 2022 年论文分类汇总戳这里
↘️[CVPR-2022-Papers](https://github.com/52CV/CVPR-2022-Papers)
↘️[WACV-2022-Papers](https://github.com/52CV/WACV-2022-Papers)
↘️[ECCV-2022-Papers](https://github.com/52CV/ECCV-2022-Papers)

## 2021年论文分类汇总戳这里
↘️[ICCV-2021-Papers](https://github.com/52CV/ICCV-2021-Papers)
↘️[CVPR-2021-Papers](https://github.com/52CV/CVPR-2021-Papers)

## 2020 年论文分类汇总戳这里
↘️[CVPR-2020-Papers](https://github.com/52CV/CVPR-2020-Papers)
↘️[ECCV-2020-Papers](https://github.com/52CV/ECCV-2020-Papers)

计650+3 篇。 

## 目录

|:cat:|:dog:|:tiger:|:wolf:|
|------|------|------|------|
|[1.其它(others)](#1)|[2.3D(三维重建\三维视觉)](#2)|[3.Object Detection(目标检测)](#3)|[4.Object Tracking(目标跟踪)](#4)|
|[5.Biometric Recognition(生物特征识别)](#5)|[6.Face(人脸)](#6)|[7.Image Progress(低层图像处理、质量评价)](#7)|[8.Image Segmentation(图像分割)](#8)|
|[9.Image Classification(图像分类)](#9)|[10.Image Synthesis(图像合成)](#10)|[11.Image/Video Editing(图像/视频编辑)](#11)|[12.Medical Image(医学影像)](#12)|
|[13.Image Captions(图像字幕)](#13)|[14.Image/Video Composition(图像/视频压缩)](#14)|[15.Image/Video Retrieval(图像/视频检索)](#15)|[16.Super-Resolution(超分辨率)](#16)|
|[17.GAN](#17)|[18.Pose](#18)|[19.UAV/Remote Sensing/Satellite Image(无人机/遥感/卫星图像)](#19)|[20.Reid](#20)|
|[21.Point Cloud(点云)](#21)|[22.OCR](#22)|[23.Optical Flow Estimation(光流估计)](#23)|[24.Few/Zero-Shot Learning/Domain Generalization/Adaptation(小/零样本/域泛化/域适应)](#24)|
|[25.Model Compression/KD/Pruning(模型压缩/知识蒸馏/剪枝)](#25)|[26.ML(机器学习)](#26)|[27.Self/Semi-Supervised Learning](#27)|[28.Style Transfer(风格迁移)](#28)|
|[29.Autonomous vehicles(自动驾驶)](#30)|[30.SLAM/AR/VR/Robotics(增强/虚拟现实/机器人)](#30)|[31.HOI(人物交互)](#31)|[32.Sign Language Recognition(手语)](#32)|
|[33.Video](#33)|[34.Action Detection](#34)|[35.Human Motion Prediction(人体运动预测)](#35)|[36.Vision Question Answering(视觉问答)](#36)|
|[37.Object Pose Estimation(物体姿势估计)](#37)|[38.Vision-Language(视觉语言)](#38)|[39.Keypoint Detection(关键点检测)](#39)|[40.Anomaly Detection(异常检测)](#40)|
|[41.Vision Transformers](#41)|[42.Dataset/Benchmark](#42)|[43.Neural Radiance Fields](#43)|[44.Rendering(渲染)](#44)|
|[45.Scene Graph Generation(场景图合成)](#45)|[46.Edge Detection](#46)|[47.Image-to-Image Translation](#47)|[48.Image Reconstruction](#48)|
|[49.Image Fusion(图像融合)](#49)|[50.Image Matching(图像匹配)](#50)|[51.Visual Place Recognition](#51)|[52.View Synthesis(视图合成)](#52)|
|[53.Computed Imaging(计算成像，如光学、几何、光场成像等)](#53)|[54.Gaze Estimation](#54)|[55.sound(语音)](#55)|


## 💥💥💥ICCV 2023 获奖论文
### 最佳论文奖——马尔奖
* [Adding Conditional Control to Text-to-Image Diffusion Models](https://arxiv.org/pdf/2302.05543.pdf)<br>:star:[code](https://github.com/lllyasviel/ControlNet)
* [Passive Ultra-Wideband Single-Photon Imaging](https://appleswithacapitala.github.io/static/docs/paper.pdf)<br>:star:[code](https://appleswithacapitala.github.io/)
### 最佳论文奖提名
* [Segment Anything](https://arxiv.org/abs/2304.02643)<br>:house:[project](https://segment-anything.com/)
### 最佳学生论文奖
* [Tracking Everything Everywhere All at Once](https://browse.arxiv.org/pdf/2306.05422.pdf)<br>:house:[project](https://github.com/qianqianwang68/omnimotion)

## Open Set Recognition(开集识别)
* [FedPD: Federated Open Set Recognition with Parameter Disentanglement](https://openaccess.thecvf.com/content/ICCV2023/papers/Yang_FedPD_Federated_Open_Set_Recognition_with_Parameter_Disentanglement_ICCV_2023_paper.pdf)

## Graph Neural Networks(图神经网络)
* [VertexSerum: Poisoning Graph Neural Networks for Link Inference](http://arxiv.org/abs/2308.01469)
* [Learning Adaptive Neighborhoods for Graph Neural Networks](http://arxiv.org/abs/2307.09065)

## Deepfake Detectors
* [Towards Understanding the Generalization of Deepfake Detectors from a Game-Theoretical View](https://openaccess.thecvf.com/content/ICCV2023/papers/Yao_Towards_Understanding_the_Generalization_of_Deepfake_Detectors_from_a_Game-Theoretical_ICCV_2023_paper.pdf)

## Scene Understanding(场景理解)
* [Efficient Computation Sharing for Multi-Task Visual Scene Understanding](http://arxiv.org/abs/2303.09663)

## Industrial Defect Detectors
* 工业缺陷定位
  * [Removing Anomalies as Noises for Industrial Defect Localization](https://openaccess.thecvf.com/content/ICCV2023/papers/Lu_Removing_Anomalies_as_Noises_for_Industrial_Defect_Localization_ICCV_2023_paper.pdf)
* 裂缝检测
  * [The Devil is in the Crack Orientation: A New Perspective for Crack Detection](https://openaccess.thecvf.com/content/ICCV2023/papers/Chen_The_Devil_is_in_the_Crack_Orientation_A_New_Perspective_ICCV_2023_paper.pdf)

## Group Affect Recognition(群体情感识别)
* [Most Important Person-Guided Dual-Branch Cross-Patch Attention for Group Affect Recognition](https://openaccess.thecvf.com/content/ICCV2023/papers/Xie_Most_Important_Person-Guided_Dual-Branch_Cross-Patch_Attention_for_Group_Affect_Recognition_ICCV_2023_paper.pdf)

## Visual Localization(视觉定位)
* [EP2P-Loc: End-to-End 3D Point to 2D Pixel Localization for Large-Scale Visual Localization](http://arxiv.org/abs/2309.07471v1)
* [Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance](http://arxiv.org/abs/2309.07403v1)

## Copyright Protection(版权保护/信息安全)
* [Towards Robust Model Watermark via Reducing Parametric Vulnerability](http://arxiv.org/abs/2309.04777v1)<br>:star:[code](https://github.com/GuanhaoGan/robust-model-watermarking)

## scene flow estimation(场景流估计)
* [EMR-MSF: Self-Supervised Recurrent Monocular Scene Flow Exploiting Ego-Motion Rigidity](http://arxiv.org/abs/2309.01296v1)
* [Fast Neural Scene Flow](http://arxiv.org/abs/2304.09121)

## Semantic Scene Completion(语义场景补全)
* [NDC-Scene: Boost Monocular 3D Semantic Scene Completion in Normalized Device Coordinates Space](http://arxiv.org/abs/2309.14616v1)<br>:star:[code](https://jiawei-yao0812.github.io/NDC-Scene/)<br>:star:[code](https://github.com/Jiawei-Yao0812/NDCScene)
* [DDIT: Semantic Scene Completion via Deformable Deep Implicit Templates](https://openaccess.thecvf.com/content/ICCV2023/papers/Li_DDIT_Semantic_Scene_Completion_via_Deformable_Deep_Implicit_Templates_ICCV_2023_paper.pdf)

## NAS
* [ShiftNAS: Improving One-shot NAS via Probability Shift](http://arxiv.org/abs/2307.08300)

<a name="55"/>

## 55.sound(语音)
* [Boosting Positive Segments for Weakly-Supervised Audio-Visual Video Parsing](https://openaccess.thecvf.com/content/ICCV2023/papers/Rachavarapu_Boosting_Positive_Segments_for_Weakly-Supervised_Audio-Visual_Video_Parsing_ICCV_2023_paper.pdf)
* [DiffV2S: Diffusion-based Video-to-Speech Synthesis with Vision-guided Speaker Embedding](http://arxiv.org/abs/2308.07787v1)
* [Omnidirectional Information Gathering for Knowledge Transfer-based Audio-Visual Navigation](http://arxiv.org/abs/2308.10306v1)
* [Sound Source Localization is All about Cross-Modal Alignment](http://arxiv.org/abs/2309.10724v1)
* 去混响
  * [AdVerb: Visually Guided Audio Dereverberation](http://arxiv.org/abs/2308.12370v1)<br>:house:[project](https://gamma.umd.edu/researchdirections/speech/adverb)
* 唇语识别
  * [Lip Reading for Low-resource Languages by Learning and Combining General Speech Knowledge and Language-specific Knowledge](http://arxiv.org/abs/2308.09311v1)
* 音频-视频生成
  * [The Power of Sound (TPoS): Audio Reactive Video Generation with Stable Diffusion](http://arxiv.org/abs/2309.04509v1)<br>:star:[code](https://ku-vai.github.io/TPoS/)

<a name="54"/>

## 54.Gaze Estimation
* [DVGaze: Dual-View Gaze Estimation](http://arxiv.org/abs/2308.10310v1)<br>:star:[code](https://github.com/yihuacheng/DVGaze)

<a name="53"/>

## 53.Computed Imaging(计算成像，如光学、几何、光场成像等)
* [Deep Geometrized Cartoon Line Inbetweening](https://openaccess.thecvf.com/content/ICCV2023/papers/Siyao_Deep_Geometrized_Cartoon_Line_Inbetweening_ICCV_2023_paper.pdf)
* [Examining Autoexposure for Challenging Scenes](http://arxiv.org/abs/2309.04542v1)
* [Vanishing Point Estimation in Uncalibrated Images with Prior Gravity Direction](http://arxiv.org/abs/2308.10694v1)<br>:star:[code](https://github.com/cvg/VP-Estimation-with-Prior-Gravity)
* [Robust Frame-to-Frame Camera Rotation Estimation in Crowded Scenes](http://arxiv.org/abs/2309.08588v1)<br>:house:[project](https://fabiendelattre.com/robust-rotation-estimation)

<a name="52"/>

## 52.View Synthesis(视图合成)
* [LoLep: Single-View View Synthesis with Locally-Learned Planes and Self-Attention Occlusion Inference](http://arxiv.org/abs/2307.12217v1)
* [Learning Unified Decompositional and Compositional NeRF for Editable Novel View Synthesis](http://arxiv.org/abs/2308.02840v1)<br>:star:[code](https://w-ted.github.io/publications/udc-nerf)
* [Efficient View Synthesis with Neural Radiance Distribution Field](http://arxiv.org/abs/2308.11130v1)
* [NeO 360: Neural Fields for Sparse View Synthesis of Outdoor Scenes](http://arxiv.org/abs/2308.12967v1)<br>:star:[code](https://zubair-irshad.github.io/projects/neo360.html)
* [PARF: Primitive-Aware Radiance Fusion for Indoor Scene Novel View Synthesis](https://openaccess.thecvf.com/content/ICCV2023/papers/Ying_PARF_Primitive-Aware_Radiance_Fusion_for_Indoor_Scene_Novel_View_Synthesis_ICCV_2023_paper.pdf)
* [Urban Radiance Field Representation with Deformable Neural Mesh Primitives](http://arxiv.org/abs/2307.10776v1)<br>:house:[project](https://dnmp.github.io/)

<a name="51"/>

## 51.Visual Place Recognition
* [EigenPlaces: Training Viewpoint Robust Models for Visual Place Recognition](http://arxiv.org/abs/2308.10832v1)<br>:star:[code](https://github.com/gmberton/auto_VPR)<br>:star:[code](https://github.com/gmberton/EigenPlaces)

<a name="50"/>

## 50.Image Matching(图像匹配)
* [OccNet: Robust Image Matching Based on 3D Occupancy Estimation for Occluded Regions](https://openaccess.thecvf.com/content/ICCV2023/papers/Fan_Occ2Net_Robust_Image_Matching_Based_on_3D_Occupancy_Estimation_for_ICCV_2023_paper.pdf)
* [Scene-Aware Feature Matching](http://arxiv.org/abs/2308.09949v1)
* [Improving Transformer-based Image Matching by Cascaded Capturing Spatially Informative Keypoints](http://arxiv.org/abs/2303.02885)

<a name="49"/>

## 49.Image Fusion(图像融合)
* [MEFLUT: Unsupervised 1D Lookup Tables for Multi-exposure Image Fusion]未公开
* [Learned Image Reasoning Prior Penetrates Deep Unfolding Network for Panchromatic and Multi-Spectral Image Fusion](http://arxiv.org/abs/2308.16083v1)<br>:star:[code](https://manman1995.github.io/)

<a name="48"/>

## 48.Image Reconstruction
* [Pixel Adaptive Deep Unfolding Transformer for Hyperspectral Image Reconstruction](http://arxiv.org/abs/2308.10820v1)<br>:star:[code](https://github.com/MyuLi/PADUT)
* [RawHDR: High Dynamic Range Image Reconstruction from a Single Raw Image](http://arxiv.org/abs/2309.02020v1)
* [Learning Continuous Exposure Value Representations for Single-Image HDR Reconstruction](http://arxiv.org/abs/2309.03900v1)<br>:star:[code](https://skchen1993.github.io/CEVR_web/)

<a name="47"/>

## 47.Image-to-Image Translation
* [General Image-to-Image Translation with One-Shot Image Guidance](http://arxiv.org/abs/2307.14352v1)<br>:star:[code](https://github.com/CrystalNeuro/visual-concept-translator)
* [Scenimefy: Learning to Craft Anime Scene via Semi-Supervised Image-to-Image Translation](http://arxiv.org/abs/2308.12968v1)<br>:star:[code](https://github.com/Yuxinn-J/Scenimefy)<br>:star:[code](https://yuxinn-j.github.io/projects/Scenimefy.html)

<a name="46"/>

## 46.Edge Detection
* [Tiny and Efficient Model for the Edge Detection Generalization](http://arxiv.org/abs/2308.06468v1)<br>:star:[code](https://github.com/xavysp/TEED)

<a name="45"/>

## 45.Scene Graph Generation(场景图合成)
* [Environment-Invariant Curriculum Relation Learning for Fine-Grained Scene Graph Generation](http://arxiv.org/abs/2308.03282v1)
* [Compositional Feature Augmentation for Unbiased Scene Graph Generation](http://arxiv.org/abs/2308.06712v1)
* [Vision Relation Transformer for Unbiased Scene Graph Generation](http://arxiv.org/abs/2308.09472v1)
* [TextPSG: Panoptic Scene Graph Generation from Textual Descriptions](https://openaccess.thecvf.com/content/ICCV2023/papers/Zhao_TextPSG_Panoptic_Scene_Graph_Generation_from_Textual_Descriptions_ICCV_2023_paper.pdf)

<a name="44"/>

## 44.Rendering(渲染)
* [HollowNeRF: Pruning Hashgrid-Based NeRFs with Trainable Collision Mitigation](http://arxiv.org/abs/2308.10122v1)
* [DNA-Rendering: A Diverse Neural Actor Repository for High-Fidelity Human-centric Rendering](https://arxiv.org/abs/2307.10173)<br>:house:[project](https://dna-rendering.github.io/)
* [S3IM: Stochastic Structural SIMilarity and Its Unreasonable Effectiveness for Neural Fields](http://arxiv.org/abs/2308.07032v1)<br>:star:[code](https://github.com/Madaoer/S3IM)
* [TransHuman: A Transformer-based Human Representation for Generalizable Neural Human Rendering](http://arxiv.org/abs/2307.12291v1)<br>:star:[code](https://pansanity666.github.io/TransHuman/)
* [Tri-MipRF: Tri-Mip Representation for Efficient Anti-Aliasing Neural Radiance Fields](http://arxiv.org/abs/2307.11335v1)<br>:star:[code](https://wbhu.github.io/projects/Tri-MipRF)
* [Rendering Humans from Object-Occluded Monocular Videos](http://arxiv.org/abs/2308.04622v1)<br>:house:[project](https://cs.stanford.edu/~xtiange/projects/occnerf/)
* [ScatterNeRF: Seeing Through Fog with Physically-Based Inverse Neural Rendering](http://arxiv.org/abs/2305.02103)

<a name="43"/>

## 43.Neural Radiance Fields
* [Neural Radiance Field with LiDAR maps](https://openaccess.thecvf.com/content/ICCV2023/papers/Chang_Neural_Radiance_Field_with_LiDAR_maps_ICCV_2023_paper.pdf)
* [Dynamic Mesh-Aware Radiance Fields](http://arxiv.org/abs/2309.04581v1)
* [Locally Stylized Neural Radiance Fields](http://arxiv.org/abs/2309.10684v1)
* [Generalizable Neural Fields as Partially Observed Neural Processes](http://arxiv.org/abs/2309.06660v1)
* [DeformToon3D: Deformable 3D Toonification from Neural Radiance Fields](http://arxiv.org/abs/2309.04410v1)<br>:house:[project](https://www.mmlab-ntu.com/project/deformtoon3d/)<br>:star:[code](https://github.com/junzhezhang/DeformToon3D)
* [Robust e-NeRF: NeRF from Sparse & Noisy Events under Non-Uniform Motion](http://arxiv.org/abs/2309.08596v1)<br>:star:[code](https://wengflow.github.io/robust-e-nerf)
* [Pose-Free Neural Radiance Fields via Implicit Pose Regularization](http://arxiv.org/abs/2308.15049v1)
* [Canonical Factors for Hybrid Neural Fields](http://arxiv.org/abs/2308.15461v1)<br>:star:[code](https://brentyi.github.io/tilted/)
* [Multi-Modal Neural Radiance Field for Monocular Dense SLAM with a Light-Weight ToF Sensor](http://arxiv.org/abs/2308.14383v1)<br>:star:[code](https://zju3dv.github.io/tof_slam/)
* [Blending-NeRF: Text-Driven Localized Editing in Neural Radiance Fields](http://arxiv.org/abs/2308.11974v1)
* [Strata-NeRF : Neural Radiance Fields for Stratified Scenes](http://arxiv.org/abs/2308.10337v1)<br>:star:[code](https://ankitatiisc.github.io/Strata-NeRF/)
* [DReg-NeRF: Deep Registration for Neural Radiance Fields](http://arxiv.org/abs/2308.09386v1)<br>:star:[code](https://github.com/AIBluefisher/DReg-NeRF)
* [Seal-3D: Interactive Pixel-Level Editing for Neural Radiance Fields](http://arxiv.org/abs/2307.15131v1)<br>:house:[project](https://windingwind.github.io/seal-3d/)<br>:star:[code](https://github.com/windingwind/seal-3d/)
* [WaveNeRF: Wavelet-based Generalizable Neural Radiance Fields](http://arxiv.org/abs/2308.04826v1)
* 版权保护
  * [CopyRNeRF: Protecting the CopyRight of Neural Radiance Fields](http://arxiv.org/abs/2307.11526v1)

<a name="42"/>

## 42.Dataset/Benchmark
* 数据集
  * [Building3D: An Urban-Scale Dataset and Benchmarks for Learning Roof Structures from Point Clouds](https://arxiv.org/pdf/2307.11914.pdf)<br>:sunflower:[dataset](https://building3d.ucalgary.ca/#)<br>:thumbsup:[ICCV2023 首个城市级别的基于航空点云的房屋建模数据集 Building3D](https://mp.weixin.qq.com/s/gKFByZ8ud2aNlG7C7t2-2Q)
  * [MOSE: A New Dataset for Video Object Segmentation in Complex Scenes](http://arxiv.org/abs/2302.01872)
  * [Audio-Visual Deception Detection: DOLOS Dataset and Parameter-Efficient Crossmodal Learning](http://arxiv.org/abs/2303.12745)
  * [MatrixCity: A Large-scale City Dataset for City-scale Neural Rendering and Beyond](https://openaccess.thecvf.com/content/ICCV2023/papers/Li_MatrixCity_A_Large-scale_City_Dataset_for_City-scale_Neural_Rendering_and_ICCV_2023_paper.pdf)
  * [LaRS: A Diverse Panoptic Maritime Obstacle Detection Dataset and Benchmark](http://arxiv.org/abs/2308.09618v1)<br>:star:[code](https://lojzezust.github.io/lars-dataset)
  * [EgoObjects: A Large-Scale Egocentric Dataset for Fine-Grained Object Understanding](http://arxiv.org/abs/2309.08816v1)<br>:star:[code](https://github.com/facebookresearch/EgoObjects)
  * [Towards Universal Image Embeddings: A Large-Scale Dataset and Challenge for Generic Image Representations](http://arxiv.org/abs/2309.01858v1)<br>:house:[project](https://cmp.felk.cvut.cz/univ_emb/)
  * [ScanNet++: A High-Fidelity Dataset of 3D Indoor Scenes](http://arxiv.org/abs/2308.11417v1)<br>:house:[project](https://youtu.be/E6P9e2r6M8I)<br>:star:[code](https://cy94.github.io/scannetpp/)
  * [Learning Optical Flow from Event Camera with Rendered Dataset](https://arxiv.org/abs/2303.11011)
  * [Efficient neural supersampling on a novel gaming dataset](http://arxiv.org/abs/2308.01483v1)
  * [AIDE: A Vision-Driven Multi-View, Multi-Modal, Multi-Tasking Dataset for Assistive Driving Perception](http://arxiv.org/abs/2307.13933v1)<br>:star:[code](https://github.com/ydk122024/AIDE)
  * [360VOT: A New Benchmark Dataset for Omnidirectional Visual Object Tracking](http://arxiv.org/abs/2307.14630v1)<br>:house:[project](https://360vot.hkustvgd.com)<br>:star:[code](https://github.com/HuajianUP/360VOT)全向视觉目标跟踪
  * [Harvard Glaucoma Detection and Progression: A Multimodal Multitask Dataset and Generalization-Reinforced Semi-Supervised Learning](http://arxiv.org/abs/2308.13411v1)<br>:house:[project](https://ophai.hms.harvard.edu/datasets/harvard-gdp1000)
* 基准
  * [From Sky to the Ground: A Large-scale Benchmark and Simple Baseline Towards Real Rain Removal](http://arxiv.org/abs/2308.03867v1)
  * [A Benchmark for Chinese-English Scene Text Image Super-Resolution](http://arxiv.org/abs/2308.03262)
  * [Towards Real-World Burst Image Super-Resolution: Benchmark and Method](http://arxiv.org/abs/2309.04803v1)<br>:star:[code](https://github.com/yjsunnn/FBANet)
  * [COCO-O: A Benchmark for Object Detectors under Natural Distribution Shifts](http://arxiv.org/abs/2307.12730v1)<br>:star:[code](https://github.com/alibaba/easyrobust/tree/main/benchmarks/coco_o)
  * [Dancing in the Dark: A Benchmark towards General Low-light Video Enhancement](https://openaccess.thecvf.com/content/ICCV2023/papers/Fu_Dancing_in_the_Dark_A_Benchmark_towards_General_Low-light_Video_ICCV_2023_paper.pdf)
* 方法
  * [Prototype-based Dataset Comparison](http://arxiv.org/abs/2309.02401v1)<br>:star:[code](https://github.com/Nanne/ProtoSim)

<a name="41"/>

## 41.Vision Transformers
* [Robustifying Token Attention for Vision Transformers](http://arxiv.org/abs/2303.11126)
* [FLatten Transformer: Vision Transformer using Focused Linear Attention](http://arxiv.org/abs/2308.00442)
* [M2T: Masking Transformers Twice for Faster Decoding](https://openaccess.thecvf.com/content/ICCV2023/papers/Mentzer_M2T_Masking_Transformers_Twice_for_Faster_Decoding_ICCV_2023_paper.pdf)
* [FDViT: Improve the Hierarchical Architecture of Vision Transformer](https://openaccess.thecvf.com/content/ICCV2023/papers/Xu_FDViT_Improve_the_Hierarchical_Architecture_of_Vision_Transformer_ICCV_2023_paper.pdf)
* [Jumping through Local Minima: Quantization in the Loss Landscape of Vision Transformers](http://arxiv.org/abs/2308.10814)
* [Rethinking Vision Transformers for MobileNet Size and Speed](http://arxiv.org/abs/2212.08059)
* [Structure Invariant Transformation for better Adversarial Transferability](http://arxiv.org/abs/2309.14700v1)<br>:star:[code](https://github.com/xiaosen-wang/SIT)
* [SG-Former: Self-guided Transformer with Evolving Token Reallocation](http://arxiv.org/abs/2308.12216v1)<br>:star:[code](https://github.com/OliverRensu/SG-Former)
* [Pre-training Vision Transformers with Very Limited Synthesized Images](http://arxiv.org/abs/2307.14710v1)
* [SMMix: Self-Motivated Image Mixing for Vision Transformers](https://arxiv.org/abs/2212.12977)
* [Revisiting Vision Transformer from the View of Path Ensemble](http://arxiv.org/abs/2308.06548v1)
* [SwinLSTM:Improving Spatiotemporal Prediction Accuracy using Swin Transformer and LSTM](http://arxiv.org/abs/2308.09891v1)<br>:star:[code](https://github.com/SongTang-x/SwinLSTM)
* [Eventful Transformers: Leveraging Temporal Redundancy in Vision Transformers](http://arxiv.org/abs/2308.13494v1)
* [Contrastive Feature Masking Open-Vocabulary Vision Transformer](http://arxiv.org/abs/2309.00775v1)
* [MMST-ViT: Climate Change-aware Crop Yield Prediction via Multi-Modal Spatial-Temporal Vision Transformer](http://arxiv.org/abs/2309.09067v1)

<a name="40"/>

## 40.Anomaly Detection(异常检测)
* [Unilaterally Aggregated Contrastive Learning with Hierarchical Augmentation for Anomaly Detection](http://arxiv.org/abs/2308.10155v1)
* 图像异常检测
  * [Focus the Discrepancy: Intra- and Inter-Correlation Learning for Image Anomaly Detection](http://arxiv.org/abs/2308.02983v1)<br>:star:[code](https://github.com/xcyao00/FOD)
* OOD
  * [CLIPN for Zero-Shot OOD Detection: Teaching CLIP to Say No](http://arxiv.org/abs/2308.12213v1)<br>:star:[code](https://github.com/xmed-lab/CLIPN)
  * [Meta OOD Learning for Continuously Adaptive OOD Detection](http://arxiv.org/abs/2309.11705v1)
  * [Nearest Neighbor Guidance for Out-of-Distribution Detection](http://arxiv.org/abs/2309.14888v1)<br>:star:[code](https://github.com/roomo7time/nnguide)

<a name="39"/>

## 39.Keypoint Detection(关键点检测)
* [Neural Interactive Keypoint Detection](http://arxiv.org/abs/2308.10174v1)<br>:star:[code](https://github.com/IDEA-Research/Click-Pose)
* [3D Implicit Transporter for Temporally Consistent Keypoint Discovery](http://arxiv.org/abs/2309.05098v1)<br>:star:[code](https://github.com/zhongcl-thu/3D-Implicit-Transporter)

<a name="38"/>

## 38.Vision-Language(视觉语言)
* [Too Large; Data Reduction for Vision-Language Pre-Training](http://arxiv.org/abs/2305.20087)
* [Equivariant Similarity for Vision-Language Foundation Models](http://arxiv.org/abs/2303.14465)
* [Going Beyond Nouns With Vision & Language Models Using Synthetic Data](http://arxiv.org/abs/2303.17590)
* [SINC: Self-Supervised In-Context Learning for Vision-Language Tasks](http://arxiv.org/abs/2307.07742)
* [Unified Visual Relationship Detection with Vision and Language Models](http://arxiv.org/abs/2303.08998)
* [ProbVLM: Probabilistic Adapter for Frozen Vison-Language Models](http://arxiv.org/abs/2307.00398)
* [Distilling Large Vision-Language Model with Out-of-Distribution Generalizability](http://arxiv.org/abs/2307.03135)
* [Distribution-Aware Prompt Tuning for Vision-Language Models](http://arxiv.org/abs/2309.03406v1)<br>:star:[code](https://github.com/mlvlab/DAPT)
* [LoGoPrompt: Synthetic Text Images Can Be Good Visual Prompts for Vision-Language Models](http://arxiv.org/abs/2309.01155v1)<br>:star:[code](https://chengshiest.github.io/logo)
* [CLIPTrans: Transferring Visual Knowledge with Pre-trained Models for Multimodal Machine Translation](http://arxiv.org/abs/2308.15226v1)
* [GrowCLIP: Data-aware Automatic Model Growing for Large-scale Contrastive Language-Image Pre-training](http://arxiv.org/abs/2308.11331v1)
* [RLIPv2: Fast Scaling of Relational Language-Image Pre-training](http://arxiv.org/abs/2308.09351v1)<br>:star:[code](https://github.com/JacobYuan7/RLIPv2)
* [Regularized Mask Tuning: Uncovering Hidden Knowledge in Pre-trained Vision-Language Models](http://arxiv.org/abs/2307.15049v1)<br>:star:[code](https://wuw2019.github.io/RMT/)
* [Why Is Prompt Tuning for Vision-Language Models Robust to Noisy Labels?](http://arxiv.org/abs/2307.11978v1)<br>:star:[code](https://github.com/CEWu/PTNL)
* [Set-level Guidance Attack: Boosting Adversarial Transferability of Vision-Language Pre-training Models](http://arxiv.org/abs/2307.14061v1)<br>:thumbsup:[ICCV 2023 Oral | 南科大VIP Lab | 针对VLP模型的集合级引导攻击](https://mp.weixin.qq.com/s/bE97oBoa4nH1c5XuOz4WWA)
* [CTP: Towards Vision-Language Continual Pretraining via Compatible Momentum Contrast and Topology Preservation](http://arxiv.org/abs/2308.07146v1)<br>:star:[code](https://github.com/KevinLight831/CTP)
* [VL-PET: Vision-and-Language Parameter-Efficient Tuning via Granularity Control](http://arxiv.org/abs/2308.09804v1)<br>:star:[code](https://github.com/HenryHZY/VL-PET)
* [Knowledge-Aware Prompt Tuning for Generalizable Vision-Language Models](http://arxiv.org/abs/2308.11186v1)
* [VLSlice: Interactive Vision-and-Language Slice Discovery](http://arxiv.org/abs/2309.06703v1)<br>:house:[project](https://ericslyman.com/vlslice/)
* 视觉表示学习
  * [Hallucination Improves the Performance of Unsupervised Visual Representation Learning](http://arxiv.org/abs/2307.12168v1)
  * [ViLLA: Fine-Grained Vision-Language Representation Learning from Real-World Data](http://arxiv.org/abs/2308.11194v1)
* VLN
  * [Learning Vision-and-Language Navigation from YouTube Videos](http://arxiv.org/abs/2307.11984v1)<br>:star:[code](https://github.com/JeremyLinky/YouTube-VLN)
  * [GridMM: Grid Memory Map for Vision-and-Language Navigation](http://arxiv.org/abs/2307.12907v1)
  * [Scaling Data Generation in Vision-and-Language Navigation](http://arxiv.org/abs/2307.15644v1)
  * [Bird's-Eye-View Scene Graph for Vision-Language Navigation](http://arxiv.org/abs/2308.04758v1)<br>:star:[code](https://github.com/DefaultRui/BEV-Scene-Graph)
  * [AerialVLN: Vision-and-Language Navigation for UAVs](http://arxiv.org/abs/2308.06735v1)<br>:star:[code](https://github.com/AirVLN/AirVLN)
  * [DREAMWALKER: Mental Planning for Continuous Vision-Language Navigation](http://arxiv.org/abs/2308.07498v1)<br>:star:[code](https://github.com/hanqingwangai/Dreamwalker)
  * [VLN-PETL: Parameter-Efficient Transfer Learning for Vision-and-Language Navigation](http://arxiv.org/abs/2308.10172v1)
  * [March in Chat: Interactive Prompting for Remote Embodied Referring Expression](http://arxiv.org/abs/2308.10141v1)
  * [Grounded Entity-Landmark Adaptive Pre-training for Vision-and-Language Navigation](http://arxiv.org/abs/2308.12587v1)
  * [BEVBert: Multimodal Map Pre-training for Language-guided Navigation](https://arxiv.org/pdf/2212.04385.pdf)<br>:star:[code](https://github.com/MarSaKi/VLN-BEVBert)

<a name="37"/>

## 37.Object Pose Estimation(物体姿势估计)
* [Nonrigid Object Contact Estimation With Regional Unwrapping Transformer](http://arxiv.org/abs/2308.14074v1)
* 6D
  * [Deep Fusion Transformer Network with Weighted Vector-Wise Keypoints Voting for Robust 6D Object Pose Estimation](http://arxiv.org/abs/2308.05438v1)
  * [Pseudo Flow Consistency for Self-Supervised 6D Object Pose Estimation](http://arxiv.org/abs/2308.10016v1)
  * [VI-Net: Boosting Category-level 6D Object Pose Estimation via Learning Decoupled Rotations on the Spherical Representations](http://arxiv.org/abs/2308.09916v1)<br>:star:[code](https://github.com/JiehongLin/VI-Net)
  * [Learning Symmetry-Aware Geometry Correspondences for 6D Object Pose Estimation](https://openaccess.thecvf.com/content/ICCV2023/papers/Zhao_Learning_Symmetry-Aware_Geometry_Correspondences_for_6D_Object_Pose_Estimation_ICCV_2023_paper.pdf)
  * [3D Neural Embedding Likelihood: Probabilistic Inverse Graphics for Robust 6D Pose Estimation](http://arxiv.org/abs/2302.03744)
* 物体计数
  * [STEERER: Resolving Scale Variations for Counting and Localization via Selective Inheritance Learning](http://arxiv.org/abs/2308.10468v1)<br>:star:[code](https://github.com/taohan10200/STEERER)
  * [Interactive Class-Agnostic Object Counting](http://arxiv.org/abs/2309.05277)

<a name="36"/>

## 36.Vision Question Answering(视觉问答)
* [Encyclopedic VQA: Visual Questions About Detailed Properties of Fine-Grained Categories](http://arxiv.org/abs/2306.09224)
* [PromptCap: Prompt-Guided Image Captioning for VQA with GPT-3](https://openaccess.thecvf.com/content/ICCV2023/papers/Hu_PromptCap_Prompt-Guided_Image_Captioning_for_VQA_with_GPT-3_ICCV_2023_paper.pdf)
* [Decouple Before Interact: Multi-Modal Prompt Learning for Continual Visual Question Answering](https://openaccess.thecvf.com/content/ICCV2023/papers/Qian_Decouple_Before_Interact_Multi-Modal_Prompt_Learning_for_Continual_Visual_Question_ICCV_2023_paper.pdf)
* Video-QA
  * [Discovering Spatio-Temporal Rationales for Video Question Answering](http://arxiv.org/abs/2307.12058v1)<br>:star:[code](https://github.com/yl3800/TranSTR)
  * [Tem-adapter: Adapting Image-Text Pretraining for Video Question Answer](http://arxiv.org/abs/2308.08414v1)
  * [Open-vocabulary Video Question Answering: A New Benchmark for Evaluating the Generalizability of Video Question Answering Models](http://arxiv.org/abs/2308.09363v1)<br>:star:[code](https://github.com/mlvlab/OVQA)

<a name="35"/>

## 35.Human Motion Prediction(人体运动预测)
* [Auxiliary Tasks Benefit 3D Skeleton-based Human Motion Prediction](http://arxiv.org/abs/2308.08942v1)<br>:star:[code](https://github.com/MediaBrain-SJTU/AuxFormer)
* [Forecast-MAE: Self-supervised Pre-training for Motion Forecasting with Masked Autoencoders](http://arxiv.org/abs/2308.09882v1)<br>:star:[code](https://github.com/jchengai/forecast-mae)
* [Priority-Centric Human Motion Generation in Discrete Latent Space](http://arxiv.org/abs/2308.14480v1)


<a name="34"/>

## 34.Action Detection
* [Memory-and-Anticipation Transformer for Online Action Understanding](http://arxiv.org/abs/2308.07893v1)<br>:star:[code](https://github.com/Echo0125/Memory-and-Anticipation-Transformer)
* [Masked Motion Predictors are Strong 3D Action Representation Learners](http://arxiv.org/abs/2308.07092v1)<br>:star:[code](https://github.com/maoyunyao/MAMP)
* [E2E-LOAD: End-to-End Long-form Online Action Detection](https://openaccess.thecvf.com/content/ICCV2023/papers/Cao_E2E-LOAD_End-to-End_Long-form_Online_Action_Detection_ICCV_2023_paper.pdf)
* [Cross-Modal Learning with 3D Deformable Attention for Action Recognition](http://arxiv.org/abs/2212.05638)
* [DiffTAD: Temporal Action Detection with Proposal Denoising Diffusion](http://arxiv.org/abs/2303.14863)
* 基于骨架的动作识别
  * [LAC -- Latent Action Composition for Skeleton-based Action Segmentation](http://arxiv.org/abs/2308.14500v1)
  * [SkeleTR: Towrads Skeleton-based Action Recognition in the Wild](http://arxiv.org/abs/2309.11445v1)
  * [Hard No-Box Adversarial Attack on Skeleton-Based Human Action Recognition with Skeleton-Motion-Informed Gradient](http://arxiv.org/abs/2308.05681)
* 开集动作识别
  * [SOAR: Scene-debiasing Open-set Action Recognition](http://arxiv.org/abs/2309.01265v1)<br>:star:[code](https://github.com/yhZhai/SOAR)
* 小样本动作识别
  * [Boosting Few-shot Action Recognition with Graph-guided Hybrid Matching](http://arxiv.org/abs/2308.09346v1)<br>:star:[code](https://github.com/jiazheng-xing/GgHM)
* 时序动作定位
  * [DDG-Net: Discriminability-Driven Graph Network for Weakly-supervised Temporal Action Localization](http://arxiv.org/abs/2307.16415v1)<br>:star:[code](https://github.com/XiaojunTang22/ICCV2023-DDGNet)
  * [Self-Feedback DETR for Temporal Action Detection](http://arxiv.org/abs/2308.10570v1)
  * [Revisiting Foreground and Background Separation in Weakly-supervised Temporal Action Localization](https://openaccess.thecvf.com/content/ICCV2023/papers/Liu_Revisiting_Foreground_and_Background_Separation_in_Weakly-supervised_Temporal_Action_Localization_ICCV_2023_paper.pdf)
* 弱监督动作定位
  * [Weakly-Supervised Action Localization by Hierarchically-structured Latent Attention Modeling](http://arxiv.org/abs/2308.09946v1)
* 小样本动作定位
  * [Few-Shot Common Action Localization via Cross-Attentional Fusion of Context and Temporal Dynamics](https://openaccess.thecvf.com/content/ICCV2023/papers/Lee_Few-Shot_Common_Action_Localization_via_Cross-Attentional_Fusion_of_Context_and_ICCV_2023_paper.pdf)

<a name="33"/>

## 33.Video
* [Neural Video Depth Stabilizer](http://arxiv.org/abs/2307.08695)
* [MMVP: Motion-Matrix-based Video Prediction](http://arxiv.org/abs/2308.16154v1)<br>:star:[code](https://github.com/Kay1794/MMVP-motion-matrix-based-video-prediction)
* [D3G: Exploring Gaussian Prior for Temporal Sentence Grounding with Glance Annotation](http://arxiv.org/abs/2308.04197v1)<br>:star:[code](https://github.com/solicucu/D3G)
* [LAN-HDR: Luminance-based Alignment Network for High Dynamic Range Video Reconstruction](http://arxiv.org/abs/2308.11116v1)
* [TALL: Thumbnail Layout for Deepfake Video Detection](http://arxiv.org/abs/2307.07494)
* [Spatio-temporal Prompting Network for Robust Video Feature Extraction](https://openaccess.thecvf.com/content/ICCV2023/papers/Sun_Spatio-temporal_Prompting_Network_for_Robust_Video_Feature_Extraction_ICCV_2023_paper.pdf)
* 视频理解
  * [Long-range Multimodal Pretraining for Movie Understanding](http://arxiv.org/abs/2308.09775v1)
  * [RefEgo: Referring Expression Comprehension Dataset from First-Person Perception of Ego4D](http://arxiv.org/abs/2308.12035v1)
* 视频合成
  * [StyleInV: A Temporal Style Modulated Inversion Network for Unconditional Video Generation](http://arxiv.org/abs/2308.16909v1)<br>:house:[project](https://www.mmlab-ntu.com/project/styleinv/index.html)<br>:star:[code](https://github.com/johannwyh/StyleInV)
* 视频稳定
  * [Fast Full-frame Video Stabilization with Iterative Optimization](http://arxiv.org/abs/2307.12774v1)
* Video Grounding(视频定位)
  * [G2L: Semantically Aligned and Uniform Video Grounding via Geodesic and Game Theory](http://arxiv.org/abs/2307.14277v1)
  * [UniVTG: Towards Unified Video-Language Temporal Grounding](http://arxiv.org/abs/2307.16715v1)<br>:star:[code](https://github.com/showlab/UniVTG)
  * [Knowing Where to Focus: Event-aware Transformer for Video Grounding](http://arxiv.org/abs/2308.06947v1)<br>:star:[code](https://github.com/jinhyunj/EaTR)
* 视频分割
  * [XMem++: Production-level Video Segmentation From Few Annotated Frames](http://arxiv.org/abs/2307.15958v1)
  * [Rethinking Amodal Video Segmentation from Learning Supervised Signals with Object-centric Representation](http://arxiv.org/abs/2309.13248v1)<br>:star:[code](https://github.com/kfan21/EoRaS)
  * [GraphEcho: Graph-Driven Unsupervised Domain Adaptation for Echocardiogram Video Segmentation](http://arxiv.org/abs/2309.11145v1)<br>:star:[code](https://github.com/xmed-lab/GraphEcho)
  * [MeViS: A Large-scale Benchmark for Video Segmentation with Motion Expressions](http://arxiv.org/abs/2308.08544v1)<br>:star:[code](https://henghuiding.github.io/MeViS)<br>:star:[code](https://henghuiding.github.io/MeViS/)
  * [MEGA: Multimodal Alignment Aggregation and Distillation For Cinematic Video Segmentation](http://arxiv.org/abs/2308.11185v1)
  * [Tracking Anything with Decoupled Video Segmentation](http://arxiv.org/abs/2309.03903v1)<br>:star:[code](https://hkchengrex.github.io/Tracking-Anything-with-DEVA)
  * [The Making and Breaking of Camouflage](http://arxiv.org/abs/2309.03899v1)
* 视频对应
  * [Learning Fine-Grained Features for Pixel-wise Video Correspondences](http://arxiv.org/abs/2308.03040v1)<br>:star:[code](https://github.com/qianduoduolr/FGVC)
* 视频感知
  * [ResQ: Residual Quantization for Video Perception](http://arxiv.org/abs/2308.09511v1)
* 视频识别
  * [Audio-Visual Glance Network for Efficient Video Recognition](http://arxiv.org/abs/2308.09322v1)
  * [Learning from Semantic Alignment between Unpaired Multiviews for Egocentric Video Recognition](http://arxiv.org/abs/2308.11489v1)<br>:star:[code](https://github.com/wqtwjt1996/SUM-L)
* 视频修补
  * [ProPainter: Improving Propagation and Transformer for Video Inpainting](http://arxiv.org/abs/2309.03897v1)<br>:star:[code](https://github.com/sczhou/ProPainter)
* 视频表示学习
  * [MGMAE: Motion Guided Masking for Video Masked Autoencoding](http://arxiv.org/abs/2308.10794v1)
* VAD
  * [TeD-SPAD: Temporal Distinctiveness for Self-supervised Privacy-preservation for video Anomaly Detection](http://arxiv.org/abs/2308.11072v1)<br>:star:[code](https://joefioresi718.github.io/TeD-SPAD_webpage/)
* Video Localization
  * [UnLoc: A Unified Framework for Video Localization Tasks](http://arxiv.org/abs/2308.11062v1)<br>:star:[code](https://github.com/google-research/scenic)
  * [Video OWL-ViT: Temporally-consistent open-world localization in video](http://arxiv.org/abs/2308.11093v1)
* 视频预测
  * [MMVP: Motion-Matrix-Based Video Prediction](http://arxiv.org/abs/2308.16154)
* 视频玻璃分割
  * [Multi-view Spectral Polarization Propagation for Video Glass Segmentation](https://openaccess.thecvf.com/content/ICCV2023/papers/Qiao_Multi-view_Spectral_Polarization_Propagation_for_Video_Glass_Segmentation_ICCV_2023_paper.pdf)

<a name="32"/>

## 32.Sign Language Recognition(手语)
* [Human Part-wise 3D Motion Context Learning for Sign Language Recognition](http://arxiv.org/abs/2308.09305v1)
* [Improving Continuous Sign Language Recognition with Cross-Lingual Signs](http://arxiv.org/abs/2308.10809v1)
* [C2ST: Cross-Modal Contextualized Sequence Transduction for Continuous Sign Language Recognition](https://openaccess.thecvf.com/content/ICCV2023/papers/Zhang_C2ST_Cross-Modal_Contextualized_Sequence_Transduction_for_Continuous_Sign_Language_Recognition_ICCV_2023_paper.pdf)
* 手语翻译
  * [Sign Language Translation with Iterative Prototype](http://arxiv.org/abs/2308.12191v1)
  * [Gloss-free Sign Language Translation: Improving from Visual-Language Pretraining](http://arxiv.org/abs/2307.14768v1)<br>:star:[code](https://github.com/zhoubenjia/GFSLT-VLP)

<a name="31"/>

## 31.Human-Object Interaction(人物交互)
* [Persistent-Transient Duality: A Multi-mechanism Approach for Modeling Human-Object Interaction](http://arxiv.org/abs/2307.12729v1)
* [Re-mine, Learn and Reason: Exploring the Cross-modal Semantic Correlations for Language-guided HOI detection](http://arxiv.org/abs/2307.13529v1)
* [Agglomerative Transformer for Human-Object Interaction Detection](http://arxiv.org/abs/2308.08370v1)
* [InterDiff: Generating 3D Human-Object Interactions with Physics-Informed Diffusion](http://arxiv.org/abs/2308.16905v1)<br>:star:[code](https://sirui-xu.github.io/InterDiff/)
* 手物交互
  * [EgoPCA: A New Framework for Egocentric Hand-Object Interaction Understanding](http://arxiv.org/abs/2309.02423v1)<br>:house:[project](https://mvig-rhos.com/ego_pca)
  * [Diffusion-Guided Reconstruction of Everyday Hand-Object Interaction Clips](http://arxiv.org/abs/2309.05663v1)<br>:star:[code](https://judyye.github.io/diffhoi-www/)
  * [AffordPose: A Large-scale Dataset of Hand-Object Interactions with Affordance-driven Hand Pose](http://arxiv.org/abs/2309.08942v1)<br>:star:[code](https://github.com/GentlesJan/AffordPose)

<a name="30"/>

## 30.SLAM/Augmented Reality/Virtual Reality/Robotics(增强/虚拟现实/机器人)
* 虚拟人物生成
  * [MODA: Mapping-Once Audio-driven Portrait Animation with Dual Attentions](http://arxiv.org/abs/2307.10008v1)
  * [NSF: Neural Surface Fields for Human Modeling from Monocular Depth](http://arxiv.org/abs/2308.14847v1)<br>:house:[project](https://yuxuan-xue.com/nsf)
* 机器人
  * [Leveraging SE(3) Equivariance for Learning 3D Geometric Shape Assembly](http://arxiv.org/abs/2309.06810v1)<br>:star:[code](https://github.com/crtie/Leveraging-SE-3-Equivariance-for-Learning-3D-Geometric-Shape-Assembly)<br>:star:[code](https://crtie.github.io/SE-3-part-assembly/)
* AR/VR
  * [HMD-NeMo: Online 3D Avatar Motion Generation From Sparse Observations](http://arxiv.org/abs/2308.11261v1)
* SLAM
  * [GO-SLAM: Global Optimization for Consistent 3D Instant Reconstruction](http://arxiv.org/abs/2309.02436v1)<br>:star:[code](https://youmi-zym.github.io/projects/GO-SLAM/)<br>:star:[code](https://github.com/youmi-zym/GO-SLAM)

<a name="29"/>

## 29.Autonomous vehicles(自动驾驶)
* [HM-ViT: Hetero-Modal Vehicle-to-Vehicle Cooperative Perception with Vision Transformer](https://openaccess.thecvf.com/content/ICCV2023/papers/Xiang_HM-ViT_Hetero-Modal_Vehicle-to-Vehicle_Cooperative_Perception_with_Vision_Transformer_ICCV_2023_paper.pdf)(https://github.com/XHwind/HM-ViT)
* 自动驾驶
  * [Improving Online Lane Graph Extraction by Object-Lane Clustering](http://arxiv.org/abs/2307.10947v1)
  * [Domain Generalization of 3D Semantic Segmentation in Autonomous Driving](http://arxiv.org/abs/2212.04245)
  * [Unsupervised 3D Perception with 2D Vision-Language Distillation for Autonomous Driving](http://arxiv.org/abs/2309.14491v1)
  * [DriveAdapter: Breaking the Coupling Barrier of Perception and Planning in End-to-End Autonomous Driving](http://arxiv.org/abs/2308.00398v1)<br>:star:[code](https://github.com/OpenDriveLab/DriveAdapter)
  * [Video Task Decathlon: Unifying Image and Video Tasks in Autonomous Driving](http://arxiv.org/abs/2309.04422v1)<br>:house:[project](https://www.vis.xyz/pub/vtd)
  * [Does Physical Adversarial Example Really Matter to Autonomous Driving? Towards System-Level Effect of Adversarial Object Evasion Attack](http://arxiv.org/abs/2308.11894v1)
  * [Towards Viewpoint Robustness in Bird's Eye View Segmentation](http://arxiv.org/abs/2309.05192v1)<br>:star:[code](https://nvlabs.github.io/viewpoint-robustness)
  * [GameFormer: Game-theoretic Modeling and Learning of Transformer-based Interactive Prediction and Planning for Autonomous Driving](http://arxiv.org/abs/2303.05760)
* 轨迹预测
  * [ADAPT: Efficient Multi-Agent Trajectory Prediction with Adaptation](http://arxiv.org/abs/2307.14187v1)<br>:star:[code](https://KUIS-AI.github.io/adapt)
  * [Fast Inference and Update of Probabilistic Density Estimation on Trajectory Prediction](http://arxiv.org/abs/2308.08824v1)<br>:star:[code](https://github.com/meaten/FlowChain-ICCV2023)
* 车道线
  * [PETRv2: A Unified Framework for 3D Perception from Multi-Camera Images](https://arxiv.org/pdf/2206.01256.pdf)<br>:star:[code](https://github.com/megvii-research/PETR.git)
  * [ADNet: Lane Shape Prediction via Anchor Decomposition](http://arxiv.org/abs/2308.10481v1)<br>:star:[code](https://github.com/)
  * [LATR: 3D Lane Detection from Monocular Images with Transformer](http://arxiv.org/abs/2308.04583v1)<br>:star:[code](https://github.com/JMoonr/LATR)
  * [Recursive Video Lane Detection](http://arxiv.org/abs/2308.11106v1)<br>:star:[code](https://github.com/dongkwonjin/RVLD)
  * [Sparse Point Guided 3D Lane Detection](https://openaccess.thecvf.com/content/ICCV2023/papers/Yao_Sparse_Point_Guided_3D_Lane_Detection_ICCV_2023_paper.pdf)

<a name="28"/>

## 28.Style Transfer(风格迁移)
* [AesPA-Net: Aesthetic Pattern-Aware Style Transfer Networks](http://arxiv.org/abs/2307.09724v1)<br>:star:[code](https://github.com/Kibeom-Hong/AesPA-Net)
* [Creative Birds: Self-Supervised Single-View 3D Style Transfer](http://arxiv.org/abs/2307.14127v1)<br>:star:[code](https://github.com/wrk226/2D-to-3D-Evolution-Transfer)
* [StyleDiffusion: Controllable Disentangled Style Transfer via Diffusion Models](http://arxiv.org/abs/2308.07863v1)
* [Zero-Shot Contrastive Loss for Text-Guided Diffusion Image Style Transfer](http://arxiv.org/abs/2303.08622)

<a name="27"/>

## 27.Self/Semi-Supervised Learning
* 全监督
  * [OPERA: Omni-Supervised Representation Learning with Hierarchical Supervisions](http://arxiv.org/abs/2210.05557)
* 自监督学习
  * [Stable and Causal Inference for Discriminative Self-supervised Deep Visual Representations](http://arxiv.org/abs/2308.08321v1)
  * [L-DAWA: Layer-wise Divergence Aware Weight Aggregation in Federated Self-Supervised Visual Representation Learning](https://openaccess.thecvf.com/content/ICCV2023/papers/Rehman_L-DAWA_Layer-wise_Divergence_Aware_Weight_Aggregation_in_Federated_Self-Supervised_Visual_ICCV_2023_paper.pdf)
  * [Randomized Quantization: A Generic Augmentation for Data Agnostic Self-supervised Learning](http://arxiv.org/abs/2212.08663)
  * [Semantics Meets Temporal Correspondence: Self-supervised Object-centric Learning in Videos](http://arxiv.org/abs/2308.09951v1)<br>:star:[code](https://github.com/shvdiwnkozbw/SMTC)
  * [Self-supervised Learning to Bring Dual Reversed Rolling Shutter Images Alive](http://arxiv.org/abs/2305.19862)
  * [Multi-Label Self-Supervised Learning with Scene Images](http://arxiv.org/abs/2308.03286)
* 半监督学习
  * [Shrinking Class Space for Enhanced Certainty in Semi-Supervised Learning](http://arxiv.org/abs/2308.06777v1)<br>:star:[code](https://github.com/LiheYoung/ShrinkMatch)
  * [Enhancing Sample Utilization through Sample Adaptive Augmentation in Semi-Supervised Learning](http://arxiv.org/abs/2309.03598v1)
  * [A Soft Nearest-Neighbor Framework for Continual Semi-Supervised Learning](http://arxiv.org/abs/2212.05102)
  * [Towards Semi-supervised Learning with Non-random Missing Labels](http://arxiv.org/abs/2308.08872v1)<br>:star:[code](https://github.com/NJUyued/PRG4SSL-MNAR)
  * [Diverse Cotraining Makes Strong Semi-Supervised Segmentor](http://arxiv.org/abs/2308.09281v1)<br>:star:[code](https://github.com/williamium3000/diverse-cotraining)
  * [Semi-Supervised Learning via Weight-aware Distillation under Class Distribution Mismatch](http://arxiv.org/abs/2308.11874v1)<br>:star:[code](https://github.com/RUC-DWBI-ML/research/tree/main/WAD-master)
  * [IOMatch: Simplifying Open-Set Semi-Supervised Learning with Joint Inliers and Outliers Utilization](http://arxiv.org/abs/2308.13168v1)<br>:star:[code](https://github.com/nukezil/IOMatch)

<a name="26"/>

## 26.Machine Learning(机器学习)
* Adversarial Learning(对抗学习) 
  * [ACTIVE: Towards Highly Transferable 3D Physical Camouflage for Universal and Robust Vehicle Evasion](http://arxiv.org/abs/2308.07009v1)<br>:star:[code](https://islab-ai.github.io/active-iccv2023/)
  * [Towards Building More Robust Models with Frequency Bias](http://arxiv.org/abs/2307.09763v1)
  * [Backpropagation Path Search On Adversarial Transferability](http://arxiv.org/abs/2308.07625v1)
  * 黑盒
    * [CGBA: Curvature-aware Geometric Black-box Attack](https://arxiv.org/abs/2308.03163)<br>:star:[code](https://github.com/Farhamdur/CGBA)
  * 对抗样本
    * [Downstream-agnostic Adversarial Examples](http://arxiv.org/abs/2307.12280v1)
    * [Benchmarking and Analyzing Robust Point Cloud Recognition: Bag of Tricks for Defending Adversarial Examples](http://arxiv.org/abs/2307.16361v1)<br>:star:[code](https://github.com/qiufan319/benchmark_pc_attack.git)
  * 对抗攻击
    * [An Adaptive Model Ensemble Adversarial Attack for Boosting Adversarial Transferability](http://arxiv.org/abs/2308.02897v1)
    * [LEA2: A Lightweight Ensemble Adversarial Attack via Non-overlapping Vulnerable Frequency Regions](https://openaccess.thecvf.com/content/ICCV2023/papers/Qian_LEA2_A_Lightweight_Ensemble_Adversarial_Attack_via_Non-overlapping_Vulnerable_Frequency_ICCV_2023_paper.pdf)
  * 对抗训练  
    * [Improving Generalization of Adversarial Training via Robust Critical Fine-Tuning](http://arxiv.org/abs/2308.02533v1)<br>:star:[code](https://github.com/microsoft/robustlearn)  
    * [Adversarial Finetuning with Latent Representation Constraint to Mitigate Accuracy-Robustness Tradeoff](http://arxiv.org/abs/2308.16454v1)
    * [Advancing Example Exploitation Can Alleviate Critical Challenges in Adversarial Training](https://openaccess.thecvf.com/content/ICCV2023/papers/Ge_Advancing_Example_Exploitation_Can_Alleviate_Critical_Challenges_in_Adversarial_Training_ICCV_2023_paper.pdf)
  * 后门
    * [Enhancing Fine-Tuning Based Backdoor Defense with Sharpness-Aware Minimization](http://arxiv.org/abs/2304.11823)
    * [Computation and Data Efficient Backdoor Attacks](https://openaccess.thecvf.com/content/ICCV2023/papers/Wu_Computation_and_Data_Efficient_Backdoor_Attacks_ICCV_2023_paper.pdf)
* Class Incremental Learning(类增量学习)
  * [Proxy Anchor-based Unsupervised Learning for Continuous Generalized Category Discovery](http://arxiv.org/abs/2307.10943v1)
  * [Knowledge Restore and Transfer for Multi-Label Class-Incremental Learning](http://arxiv.org/abs/2302.13334)
  * [Audio-Visual Class-Incremental Learning](http://arxiv.org/abs/2308.11073v1)<br>:star:[code](https://github.com/weiguoPian/AV-CIL_ICCV2023)
  * [Masked Autoencoders are Efficient Class Incremental Learners](http://arxiv.org/abs/2308.12510v1)<br>:star:[code](https://github.com/scok30/MAE-CIL)
  * [Heterogeneous Forgetting Compensation for Class-Incremental Learning](http://arxiv.org/abs/2308.03374v1)<br>:star:[code](https://github.com/JiahuaDong/HFC)
  * [Class-Incremental Grouping Network for Continual Audio-Visual Learning](http://arxiv.org/abs/2309.05281v1)<br>:star:[code](https://github.com/stoneMo/CIGN)
  * [Space-time Prompting for Video Class-incremental Learning](https://openaccess.thecvf.com/content/ICCV2023/papers/Pei_Space-time_Prompting_for_Video_Class-incremental_Learning_ICCV_2023_paper.pdf)
* 多任务学习
  * [Efficient Controllable Multi-Task Architectures](http://arxiv.org/abs/2308.11744v1)
  * [MAS: Towards Resource-Efficient Federated Multiple-Task Learning](http://arxiv.org/abs/2307.11285v1)
  * [Vision Transformer Adapters for Generalizable Multitask Learning](http://arxiv.org/abs/2308.12372v1)<br>:star:[code](https://ivrl.github.io/VTAGML)
  * [TaskExpert: Dynamically Assembling Multi-Task Representations with Memorial Mixture-of-Experts](http://arxiv.org/abs/2307.15324v1)<br>:star:[code](https://github.com/prismformore/Multi-Task-Transformer)
  * [Achievement-Based Training Progress Balancing for Multi-Task Learning](https://openaccess.thecvf.com/content/ICCV2023/papers/Yun_Achievement-Based_Training_Progress_Balancing_for_Multi-Task_Learning_ICCV_2023_paper.pdf)
* 持续学习
  * [CLR: Channel-wise Lightweight Reprogramming for Continual Learning](http://arxiv.org/abs/2307.11386v1)<br>:star:[code](https://github.com/gyhandy/Channel-wise-Lightweight-Reprogramming)
  * [CLNeRF: Continual Learning Meets NeRF](http://arxiv.org/abs/2308.14816v1)<br>:star:[code](https://github.com/IntelLabs/CLNeRF)<br>:house:[project](https://youtu.be/nLRt6OoDGq0?si=8yD6k-8MMBJInQPs)
  * [Exemplar-Free Continual Transformer with Convolutions](http://arxiv.org/abs/2308.11357v1)
  * [Online Prototype Learning for Online Continual Learning](http://arxiv.org/abs/2308.00301v1)<br>:star:[code](https://github.com/weilllllls/OnPro)
  * [CBA: Improving Online Continual Learning via Continual Bias Adaptor](http://arxiv.org/abs/2308.06925v1)
  * [Introducing Language Guidance in Prompt-based Continual Learning](http://arxiv.org/abs/2308.15827v1)
  * [NAPA-VQ: Neighborhood Aware Prototype Augmentation with Vector Quantization for Continual Learning](http://arxiv.org/abs/2308.09297v1)<br>:star:[code](https://github.com/TamashaM/NAPA-VQ.git)
  * [Generating Instance-level Prompts for Rehearsal-free Continual Learning](https://openaccess.thecvf.com/content/ICCV2023/papers/Jung_Generating_Instance-level_Prompts_for_Rehearsal-free_Continual_Learning_ICCV_2023_paper.pdf)
  * [Online Continual Learning on Hierarchical Label Expansion](http://arxiv.org/abs/2308.14374)
* 增量学习 
  * [When Prompt-based Incremental Learning Does Not Meet Strong Pretraining](http://arxiv.org/abs/2308.10445v1)<br>:star:[code](https://github.com/TOM-tym/APG)
* Federated Learning(联邦学习)
  * [ProtoFL: Unsupervised Federated Learning via Prototypical Distillation](http://arxiv.org/abs/2307.12450v1)
  * [Efficient Model Personalization in Federated Learning via Client-Specific Prompt Generation](http://arxiv.org/abs/2308.15367v1)
  * [FedPerfix: Towards Partial Model Personalization of Vision Transformers in Federated Learning](http://arxiv.org/abs/2308.09160v1)
  * [Workie-Talkie: Accelerating Federated Learning by Overlapping Computing and Communications via Contrastive Regularization](https://openaccess.thecvf.com/content/ICCV2023/papers/Chen_Workie-Talkie_Accelerating_Federated_Learning_by_Overlapping_Computing_and_Communications_via_ICCV_2023_paper.pdf)
  * [Generative Gradient Inversion via Over-Parameterized Networks in Federated Learning](https://openaccess.thecvf.com/content/ICCV2023/papers/Zhang_Generative_Gradient_Inversion_via_Over-Parameterized_Networks_in_Federated_Learning_ICCV_2023_paper.pdf)
* Reinforcement Learning(强化学习)
  * [Improving Generalization in Visual Reinforcement Learning via Conflict-aware Gradient Agreement Augmentation](http://arxiv.org/abs/2308.01194v1)
  * [DISeR: Designing Imaging Systems with Reinforcement Learning](http://arxiv.org/abs/2309.13851v1)<br>:star:[code](https://tzofi.github.io/diser)
  * [Learning to Identify Critical States for Reinforcement Learning from Videos](http://arxiv.org/abs/2308.07795v1)<br>:star:[code](https://github.com/AI-Initiative-KAUST/VideoRLCS)
  * [Towards Attack-tolerant Federated Learning via Critical Parameter Analysis](http://arxiv.org/abs/2308.09318)
* 迁移学习
  * [Disposable Transfer Learning for Selective Source Task Unlearning](http://arxiv.org/abs/2308.09971v1)
  * [Building a Winning Team: Selecting Source Model Ensembles using a Submodular Transferability Estimation Approach](http://arxiv.org/abs/2309.02429v1)
  * [Exploring Model Transferability through the Lens of Potential Energy](http://arxiv.org/abs/2308.15074v1)<br>:star:[code](https://github.com/lixiaotong97/PED)
  * [Disentangling Spatial and Temporal Learning for Efficient Image-to-Video Transfer Learning](http://arxiv.org/abs/2309.07911v1)<br>:star:[code](https://github.com/alibaba-mmai-research/DiST)
* 元学习
  * [Meta-ZSDETR: Zero-shot DETR with Meta-learning](http://arxiv.org/abs/2308.09540v1)
* 度量学习
  * [Generalized Sum Pooling for Metric Learning](http://arxiv.org/abs/2308.09228v1)
  * [HSE: Hybrid Species Embedding for Deep Metric Learning](https://openaccess.thecvf.com/content/ICCV2023/papers/Yang_HSE_Hybrid_Species_Embedding_for_Deep_Metric_Learning_ICCV_2023_paper.pdf)
* 多模态学习
  * [Preserving Modality Structure Improves Multi-Modal Learning](http://arxiv.org/abs/2308.13077v1)<br>:star:[code](https://github.com/Swetha5/Multi_Sinkhorn_Knopp)
* 对比学习
  * [Contrastive Learning Relies More on Spatial Inductive Bias Than Supervised Learning: An Empirical Study](https://openaccess.thecvf.com/content/ICCV2023/papers/Zhong_Contrastive_Learning_Relies_More_on_Spatial_Inductive_Bias_Than_Supervised_ICCV_2023_paper.pdf)

<a name="25"/>

## 25.Model Compression/Knowledge Distillation/Pruning(模型压缩/知识蒸馏/剪枝)
* 量化
  * [EMQ: Evolving Training-free Proxies for Automated Mixed Precision Quantization](http://arxiv.org/abs/2307.10554v1)
  * [Causal-DFQ: Causality Guided Data-free Network Quantization](http://arxiv.org/abs/2309.13682v1)<br>:star:[code](https://github.com/42Shawn/Causal-DFQ)
  * [DenseShift: Towards Accurate and Efficient Low-Bit Power-of-Two Quantization](http://arxiv.org/abs/2208.09708)
  * [EQ-Net: Elastic Quantization Neural Networks](https://openaccess.thecvf.com/content/ICCV2023/papers/Xu_EQ-Net_Elastic_Quantization_Neural_Networks_ICCV_2023_paper.pdf)
* 剪枝
  * [Unified Data-Free Compression: Pruning and Quantization without Fine-Tuning](http://arxiv.org/abs/2308.07209v1)
* 轻量级网络
  * [Adaptive Frequency Filters As Efficient Global Token Mixers](http://arxiv.org/abs/2307.14008v1)
* 知识蒸馏
  * [DOT: A Distillation-Oriented Trainer](https://arxiv.org/abs/2307.08436)
  * [Automated Knowledge Distillation via Monte Carlo Tree Search](https://openaccess.thecvf.com/content/ICCV2023/papers/Li_Automated_Knowledge_Distillation_via_Monte_Carlo_Tree_Search_ICCV_2023_paper.pdf)
  * [Cumulative Spatial Knowledge Distillation for Vision Transformers](https://arxiv.org/abs/2307.08500)
  * [Multi-Label Knowledge Distillation](http://arxiv.org/abs/2308.06453v1)<br>:star:[code](https://github.com/penghui-yang/L2D)
  * [DistillBEV: Boosting Multi-Camera 3D Object Detection with Cross-Modal Knowledge Distillation](http://arxiv.org/abs/2309.15109v1)
  * [Alleviating Catastrophic Forgetting of Incremental Object Detection via Within-Class and Between-Class Knowledge Distillation](https://openaccess.thecvf.com/content/ICCV2023/papers/Kang_Alleviating_Catastrophic_Forgetting_of_Incremental_Object_Detection_via_Within-Class_and_ICCV_2023_paper.pdf)
* 模型压缩
  * [Lossy and Lossless (L2) Post-training Model Size Compression](https://openaccess.thecvf.com/content/ICCV2023/papers/Shi_Lossy_and_Lossless_L2_Post-training_Model_Size_Compression_ICCV_2023_paper.pdf)

<a name="24"/>

## 24.Few/Zero-Shot Learning/Domain Generalization/Adaptation(小/零样本/域泛化/域适应)
* 域适应
  * [Unsupervised Accuracy Estimation of Deep Visual Models using Domain-Adaptive Adversarial Perturbation without Source Samples](http://arxiv.org/abs/2307.10062v1)
  * [Order-preserving Consistency Regularization for Domain Adaptation and Generalization](http://arxiv.org/abs/2309.13258v1)
  * [StyleDomain: Efficient and Lightweight Parameterizations of StyleGAN for One-shot and Few-shot Domain Adaptation](http://arxiv.org/abs/2212.10229)
  * [SFHarmony: Source Free Domain Adaptation for Distributed Neuroimaging Analysis](http://arxiv.org/abs/2303.15965)
  * [Domain Adaptive Few-Shot Open-Set Learning](http://arxiv.org/abs/2309.12814)
  * [Confidence-based Visual Dispersal for Few-shot Unsupervised Domain Adaptation](http://arxiv.org/abs/2309.15575v1)<br>:star:[code](https://github.com/Bostoncake/C-VisDiT)
  * [LiDAR-UDA: Self-ensembling Through Time for Unsupervised LiDAR Domain Adaptation](http://arxiv.org/abs/2309.13523v1)<br>:star:[code](https://github.com/JHLee0513/LiDARUDA)
  * [Unsupervised Domain Adaptive Detection with Network Stability Analysis](http://arxiv.org/abs/2308.08182v1)<br>:star:[code](https://github.com/tiankongzhang/NSA)
  * [The Unreasonable Effectiveness of Large Language-Vision Models for Source-free Video Domain Adaptation](http://arxiv.org/abs/2308.09139v1)<br>:star:[code](https://github.com/giaczara/dallv)
  * [Domain-Specificity Inducing Transformers for Source-Free Domain Adaptation](http://arxiv.org/abs/2308.14023v1)<br>:house:[project](http://val.cds.iisc.ac.in/DSiT-SFDA)
  * [DomainAdaptor: A Novel Approach to Test-time Adaptation](http://arxiv.org/abs/2308.10297v1)<br>:star:[code](https://github.com/koncle/DomainAdaptor)
  * [GeT: Generative Target Structure Debiasing for Domain Adaptation](http://arxiv.org/abs/2308.10205v1)<br>:star:[code](https://lulusindazc.github.io/getproject/)
  * [Black-box Unsupervised Domain Adaptation with Bi-directional Atkinson-Shiffrin Memory](http://arxiv.org/abs/2308.13236v1)
  * [Towards Better Robustness against Common Corruptions for Unsupervised Domain Adaptation](https://openaccess.thecvf.com/content/ICCV2023/papers/Gao_Towards_Better_Robustness_against_Common_Corruptions_for_Unsupervised_Domain_Adaptation_ICCV_2023_paper.pdf)
  * [Bidirectional Alignment for Domain Adaptive Detection with Transformers](https://openaccess.thecvf.com/content/ICCV2023/papers/He_Bidirectional_Alignment_for_Domain_Adaptive_Detection_with_Transformers_ICCV_2023_paper.pdf)
* 域泛化
  * [Flatness-Aware Minimization for Domain Generalization](http://arxiv.org/abs/2307.11108v1)
  * [DomainDrop: Suppressing Domain-Sensitive Channels for Domain Generalization](http://arxiv.org/abs/2308.10285v1)<br>:star:[code](https://github.com/lingeringlight/DomainDrop)
  * [Domain Generalization via Balancing Training Difficulty and Model Capability](http://arxiv.org/abs/2309.00844v1)
  * [DandelionNet: Domain Composition with Instance Adaptive Classification for Domain Generalization](https://openaccess.thecvf.com/content/ICCV2023/papers/Hu_DandelionNet_Domain_Composition_with_Instance_Adaptive_Classification_for_Domain_Generalization_ICCV_2023_paper.pdf)
  * [iDAG: Invariant DAG Searching for Domain Generalization](https://openaccess.thecvf.com/content/ICCV2023/papers/Huang_iDAG_Invariant_DAG_Searching_for_Domain_Generalization_ICCV_2023_paper.pdf)
  * [A Sentence Speaks a Thousand Images: Domain Generalization through Distilling CLIP with Language Guidance](http://arxiv.org/abs/2309.12530v1)
  * [Understanding Hessian Alignment for Domain Generalization](http://arxiv.org/abs/2308.11778v1)<br>:star:[code](https://github.com/huawei-noah/Federated-Learning/tree/main/HessianAlignment)
  * [Domain Generalization via Rationale Invariance](http://arxiv.org/abs/2308.11158v1)<br>:star:[code](https://github.com/liangchen527/RIDG)
  * [PromptStyler: Prompt-driven Style Generation for Source-free Domain Generalization](http://arxiv.org/abs/2307.15199v1)<br>:house:[project](https://promptstyler.github.io/)
  * [Generalizable Decision Boundaries: Dualistic Meta-Learning for Open Set Domain Generalization](http://arxiv.org/abs/2308.09391v1)<br>:star:[code](https://github.com/zzwdx/MEDIC)
* 零样本学习
  * [Hyperbolic Audio-visual Zero-shot Learning](http://arxiv.org/abs/2308.12558v1)
  * [Continual Zero-Shot Learning through Semantically Guided Generative Random Walks](http://arxiv.org/abs/2308.12366v1)<br>:star:[code](https://github.com/wx-zhang/IGCZSL)
  * [Hierarchical Visual Primitive Experts for Compositional Zero-Shot Learning](http://arxiv.org/abs/2308.04016v1)<br>:star:[code](https://github.com/HanjaeKim98/CoT)
  * [Distilled Reverse Attention Network for Open-world Compositional Zero-Shot Learning](http://arxiv.org/abs/2303.00404)
* 小样本学习
  * [Prototypes-oriented Transductive Few-shot Learning with Conditional Transport](http://arxiv.org/abs/2308.03047v1)
  * [CDFSL-V: Cross-Domain Few-Shot Learning for Videos](http://arxiv.org/abs/2309.03989v1)<br>:star:[code](https://github.com/Sarinda251/CDFSL-V)
  * [Read-only Prompt Optimization for Vision-Language Few-shot Learning](http://arxiv.org/abs/2308.14960v1)<br>:star:[code](https://github.com/mlvlab/RPO)

<a name="23"/>

## 23.Optical Flow Estimation(光流估计)
* [GAFlow: Incorporating Gaussian Attention into Optical Flow]未公开
* [Explicit Motion Disentangling for Efficient Optical Flow Estimation]未公开
* [VideoFlow: Exploiting Temporal Cues for Multi-frame Optical Flow Estimation](https://arxiv.org/abs/2303.08340)<br>:star:[code](https://github.com/XiaoyuShi97/VideoFlow)<br>:thumbsup:[ICCV2023|港中文MMLab提出多帧光流估计模型VideoFlow，充分挖掘时序线索，Sintel与KITTI榜单排名第一](https://mp.weixin.qq.com/s/jsHDk055nSCmkJ8TXch_Lg)
* [MPI-Flow: Learning Realistic Optical Flow with Multiplane Images](http://arxiv.org/abs/2309.06714v1)<br>:star:[code](https://github.com/Sharpiless/MPI-Flow)
* [RPEFlow: Multimodal Fusion of RGB-PointCloud-Event for Joint Optical Flow and Scene Flow Estimation](http://arxiv.org/abs/2309.15082v1)<br>:star:[code](https://github.com/danqu130/RPEFlow)<br>:star:[code](https://npucvr.github.io/RPEFlow)
* [Event-based Temporally Dense Optical Flow Estimation with Sequential Learning](https://openaccess.thecvf.com/content/ICCV2023/papers/Ponghiran_Event-based_Temporally_Dense_Optical_Flow_Estimation_with_Sequential_Learning_ICCV_2023_paper.pdf)

<a name="22"/>

## 22.OCR
* [Self-Supervised Character-to-Character Distillation for Text Recognition](http://arxiv.org/abs/2211.00288)
* [Vision Grid Transformer for Document Layout Analysis](http://arxiv.org/abs/2308.14978v1)<br>:star:[code](https://github.com/AlibabaResearch/AdvancedLiterateMachinery)
* [CNN based Cuneiform Sign Detection Learned from Annotated 3D Renderings and Mapped Photographs with Illumination Augmentation](http://arxiv.org/abs/2308.11277v1)<br>:house:[project](https://gigamesh.eu)
* [ESTextSpotter: Towards Better Scene Text Spotting with Explicit Synergy in Transformer](http://arxiv.org/abs/2308.10147v1)<br>:star:[code](https://github.com/mxin262/ESTextSpotter)
* 场景文本识别
  * [LISTER: Neighbor Decoding for Length-Insensitive Scene Text Recognition](http://arxiv.org/abs/2308.12774v1)
* 中文文本识别
  * [Chinese Text Recognition with A Pre-Trained CLIP-Like Model Through Image-IDS Aligning](http://arxiv.org/abs/2309.01083v1)<br>:star:[code](https://github.com/FudanVI/FudanOCR/tree/main/image-ids-CTR)
* 文档理解
  * [Attention Where It Matters: Rethinking Visual Document Understanding with Selective Region Concentration](http://arxiv.org/abs/2309.01131v1)
  * [SCOB: Universal Text Understanding via Character-wise Supervised Contrastive Learning with Online Text Rendering for Bridging Domain Gap](http://arxiv.org/abs/2309.12382v1)<br>:star:[code](https://github.com/naver-ai/scob)
* 字体生成
  * [Few shot font generation via transferring similarity guided global style and quantization local style](http://arxiv.org/abs/2309.00827v1)<br>:star:[code](https://github.com/awei669/VQ-Font)
* 实体识别
  * [Open-domain Visual Entity Recognition: Towards Recognizing Millions of Wikipedia Entities](http://arxiv.org/abs/2302.11154)

<a name="21"/>

## 21.Point Cloud(点云)
* [Ponder: Point Cloud Pre-training via Neural Rendering](http://arxiv.org/abs/2301.00157)
* [EPiC: Ensemble of Partial Point Clouds for Robust Classification](http://arxiv.org/abs/2303.11419)
* [Point Contrastive Prediction with Semantic Clustering for Self-Supervised Learning on Point Cloud Videos](http://arxiv.org/abs/2308.09247v1)
* [Masked Spatio-Temporal Structure Prediction for Self-supervised Learning on Point Cloud Videos](http://arxiv.org/abs/2308.09245v1)
* [SC3K: Self-supervised and Coherent 3D Keypoints Estimation from Rotated, Noisy, and Decimated Point Cloud Data](http://arxiv.org/abs/2308.05410v1)<br>:star:[code](https://github.com/IITPAVIS/SC3K)
* [2D3D-MATR: 2D-3D Matching Transformer for Detection-free Registration between Images and Point Clouds](http://arxiv.org/abs/2308.05667v1)<br>:star:[code](https://github.com/minhaolee/2D3DMATR)
* [DELFlow: Dense Efficient Learning of Scene Flow for Large-Scale Point Clouds](http://arxiv.org/abs/2308.04383v1)<br>:star:[code](https://github.com/IRMVLab/DELFlow)
* [Sketch and Text Guided Diffusion Model for Colored Point Cloud Generation](http://arxiv.org/abs/2308.02874v1)
* [Clustering based Point Cloud Representation Learning for 3D Analysis](http://arxiv.org/abs/2307.14605v1)<br>:star:[code](https://github.com/FengZicai/Cluster3Dseg/)
* [Take-A-Photo: 3D-to-2D Generative Pre-training of Point Cloud Models](http://arxiv.org/abs/2307.14971v1)<br>:house:[project](https://tap.ivg-research.xyz)<br>:star:[code](https://github.com/wangzy22/TAP)
* 点云配准
  * [Density-invariant Features for Distant Point Cloud Registration](http://arxiv.org/abs/2307.09788v1)
  * [SIRA-PCR: Sim-to-Real Adaptation for 3D Point Cloud Registration]未公开
  * [PointMBF: A Multi-scale Bidirectional Fusion Network for Unsupervised RGB-D Point Cloud Registration](http://arxiv.org/abs/2308.04782v1)<br>:star:[code](https://github.com/phdymz/PointMBF)
  * [Sample-adaptive Augmentation for Point Cloud Recognition Against Real-world Corruptions](http://arxiv.org/abs/2309.10431v1)<br>:star:[code](https://github.com/Roywangj/AdaptPoint)
  * [AutoSynth: Learning to Generate 3D Training Data for Object Point Cloud Registration](http://arxiv.org/abs/2309.11170v1)
* 点云分割
  * [See More and Know More: Zero-shot Point Cloud Segmentation via Multi-modal Visual Data](http://arxiv.org/abs/2307.10782v1)
  * [CPCM: Contextual Point Cloud Modeling for Weakly-supervised Point Cloud Semantic Segmentation](http://arxiv.org/abs/2307.10316v1)
  * [GaPro: Box-Supervised 3D Point Cloud Instance Segmentation Using Gaussian Processes as Pseudo Labelers](http://arxiv.org/abs/2307.13251v1)<br>:star:[code](https://github.com/VinAIResearch/GaPro)
  * [Hierarchical Point-based Active Learning for Semi-supervised Point Cloud Semantic Segmentation](http://arxiv.org/abs/2308.11166v1)<br>:star:[code](https://github.com/SmiletoE/HPAL)
  * [Generalized Few-Shot Point Cloud Segmentation Via Geometric Words](http://arxiv.org/abs/2309.11222v1)<br>:star:[code](https://github.com/Pixie8888/GFS-3DSeg_GWs)
* 点云补全
  * [P2C: Self-Supervised Point Cloud Completion from Single Partial Clouds](http://arxiv.org/abs/2307.14726v1)<br>:star:[code](https://github.com/CuiRuikai/Partial2Complete)
* 3D点云
  * [3DHacker: Spectrum-based Decision Boundary Generation for Hard-label 3D Point Cloud Attack](http://arxiv.org/abs/2308.07546v1)
  * [GridPull: Towards Scalability in Learning Implicit Representations from 3D Point Clouds](http://arxiv.org/abs/2308.13175v1)<br>:star:[code](https://github.com/chenchao15/GridPull)
  * [DiffFacto: Controllable Part-Based 3D Point Cloud Generation with Cross Diffusion](http://arxiv.org/abs/2305.01921)

<a name="20"/>

## 20.Reid(人员重识别/步态识别/行人检测)
* Reid
  * [Identity-Seeking Self-Supervised Representation Learning for Generalizable Person Re-identification](http://arxiv.org/abs/2308.08887v1)<br>:star:[code](https://github.com/dcp15/ISR_ICCV2023_Oral)
  * [Unified Pre-training with Pseudo Texts for Text-To-Image Person Re-identification](http://arxiv.org/abs/2309.01420v1)
  * [Camera-Driven Representation Learning for Unsupervised Domain Adaptive Person Re-identification](http://arxiv.org/abs/2308.11901v1)
  * [Learning Clothing and Pose Invariant 3D Shape Representation for Long-Term Person Re-Identification](http://arxiv.org/abs/2308.10658v1)
  * [Identity-Seeking Self-Supervised Representation Learning for Generalizable Person Re-Identification](http://arxiv.org/abs/2308.08887)
  * 可见光红外重识别
    * [Modality Unifying Network for Visible-Infrared Person Re-Identification](http://arxiv.org/abs/2309.06262v1)
* 步态识别
  * [Hierarchical Spatio-Temporal Representation Learning for Gait Recognition](http://arxiv.org/abs/2307.09856v1)
  * [GPGait: Generalized Pose-based Gait Recognition](http://arxiv.org/abs/2303.05234)
* 人群计数
  * [Calibrating Uncertainty for Semi-Supervised Crowd Counting](http://arxiv.org/abs/2308.09887v1)
  * [Point-Query Quadtree for Crowd Counting, Localization, and More](http://arxiv.org/abs/2308.13814v1)<br>:star:[code](https://github.com/cxliu0/PET)

<a name="19"/>

## 19.UAV/Remote Sensing/Satellite Image(无人机/遥感/卫星图像)
* [View Consistent Purification for Accurate Cross-View Localization](http://arxiv.org/abs/2308.08110v1)
* [Class Prior-Free Positive-Unlabeled Learning with Taylor Variational Loss for Hyperspectral Remote Sensing Imagery](http://arxiv.org/abs/2308.15081v1)<br>:star:[code](https://github.com/Hengwei-Zhao96/T-HOneCls)

<a name="18"/>

## 18.Human Pose Estimation
* 人体姿态估计
  * [DiffPose: SpatioTemporal Diffusion Model for Video-Based Human Pose Estimation](http://arxiv.org/abs/2307.16687v1)
  * [Source-free Domain Adaptive Human Pose Estimation](http://arxiv.org/abs/2308.03202v1)
  * [Prior-guided Source-free Domain Adaptation for Human Pose Estimation](http://arxiv.org/abs/2308.13954v1)
  * [TEMPO: Efficient Multi-View Pose Estimation, Tracking, and Forecasting](http://arxiv.org/abs/2309.07910v1)
* 多人姿态估计
  * [Group Pose: A Simple Baseline for End-to-End Multi-person Pose Estimation](http://arxiv.org/abs/2308.07313v1)<br>:star:[code](https://github.com/Michel-liu/GroupPose-Paddle)<br>:star:[code](https://github.com/Michel-liu/GroupPose)
* 3D人体姿态估计
  * [3D-Aware Neural Body Fitting for Occlusion Robust 3D Human Pose Estimation](http://arxiv.org/abs/2308.10123v1)<br>:star:[code](https://github.com/edz-o/3DNBF)<br>:star:[code](https://3dnbf.github.io/)
  * [Co-Evolution of Pose and Mesh for 3D Human Body Estimation from Video](http://arxiv.org/abs/2308.10305v1)<br>:star:[code](https://kasvii.github.io/PMCE)<br>:star:[code](https://github.com/kasvii/PMCE)
  * [EMDB: The Electromagnetic Database of Global 3D Human Pose and Shape in the Wild](http://arxiv.org/abs/2308.16894v1)<br>:house:[project](https://ait.ethz.ch/emdb)
  * [PoseFix: Correcting 3D Human Poses with Natural Language](http://arxiv.org/abs/2309.08480v1)
  * [Towards Robust and Smooth 3D Multi-Person Pose Estimation from Monocular Videos in the Wild](http://arxiv.org/abs/2309.08644v1)<br>:house:[project](https://www.youtube.com/@potr3d)
* 人体网格恢复
  * [JOTR: 3D Joint Contrastive Learning with Transformers for Occluded Human Mesh Recovery](http://arxiv.org/abs/2307.16377v1)
  * [Distribution-Aligned Diffusion for Human Mesh Recovery](http://arxiv.org/abs/2308.13369v1)<br>:star:[code](https://gongjia0208.github.io/HMDiff/)
  * [Cloth2Body: Generating 3D Human Body Mesh from 2D Clothing](https://openaccess.thecvf.com/content/ICCV2023/papers/Dai_Cloth2Body_Generating_3D_Human_Body_Mesh_from_2D_Clothing_ICCV_2023_paper.pdf)
  * [3D Human Mesh Recovery with Sequentially Global Rotation Estimation](https://openaccess.thecvf.com/content/ICCV2023/papers/Wang_3D_Human_Mesh_Recovery_with_Sequentially_Global_Rotation_Estimation_ICCV_2023_paper.pdf)
* 多人网格恢复
  * [Coordinate Transformer: Achieving Single-stage Multi-person Mesh Recovery from Videos](http://arxiv.org/abs/2308.10334v1)<br>:star:[code](https://github.com/Li-Hao-yuan/CoordFormer)
* 3D人体恢复
  * [ReFit: Recurrent Fitting Network for 3D Human Recovery](http://arxiv.org/abs/2308.11184v1)<br>:star:[code](https://yufu-wang.github.io/refit_humans/)
* 姿势迁移
  * [Weakly-supervised 3D Pose Transfer with Keypoints](http://arxiv.org/abs/2307.13459v1)
* 三维人体重建
  * [Body Knowledge and Uncertainty Modeling for Monocular 3D Human Body Reconstruction](http://arxiv.org/abs/2308.00799v1)Meta
  * [Chupa: Carving 3D Clothed Humans from Skinned Shape Priors using 2D Diffusion Probabilistic Models](http://arxiv.org/abs/2305.11870)
* 三维人体网格重建
  * [Cyclic Test-Time Adaptation on Monocular Video for 3D Human Mesh Reconstruction](http://arxiv.org/abs/2308.06554v1)<br>:star:[code](https://github.com/hygenie1228/CycleAdapt_RELEASE)
* 手部姿势估计
  * [Deformer: Dynamic Fusion Transformer for Robust Hand Pose Estimation](http://arxiv.org/abs/2303.04991)
  * [HaMuCo: Hand Pose Estimation via Multiview Collaborative Self-Supervised Learning](http://arxiv.org/abs/2302.00988)
* 3D手部姿态估计
  * [OCHID-Fi: Occlusion-Robust Hand Pose Estimation in 3D via RF-Vision](http://arxiv.org/abs/2308.10146v1)
  * [RenderIH: A Large-scale Synthetic Dataset for 3D Interacting Hand Pose Estimation](http://arxiv.org/abs/2309.09301v1)<br>:star:[code](https://github.com/adwardlee/RenderIH)
* 手-物建模
  * [CHORD: Category-level Hand-held Object Reconstruction via Shape Deformation](http://arxiv.org/abs/2308.10574v1)<br>:star:[code](https://kailinli.github.io/CHORD)
  * [Dynamic Hyperbolic Attention Network for Fine Hand-object Reconstruction](http://arxiv.org/abs/2309.02965v1)
  * [Reconstructing Interacting Hands with Interaction Prior from Monocular Images](http://arxiv.org/abs/2308.14082v1)<br>:star:[code](https://github.com/binghui-z/InterPrior_pytorch)
* 手部重建
  * [Spectral Graphormer: Spectral Graph-based Transformer for Egocentric Two-Hand Reconstruction using Multi-View Color Images](http://arxiv.org/abs/2308.11015v1)
* 手势生成
  * [LivelySpeaker: Towards Semantic-Aware Co-Speech Gesture Generation](http://arxiv.org/abs/2309.09294v1)
* 手势识别
  * [Learning Robust Representations with Information Bottleneck and Memory Network for RGB-D-based Gesture Recognition](https://openaccess.thecvf.com/content/ICCV2023/papers/Li_Learning_Robust_Representations_with_Information_Bottleneck_and_Memory_Network_for_ICCV_2023_paper.pdf)
* 人体合成
  * [UnitedHuman: Harnessing Multi-Source Data for High-Resolution Human Generation](http://arxiv.org/abs/2309.14335v1)<br>:star:[code](https://unitedhuman.github.io/)<br>:star:[code](https://github.com/UnitedHuman/UnitedHuman)
* 3D 人体运动生成
  * [Make-An-Animation: Large-Scale Text-conditional 3D Human Motion Generation](https://openaccess.thecvf.com/content/ICCV2023/papers/Azadi_Make-An-Animation_Large-Scale_Text-conditional_3D_Human_Motion_Generation_ICCV_2023_paper.pdf)
  * [Synthesizing Diverse Human Motions in 3D Indoor Scenes](http://arxiv.org/abs/2305.12411)
  * [TMR: Text-to-Motion Retrieval Using Contrastive 3D Human Motion Synthesis](http://arxiv.org/abs/2305.00976)
  * [Guided Motion Diffusion for Controllable Human Motion Synthesis](https://openaccess.thecvf.com/content/ICCV2023/papers/Karunratanakul_Guided_Motion_Diffusion_for_Controllable_Human_Motion_Synthesis_ICCV_2023_paper.pdf)

<a name="17"/>

## 17.Generative Adversarial Network
* [LFS-GAN: Lifelong Few-Shot Image Generation](http://arxiv.org/abs/2308.11917v1)
* [What can Discriminator do? Towards Box-free Ownership Verification of Generative Adversarial Network](http://arxiv.org/abs/2307.15860v1)<br>:star:[code](https://github.com/AbstractTeen/gan_ownership_verification)
* [Improving Diversity in Zero-Shot GAN Adaptation with Semantic Variations](http://arxiv.org/abs/2308.10554v1)
* [LinkGAN: Linking GAN Latents to Pixels for Controllable Image Synthesis](http://arxiv.org/abs/2301.04604)
* [Smoothness Similarity Regularization for Few-Shot GAN Adaptation](http://arxiv.org/abs/2308.09717v1)
* GAN 逆映射
  * [Diverse Inpainting and Editing with GAN Inversion](http://arxiv.org/abs/2307.15033v1)

<a name="16"/>

## 16.Super-Resolution(超分辨率)
* [On the Effectiveness of Spectral Discriminators for Perceptual Quality Improvement](http://arxiv.org/abs/2307.12027v1)<br>:star:[code](https://github.com/Luciennnnnnn/DualFormer)
* [Dual Aggregation Transformer for Image Super-Resolution](http://arxiv.org/abs/2308.03364v1)<br>:star:[code](https://github.com/zhengchen1999/DAT)
* [Feature Modulation Transformer: Cross-Refinement of Global Representation via High-Frequency Prior for Image Super-Resolution](http://arxiv.org/abs/2308.05022v1)<br>:star:[code](https://github.com/AVC2-UESTC/CRAFT-SR.git)
* [MetaF2N: Blind Image Super-Resolution by Learning Efficient Model Adaptation from Faces](http://arxiv.org/abs/2309.08113v1)<br>:star:[code](https://github.com/yinzhicun/MetaF2N)
* [HSR-Diff: Hyperspectral Image Super-Resolution via Conditional Diffusion Models](https://openaccess.thecvf.com/content/ICCV2023/papers/Wu_HSR-Diff_Hyperspectral_Image_Super-Resolution_via_Conditional_Diffusion_Models_ICCV_2023_paper.pdf)

<a name="15"/>

## 15.Image/Video Retrieval(图像/视频检索)
* [Learning Spatial-context-aware Global Visual Feature Representation for Instance Image Retrieval](https://openaccess.thecvf.com/content/ICCV2023/papers/Zhang_Learning_Spatial-context-aware_Global_Visual_Feature_Representation_for_Instance_Image_Retrieval_ICCV_2023_paper.pdf)
* [U-RED: Unsupervised 3D Shape Retrieval and Deformation for Partial Point Clouds](http://arxiv.org/abs/2308.06383v1)
* [DeDrift: Robust Similarity Search under Content Drift](http://arxiv.org/abs/2308.02752v1)
* [FashionNTM: Multi-turn Fashion Image Retrieval via Cascaded Memory](http://arxiv.org/abs/2308.10170v1)<br>:house:[project](https://sites.google.com/eng.ucsd.edu/fashionntm)
* [Coarse-to-Fine: Learning Compact Discriminative Representation for Single-Stage Image Retrieval](http://arxiv.org/abs/2308.04008v1)<br>:star:[code](https://github.com/bassyess/CFCD)
* [Global Features are All You Need for Image Retrieval and Reranking](http://arxiv.org/abs/2308.06954v1)<br>:star:[code](https://github.com/ShihaoShao-GH/SuperGlobal)
* 图像-文本检索
  * [LexLIP: Lexicon-Bottlenecked Language-Image Pre-Training for Large-Scale Image-Text Sparse Retrieval](https://openaccess.thecvf.com/content/ICCV2023/papers/Luo_LexLIP_Lexicon-Bottlenecked_Language-Image_Pre-Training_for_Large-Scale_Image-Text_Sparse_Retrieval_ICCV_2023_paper.pdf)
* 文本-视频检索
  * [Helping Hands: An Object-Aware Ego-Centric Video Recognition Model](http://arxiv.org/abs/2308.07918v1)
  * [Prompt Switch: Efficient CLIP Adaptation for Text-Video Retrieval](http://arxiv.org/abs/2308.07648v1)
  * [In-Style: Bridging Text and Uncurated Videos with Style Transfer for Text-Video Retrieval](http://arxiv.org/abs/2309.08928v1)<br>:star:[code](https://github.com/ninatu/in_style)
* 视频-文本检索  
  * [Multi-event Video-Text Retrieval](http://arxiv.org/abs/2308.11551v1)<br>:star:[code](https://github.com/gengyuanmax/MeVTR)
  * [Unified Coarse-to-Fine Alignment for Video-Text Retrieval](http://arxiv.org/abs/2309.10091v1)<br>:star:[code](https://github.com/Ziyang412/UCoFiA)
* 视频检索
  * [Simple Baselines for Interactive Video Retrieval with Questions and Answers](http://arxiv.org/abs/2308.10402v1)<br>:star:[code](https://github.com/kevinliang888/IVR-QA-baselines)

<a name="14"/>

## 14.Image/Video Composition(图像/视频压缩)
* 图像压缩
  * [TF-ICON: Diffusion-Based Training-Free Cross-Domain Image Composition](http://arxiv.org/abs/2307.12493v1)<br>:star:[code](https://github.com/Shilin-LU/TF-ICON)
  * [COMPASS: High-Efficiency Deep Image Compression with Arbitrary-scale Spatial Scalability](http://arxiv.org/abs/2309.07926v1)
* 视频压缩
  * [Deep Optics for Video Snapshot Compressive Imaging](https://openaccess.thecvf.com/content/ICCV2023/papers/Wang_Deep_Optics_for_Video_Snapshot_Compressive_Imaging_ICCV_2023_paper.pdf)

<a name="13"/>

## 13.Image Captions(图像字幕)
* [Guiding Image Captioning Models Toward More Specific Captions](http://arxiv.org/abs/2307.16686v1)
* [OxfordTVG-HIC: Can Machine Make Humorous Captions from Images?](http://arxiv.org/abs/2307.11636v1)
* [Transferable Decoding with Visual Entities for Zero-Shot Image Captioning](http://arxiv.org/abs/2307.16525v1)<br>:star:[code](https://github.com/FeiElysia/ViECap)
* [Explore and Tell: Embodied Visual Captioning in 3D Environments](http://arxiv.org/abs/2308.10447v1)<br>:star:[code](https://aim3-ruc.github.io/ExploreAndTell)
* [With a Little Help from your own Past: Prototypical Memory Networks for Image Captioning](http://arxiv.org/abs/2308.12383v1)<br>:star:[code](https://github.com/aimagelab/PMA-Net)
* 更改字幕
  * [Self-supervised Cross-view Representation Reconstruction for Change Captioning](https://openaccess.thecvf.com/content/ICCV2023/papers/Tu_Self-supervised_Cross-view_Representation_Reconstruction_for_Change_Captioning_ICCV_2023_paper.pdf)


<a name="12"/>

## 12.Medical Image(医学影像)
* [A skeletonization algorithm for gradient-based optimization](http://arxiv.org/abs/2309.02527v1)
* [Learning to Distill Global Representation for Sparse-View CT](http://arxiv.org/abs/2308.08463v1)<br>:star:[code](https://github.com/longzilicart/GloReDi)
* [Taxonomy Adaptive Cross-Domain Adaptation in Medical Imaging via Optimization Trajectory Distillation](http://arxiv.org/abs/2307.14709v1)
* [Dual Meta-Learning with Longitudinally Generalized Regularization for One-Shot Brain Tissue Segmentation Across the Human Lifespan](http://arxiv.org/abs/2308.06774v1)<br>:star:[code](https://github.com/ladderlab-xjtu/DuMeta)
* [ConSlide: Asynchronous Hierarchical Interaction Transformer with Breakup-Reorganize Rehearsal for Continual Whole Slide Image Analysis](http://arxiv.org/abs/2308.13324v1)
* 医学影像配准
  * [Towards Saner Deep Image Registration](http://arxiv.org/abs/2307.09696v1)<br>:star:[code](https://github.com/tuffr5/Saner-deep-registration)
  * [Preserving Tumor Volumes for Unsupervised Medical Image Registration](http://arxiv.org/abs/2309.10153v1)<br>:star:[code](https://dddraxxx.github.io/Volume-Preserving-Registration/)
* 医学报告生成
  * [PRIOR: Prototype Representation Joint Learning from Medical Images and Reports](http://arxiv.org/abs/2307.12577v1)<br>:star:[code](https://github.com/QtacierP/PRIOR)
  * [Unify, Align and Refine: Multi-Level Semantic Alignment for Radiology Report Generation](http://arxiv.org/abs/2303.15932)
* 切片分类
  * [Multiple Instance Learning Framework with Masked Hard Instance Mining for Whole Slide Image Classification](http://arxiv.org/abs/2307.15254v1)<br>:star:[code](https://github.com/DearCaat/MHIM-MIL)
* 细胞核检测
  * [Affine-Consistent Transformer for Multi-Class Cell Nuclei Detection](https://openaccess.thecvf.com/content/ICCV2023/papers/Huang_Affine-Consistent_Transformer_for_Multi-Class_Cell_Nuclei_Detection_ICCV_2023_paper.pdf)
* X 射线
  * [MedKLIP: Medical Knowledge Enhanced Language-Image Pre-Training for X-ray Diagnosis](https://openaccess.thecvf.com/content/ICCV2023/papers/Wu_MedKLIP_Medical_Knowledge_Enhanced_Language-Image_Pre-Training_for_X-ray_Diagnosis_ICCV_2023_paper.pdf)
  * [BoMD: Bag of Multi-label Descriptors for Noisy Chest X-ray Classification](https://openaccess.thecvf.com/content/ICCV2023/papers/Chen_BoMD_Bag_of_Multi-label_Descriptors_for_Noisy_Chest_X-ray_Classification_ICCV_2023_paper.pdf)
* MRI 超分辨率
  * [Rethinking Multi-Contrast MRI Super-Resolution: Rectangle-Window Cross-Attention Transformer and Arbitrary-Scale Upsampling](https://openaccess.thecvf.com/content/ICCV2023/papers/Li_Rethinking_Multi-Contrast_MRI_Super-Resolution_Rectangle-Window_Cross-Attention_Transformer_and_Arbitrary-Scale_Upsampling_ICCV_2023_paper.pdf)
* 脑肿瘤分割
  * [Scratch Each Other's Back: Incomplete Multi-Modal Brain Tumor Segmentation via Category Aware Group Self-Support Learning](https://openaccess.thecvf.com/content/ICCV2023/papers/Qiu_Scratch_Each_Others_Back_Incomplete_Multi-Modal_Brain_Tumor_Segmentation_via_ICCV_2023_paper.pdf)
* 器官分割和肿瘤检测
  * [CLIP-Driven Universal Model for Organ Segmentation and Tumor Detection](http://arxiv.org/abs/2301.00785)

<a name="11"/>

## 11.Image/Video Editing(图像/视频编辑)
* 图像编辑
  * [Effective Real Image Editing with Accelerated Iterative Diffusion Inversion](http://arxiv.org/abs/2309.04907v1)
  * [Multimodal Garment Designer: Human-Centric Latent Diffusion Models for Fashion Image Editing](http://arxiv.org/abs/2304.02051)
* 视频编辑
  * [StableVideo: Text-driven Consistency-aware Diffusion Video Editing](http://arxiv.org/abs/2308.09592v1)<br>:star:[code](https://github.com/rese1f/StableVideo)

<a name="10"/>

## 10.Image Synthesis(图像合成)
* [Foreground Object Search by Distilling Composite Image Feature](http://arxiv.org/abs/2308.04990v1)<br>:star:[code](https://github.com/bcmi/Foreground-Object-Search-Dataset-FOSD)
* 图像生成
  * [MosaiQ: Quantum Generative Adversarial Networks for Image Generation on NISQ Computers](http://arxiv.org/abs/2308.11096v1)
  * [Ray Conditioning: Trading Photo-consistency for Photo-realism in Multi-view Image Generation](http://arxiv.org/abs/2304.13681)
  * [Both Diverse and Realism Matter: Physical Attribute and Style Alignment for Rainy Image Generation](https://openaccess.thecvf.com/content/ICCV2023/papers/Yu_Both_Diverse_and_Realism_Matter_Physical_Attribute_and_Style_Alignment_ICCV_2023_paper.pdf)
* 图像合成
  * [Conditional 360-degree Image Synthesis for Immersive Indoor Scene Decoration](http://arxiv.org/abs/2307.09621v1)
  * [SideGAN: 3D-Aware Generative Model for Improved Side-View Image Synthesis](http://arxiv.org/abs/2309.10388v1)
  * [VeRi3D: Generative Vertex-based Radiance Fields for 3D Controllable Human Image Synthesis](https://openaccess.thecvf.com/content/ICCV2023/papers/Ji_Anchor_Structure_Regularization_Induced_Multi-view_Subspace_Clustering_via_Enhanced_Tensor_ICCV_2023_paper.pdf)
* 文本-图像合成
  * [Dense Text-to-Image Generation with Attention Modulation](http://arxiv.org/abs/2308.12964v1)<br>:star:[code](https://github.com/naver-ai/DenseDiffusion)
  * [ITI-GEN: Inclusive Text-to-Image Generation](http://arxiv.org/abs/2309.05569v1)<br>:star:[code](https://czhang0528.github.io/iti-gen)
  * [BoxDiff: Text-to-Image Synthesis with Training-Free Box-Constrained Diffusion](http://arxiv.org/abs/2307.10816v1)<br>:star:[code](https://github.com/Sierkinhane/BoxDiff)
   * [Learning to Generate Semantic Layouts for Higher Text-Image Correspondence in Text-to-Image Synthesis](http://arxiv.org/abs/2308.08157v1)<br>:star:[code](https://pmh9960.github.io/research/GCDP)
   * [Unsupervised Compositional Concepts Discovery with Text-to-Image Generative Models](http://arxiv.org/abs/2306.05357)
   * [Rickrolling the Artist: Injecting Backdoors into Text Encoders for Text-to-Image Synthesis](http://arxiv.org/abs/2211.02408)
   * [Text-Conditioned Sampling Framework for Text-to-Image Generation with Masked Generative Models](http://arxiv.org/abs/2304.01515)
   * [Zero-Shot Spatial Layout Conditioning for Text-to-Image Diffusion Models](http://arxiv.org/abs/2306.13754)
* 音频驱动的图像生成
  * [Generating Realistic Images from In-the-wild Sounds](http://arxiv.org/abs/2309.02405v1)
* X-图像生成
  * [GlueGen: Plug and Play Multi-modal Encoders for X-to-image Generation](https://openaccess.thecvf.com/content/ICCV2023/papers/Qin_GlueGen_Plug_and_Play_Multi-modal_Encoders_for_X-to-image_Generation_ICCV_2023_paper.pdf)
* 扩散
  * [DPM-OT: A New Diffusion Probabilistic Model Based on Optimal Transport](http://arxiv.org/abs/2307.11308v1)<br>:star:[code](https://github.com/cognaclee/DPM-OT)
  * [Q-Diffusion: Quantizing Diffusion Models](https://openaccess.thecvf.com/content/ICCV2023/papers/Li_Q-Diffusion_Quantizing_Diffusion_Models_ICCV_2023_paper.pdf)
  * [Phasic Content Fusing Diffusion Model with Directional Distribution Consistency for Few-Shot Model Adaption](http://arxiv.org/abs/2309.03729v1)<br>:star:[code](https://github.com/sjtuplayer/few-shot-diffusion)
  * [DiffGuard: Semantic Mismatch-Guided Out-of-Distribution Detection using Pre-trained Diffusion Models](http://arxiv.org/abs/2308.07687v1)
  * [DiffDis: Empowering Generative Diffusion Model with Cross-Modal Discrimination Capability](http://arxiv.org/abs/2308.09306v1)
  * [Texture Generation on 3D Meshes with Point-UV Diffusion](http://arxiv.org/abs/2308.10490v1)<br>:star:[code](https://cvmi-lab.github.io/Point-UV-Diffusion)
  * [Stochastic Segmentation with Conditional Categorical Diffusion Models](http://arxiv.org/abs/2303.08888)
  * [DIFFGUARD: Semantic Mismatch-Guided Out-of-Distribution Detection Using Pre-Trained Diffusion Models](http://arxiv.org/abs/2308.07687)
  * [Erasing Concepts from Diffusion Models](http://arxiv.org/abs/2303.07345)
  * [A Complete Recipe for Diffusion Generative Models](https://openaccess.thecvf.com/content/ICCV2023/papers/Pandey_A_Complete_Recipe_for_Diffusion_Generative_Models_ICCV_2023_paper.pdf)
  * [SVDiff: Compact Parameter Space for Diffusion Fine-Tuning](http://arxiv.org/abs/2303.11305)
  * [TexFusion: Synthesizing 3D Textures with Text-Guided Image Diffusion Models](https://openaccess.thecvf.com/content/ICCV2023/papers/Cao_TexFusion_Synthesizing_3D_Textures_with_Text-Guided_Image_Diffusion_Models_ICCV_2023_paper.pdf)
* 3D形状生成
  * [3D Semantic Subspace Traverser: Empowering 3D Generative Model with Shape Editing Capability](http://arxiv.org/abs/2307.14051v1)<br>:star:[code](https://github.com/TrepangCat/3D_Semantic_Subspace_Traverser)
* 故事可视化
  * [Story Visualization by Online Text Augmentation with Context Memory](http://arxiv.org/abs/2308.07575v1)
* AIGC
  * [ReMoDiffuse: Retrieval-Augmented Motion Diffusion Model](https://arxiv.org/pdf/2304.01116.pdf)<br>:house:[project](https://mingyuan-zhang.github.io/projects/ReMoDiffuse.html)<br>:thumbsup:[ICCV 2023|重塑人体动作生成，融合扩散模型与检索策略的新范式ReMoDiffuse来了](https://mp.weixin.qq.com/s/IOuqsd0e6tj_qFnxpRPaEA)

<a name="9"/>

## 9.Image Classification(图像分类)
* [A step towards understanding why classification helps regression](http://arxiv.org/abs/2308.10603v1)
* [Image-free Classifier Injection for Zero-Shot Classification](http://arxiv.org/abs/2308.10599v1)<br>:star:[code](https://github.com/ExplainableML/ImageFreeZSL)
* [Label-Free Event-based Object Recognition via Joint Learning with Image Reconstruction from Events](http://arxiv.org/abs/2308.09383v1)<br>:star:[code](https://github.com/Chohoonhee/Ev-LaFOR)
* [ImbSAM: A Closer Look at Sharpness-Aware Minimization in Class-Imbalanced Recognition](http://arxiv.org/abs/2308.07815v1)<br>:star:[code](https://github.com/cool-xuan/Imbalanced_SAM)
* [Learning Concise and Descriptive Attributes for Visual Recognition](http://arxiv.org/abs/2308.03685v1)
* [Get the Best of Both Worlds: Improving Accuracy and Transferability by Grassmann Class Representation](http://arxiv.org/abs/2308.01547v1)<br>:star:[code](https://github.com/innerlee/GCR)
* [What do neural networks learn in image classification? A frequency shortcut perspective](http://arxiv.org/abs/2307.09829v1)
* 多标签图像分类
  * [CDUL: CLIP-Driven Unsupervised Learning for Multi-Label Image Classification](http://arxiv.org/abs/2307.16634v1)
  * [Learning in Imperfect Environment: Multi-Label Classification with Long-Tailed Distribution and Partial Labels](http://arxiv.org/abs/2304.10539)
* 细粒度识别
  * [Learning Gabor Texture Features for Fine-Grained Recognition](http://arxiv.org/abs/2308.05396v1)
* 长尾识别
  * [MDCS: More Diverse Experts with Consistency Self-distillation for Long-tailed Recognition](http://arxiv.org/abs/2308.09922v1)<br>:star:[code](https://github.com/fistyee/MDCS)
* 长尾分类
  * [AREA: Adaptive Reweighting via Effective Area for Long-Tailed Classification](https://openaccess.thecvf.com/content/ICCV2023/papers/Chen_AREA_Adaptive_Reweighting_via_Effective_Area_for_Long-Tailed_Classification_ICCV_2023_paper.pdf)

<a name="8"/>

## 8.Image Segmentation(图像分割)
* [SegGPT: Towards Segmenting Everything in Context](https://openaccess.thecvf.com/content/ICCV2023/papers/Wang_SegGPT_Towards_Segmenting_Everything_in_Context_ICCV_2023_paper.pdf)
* [Coarse-to-Fine Amodal Segmentation with Shape Prior](http://arxiv.org/abs/2308.16825v1)<br>:star:[code](http://jianxgao.github.io/C2F-Seg)
* [SegPrompt: Boosting Open-world Segmentation via Category-level Prompt Learning](http://arxiv.org/abs/2308.06531v1)<br>:star:[code](https://github.com/aim-uofa/SegPrompt)
* [Multi-interactive Feature Learning and a Full-time Multi-modality Benchmark for Image Fusion and Segmentation](https://arxiv.org/abs/2308.02097)<br>:star:[code](https://github.com/JinyuanLiu-CV/SegMiF)
* [Unmasking Anomalies in Road-Scene Segmentation](http://arxiv.org/abs/2307.13316v1)<br>:star:[code](https://github.com/shyam671/Mask2Anomaly-Unmasking-Anomalies-in-Road-Scene-Segmentation)
* [UniSeg: A Unified Multi-Modal LiDAR Segmentation Network and the OpenPCSeg Codebase](http://arxiv.org/abs/2309.05573v1)<br>:star:[code](https://github.com/PJLab-ADG/PCSeg)
* [3D Segmentation of Humans in Point Clouds with Synthetic Data](http://arxiv.org/abs/2212.00786)
* 指代图像分割
  * [Bridging Vision and Language Encoders: Parameter-Efficient Tuning for Referring Image Segmentation](http://arxiv.org/abs/2307.11545v1)<br>:star:[code](https://github.com/kkakkkka/ETRIS)
  * [Beyond One-to-One: Rethinking the Referring Image Segmentation](http://arxiv.org/abs/2308.13853v1)<br>:star:[code](https://github.com/toggle1995/RIS-DMMI)
  * [Referring Image Segmentation Using Text Supervision](http://arxiv.org/abs/2308.14575v1)<br>:star:[code](https://github.com/fawnliu/TRIS)
  * [Shatter and Gather: Learning Referring Image Segmentation with Text Supervision](http://arxiv.org/abs/2308.15512v1)
* 小样本分割
  * [Self-Calibrated Cross Attention Network for Few-Shot Segmentation](http://arxiv.org/abs/2308.09294v1)<br>:star:[code](https://github.com/Sam1224/SCCAN)
* 语义分割
  * [A Good Student is Cooperative and Reliable: CNN-Transformer Collaborative Learning for Semantic Segmentation](http://arxiv.org/abs/2307.12574v1)
  * [DiffuMask: Synthesizing Images with Pixel-level Annotations for Semantic Segmentation Using Diffusion Models](http://arxiv.org/abs/2303.11681)
  * [Informative Data Mining for One-Shot Cross-Domain Semantic Segmentation](http://arxiv.org/abs/2309.14241v1)<br>:star:[code](https://github.com/yxiwang/IDM)
  * [CMDA: Cross-Modality Domain Adaptation for Nighttime Semantic Segmentation](http://arxiv.org/abs/2307.15942v1)<br>:star:[code](https://github.com/XiaRho/CMDA)
  * [To Adapt or Not to Adapt? Real-Time Adaptation for Semantic Segmentation](http://arxiv.org/abs/2307.15063v1)<br>:star:[code](https://marcbotet.github.io/hamlet-web/)
  * [Look at the Neighbor: Distortion-aware Unsupervised Domain Adaptation for Panoramic Semantic Segmentation](http://arxiv.org/abs/2308.05493v1)
  * [SVQNet: Sparse Voxel-Adjacent Query Network for 4D Spatio-Temporal LiDAR Semantic Segmentation](http://arxiv.org/abs/2308.13323v1)
  * [Preparing the Future for Continual Semantic Segmentation](https://openaccess.thecvf.com/content/ICCV2023/papers/Lin_Preparing_the_Future_for_Continual_Semantic_Segmentation_ICCV_2023_paper.pdf)
  * 点云语义分割
    * [Retro-FPN: Retrospective Feature Pyramid Network for Point Cloud Semantic Segmentation](http://arxiv.org/abs/2308.09314v1)<br>:star:[code](https://github.com/AllenXiangX/Retro-FPN)
  * 小样本语义分割
    * [Prototypical Kernel Learning and Open-set Foreground Perception for Generalized Few-shot Semantic Segmentation](http://arxiv.org/abs/2308.04952v1)
  * 半监督语义分割
    * [Space Engage: Collaborative Space Supervision for Contrastive-based Semi-Supervised Semantic Segmentation](http://arxiv.org/abs/2307.09755v1)
    * [Logic-induced Diagnostic Reasoning for Semi-supervised Semantic Segmentation](http://arxiv.org/abs/2308.12595v1)<br>:star:[code](https://github.com/leonnnop/LogicDiag)
    * [Enhanced Soft Label for Semi-Supervised Semantic Segmentation](https://openaccess.thecvf.com/content/ICCV2023/papers/Ma_Enhanced_Soft_Label_for_Semi-Supervised_Semantic_Segmentation_ICCV_2023_paper.pdf)
  * 域适应语义分割
    * [Diffusion-based Image Translation with Label Guidance for Domain Adaptive Semantic Segmentation](http://arxiv.org/abs/2308.12350v1)
  * 开放词汇语义分割
    * [Exploring Open-Vocabulary Semantic Segmentation from CLIP Vision Encoder Distillation Only](https://openaccess.thecvf.com/content/ICCV2023/papers/Chen_Exploring_Open-Vocabulary_Semantic_Segmentation_from_CLIP_Vision_Encoder_Distillation_Only_ICCV_2023_paper.pdf)
  * 3D 语义分割
    * [BEV-DG: Cross-Modal Learning under Bird's-Eye View for Domain Generalization of 3D Semantic Segmentation](http://arxiv.org/abs/2308.06530v1)
* 实例分割
  * [Mask-Attention-Free Transformer for 3D Instance Segmentation](http://arxiv.org/abs/2309.01692v1)<br>:star:[code](https://github.com/dvlab-research/Mask-Attention-Free-Transformer)
  * [WaterMask: Instance Segmentation for Underwater Imagery](https://openaccess.thecvf.com/content/ICCV2023/papers/Lian_WaterMask_Instance_Segmentation_for_Underwater_Imagery_ICCV_2023_paper.pdf)
  * 半监督实例分割
    * [Pseudo-label Alignment for Semi-supervised Instance Segmentation](http://arxiv.org/abs/2308.05359v1)<br>:star:[code](https://github.com/hujiecpp/PAIS)
  * 开放世界实例分割
    * [Exploring Transformers for Open-world Instance Segmentation](http://arxiv.org/abs/2308.04206v1)
* 全景分割
  * [Towards Deeply Unified Depth-aware Panoptic Segmentation with Bi-directional Guidance Learning](http://arxiv.org/abs/2307.14786v1)
  * [EDAPS: Enhanced Domain-Adaptive Panoptic Segmentation](http://arxiv.org/abs/2304.14291)
  * [Point2Mask: Point-supervised Panoptic Segmentation via Optimal Transport](http://arxiv.org/abs/2308.01779v1)<br>:star:[code](https://github.com/LiWentomng/Point2Mask)
  * [LiDAR-Camera Panoptic Segmentation via Geometry-Consistent and Semantic-Aware Alignment](http://arxiv.org/abs/2308.01686v1)<br>:star:[code](https://github.com/zhangzw12319/lcps.git)
* VIS
  * [CTVIS: Consistent Training for Online Video Instance Segmentation](http://arxiv.org/abs/2307.12616v1)<br>:star:[code](https://github.com/KainingYing/CTVIS)
  * [DVIS: Decoupled Video Instance Segmentation Framework](https://arxiv.org/pdf/2306.03413.pdf)<br>:star:[code](https://github.com/zhang-tao-whu/DVIS)<br>:thumbsup:[ICCV 2023 | 发挥offline方法的潜力，武大&快手提出解耦合的视频实例分割框架DVIS](https://mp.weixin.qq.com/s/_MlryCfg_rRMZMfgINPwXw)
  * [TCOVIS: Temporally Consistent Online Video Instance Segmentation](http://arxiv.org/abs/2309.11857v1)<br>:star:[code](https://github.com/jun-long-li/TCOVIS)
* VOS
  * [Spectrum-guided Multi-granularity Referring Video Object Segmentation](http://arxiv.org/abs/2307.13537v1)<br>:star:[code](https://github.com/bo-miao/SgMg)
  * [Multi-grained Temporal Prototype Learning for Few-shot Video Object Segmentation](http://arxiv.org/abs/2309.11160v1)<br>:star:[code](https://github.com/nankepan/VIPMT)
  * [Temporal Collection and Distribution for Referring Video Object Segmentation](http://arxiv.org/abs/2309.03473v1)<br>:star:[code](https://toneyaya.github.io/tempcd/)
  * [OnlineRefer: A Simple Online Baseline for Referring Video Object Segmentation](https://arxiv.org/abs/2307.09356)<br>:star:[code](https://github.com/wudongming97/OnlineRefer)
  * [Isomer: Isomerous Transformer for Zero-shot Video Object Segmentation](http://arxiv.org/abs/2308.06693v1)<br>:star:[code](https://github.com/DLUT-yyc/Isomer)
  * [Scalable Video Object Segmentation with Simplified Framework](http://arxiv.org/abs/2308.09903v1)
  * [Learning Cross-Modal Affinity for Referring Video Object Segmentation Targeting Limited Samples](http://arxiv.org/abs/2309.02041v1)<br>:star:[code](https://github.com/hengliusky/Few_shot_RVOS)
* 动作分割
  * [How Much Temporal Long-Term Context is Needed for Action Segmentation?](http://arxiv.org/abs/2308.11358v1)
* 基于文本的图像分割
  * [LD-ZNet: A Latent Diffusion Approach for Text-Based Image Segmentation](https://openaccess.thecvf.com/content/ICCV2023/papers/PNVR_LD-ZNet_A_Latent_Diffusion_Approach_for_Text-Based_Image_Segmentation_ICCV_2023_paper.pdf)

<a name="7"/>

## 7.Image Progress(低层图像处理、质量评价)
* [DRAW: Defending Camera-shooted RAW against Image Manipulation](http://arxiv.org/abs/2307.16418v1)
* [Towards Generic Image Manipulation Detection with Weakly-Supervised Self-Consistency Learning](http://arxiv.org/abs/2309.01246v1)<br>:star:[code](https://github.com/yhZhai/WSCL)
* [Improving Lens Flare Removal with General Purpose Pipeline and Multiple Light Sources Recovery](http://arxiv.org/abs/2308.16460v1)
* 图像恢复
  * [Physics-Driven Turbulence Image Restoration with Stochastic Refinement](http://arxiv.org/abs/2307.10603v1)<br>:star:[code](https://github.com/VITA-Group/PiRN)
  * [Under-Display Camera Image Restoration with Scattering Effect](http://arxiv.org/abs/2308.04163v1)<br>:star:[code](https://github.com/NamecantbeNULL/SRUDC)
  * [Learning Image-Adaptive Codebooks for Class-Agnostic Image Restoration](http://arxiv.org/abs/2306.06513)
* 图像修复
  * [Rethinking Fast Fourier Convolution in Image Inpainting](https://openaccess.thecvf.com/content/ICCV2023/papers/Chu_Rethinking_Fast_Fourier_Convolution_in_Image_Inpainting_ICCV_2023_paper.pdf)
* 图像/视频增强
  * [Lighting up NeRF via Unsupervised Decomposition and Enhancement](http://arxiv.org/abs/2307.10664v1)<br>:house:[project](https://whyy.site/paper/llnerf)
  * [Implicit Neural Representation for Cooperative Low-light Image Enhancement](https://arxiv.org/pdf/2303.11722.pdf)<br>:star:[code](https://github.com/Ysz2022/NeRCo)<br>:thumbsup:[ICCV2023 | 将隐式神经表征用于“低光增强”，北大张健团队提出NeRCo](https://mp.weixin.qq.com/s/nkalW0aZqkoIZHW5SX91Ag)
  * [Low-Light Image Enhancement with Illumination-Aware Gamma Correction and Complete Image Modelling Network](http://arxiv.org/abs/2308.08220v1)
  * [Diff-Retinex: Rethinking Low-light Image Enhancement with A Generative Diffusion Model](http://arxiv.org/abs/2308.13164v1)
  * [Empowering Low-Light Image Enhancer through Customized Learnable Priors](http://arxiv.org/abs/2309.01958v1)<br>:star:[code](https://github.com/zheng980629/CUE)
  * [ExposureDiffusion: Learning to Expose for Low-light Image Enhancement](http://arxiv.org/abs/2307.07710)
  * [NIR-assisted Video Enhancement via Unpaired 24-hour Data](https://openaccess.thecvf.com/content/ICCV2023/papers/Niu_NIR-assisted_Video_Enhancement_via_Unpaired_24-hour_Data_ICCV_2023_paper.pdf)
* 图像去雨
  * [Sparse Sampling Transformer with Uncertainty-Driven Ranking for Unified Removal of Raindrops and Rain Streaks](http://arxiv.org/abs/2308.14153v1)
* 图像去噪
  * [Random Sub-Samples Generation for Self-Supervised Real Image Denoising](http://arxiv.org/abs/2307.16825v1)<br>:star:[code](https://github.com/p1y2z3/SDAP)
  * [Lighting Every Darkness in Two Pairs: A Calibration-Free Pipeline for RAW Denoising](http://arxiv.org/abs/2308.03448v1)<br>:star:[code](https://github.com/Srameo/LED)
  * [Score Priors Guided Deep Variational Inference for Unsupervised Real-World Single Image Denoising](http://arxiv.org/abs/2308.04682v1)
  * [Unsupervised Image Denoising in Real-World Scenarios via Self-Collaboration Parallel Generative Adversarial Branches](http://arxiv.org/abs/2308.06776v1)
  * [Multi-view Self-supervised Disentanglement for General Image Denoising](http://arxiv.org/abs/2309.05049v1)
  * [RED-PSM: Regularization by Denoising of Partially Separable Models for Dynamic Imaging](https://openaccess.thecvf.com/content/ICCV2023/papers/Iskender_RED-PSM_Regularization_by_Denoising_of_Partially_Separable_Models_for_Dynamic_ICCV_2023_paper.pdf)
* 图像去雾
  * [MB-TaylorFormer: Multi-branch Efficient Transformer Expanded by Taylor Formula for Image Dehazing](https://arxiv.org/abs/2308.14036)<br>:star:[code](https://github.com/FVL2020/ICCV-2023-MB-TaylorFormer)<br>:thumbsup:[ICCV2023 | 更快、更灵活的 Transformer图像去雾网络](https://mp.weixin.qq.com/s/nZ9WFBci0WyeBe1Fyi3rnA)
* 图像去除阴影
  * [Boundary-Aware Divide and Conquer: A Diffusion-Based Solution for Unsupervised Shadow Removal](https://openaccess.thecvf.com/content/ICCV2023/papers/Guo_Boundary-Aware_Divide_and_Conquer_A_Diffusion-Based_Solution_for_Unsupervised_Shadow_ICCV_2023_paper.pdf)
* 图像/视频去模糊
  * [Exploring Temporal Frequency Spectrum in Deep Video Deblurring](https://openaccess.thecvf.com/content/ICCV2023/papers/Zhu_Exploring_Temporal_Frequency_Spectrum_in_Deep_Video_Deblurring_ICCV_2023_paper.pdf)
* 图像/视频区摩尔纹
  * [Deep Video Demoireing via Compact Invertible Dyadic Decomposition](https://openaccess.thecvf.com/content/ICCV2023/papers/Quan_Deep_Video_Demoireing_via_Compact_Invertible_Dyadic_Decomposition_ICCV_2023_paper.pdf)
* 去马赛克/去鬼影
  * [Joint Demosaicing and Deghosting of Time-Varying Exposures for Single-Shot HDR Imaging](https://openaccess.thecvf.com/content/ICCV2023/papers/Kim_Joint_Demosaicing_and_Deghosting_of_Time-Varying_Exposures_for_Single-Shot_HDR_ICCV_2023_paper.pdf)
* 质量评估
  * [Test Time Adaptation for Blind Image Quality Assessment](http://arxiv.org/abs/2307.14735v1)
* 图像和谐化
  * [Learning Global-aware Kernel for Image Harmonization](https://arxiv.org/pdf/2305.11676.pdf)
  * [Deep Image Harmonization with Learnable Augmentation](http://arxiv.org/abs/2308.00376v1)<br>:star:[code](https://github.com/bcmi/SycoNet-Adaptive-Image-Harmonization)
  * [Deep Image Harmonization with Globally Guided Feature Transformation and Relation Distillation](http://arxiv.org/abs/2308.00356v1)<br>:star:[code](https://github.com/bcmi/Image-Harmonization-Dataset-ccHarmony)
* 图像校正
  * [SimFIR: A Simple Framework for Fisheye Image Rectification with Self-supervised Representation Learning](http://arxiv.org/abs/2308.09040v1)
* 图像拼接
  * [Parallax-Tolerant Unsupervised Deep Image Stitching](http://arxiv.org/abs/2302.08207)
* 图像着色
  * [DDColor: Towards Photo-Realistic Image Colorization via Dual Decoders](http://arxiv.org/abs/2212.11613)

<a name="6"/>

## 6.Face(人脸)
* [Efficient Region-Aware Neural Radiance Fields for High-Fidelity Talking Portrait Synthesis](http://arxiv.org/abs/2307.09323)
* [UniFace: Unified Cross-Entropy Loss for Deep Face Recognition](https://openaccess.thecvf.com/content/ICCV2023/papers/Zhou_UniFace_Unified_Cross-Entropy_Loss_for_Deep_Face_Recognition_ICCV_2023_paper.pdf)
* [Human-Inspired Facial Sketch Synthesis with Dynamic Adaptation](http://arxiv.org/abs/2309.00216v1)<br>:star:[code](https://github.com/AiArt-HDU/HIDA)
* [Can Language Models Learn to Listen?](http://arxiv.org/abs/2308.10897v1)<br>:house:[project](https://people.eecs.berkeley.edu/~evonne_ng/projects/text2listen/)
* 说话头合成
  * [Implicit Identity Representation Conditioned Memory Compensation Network for Talking Head video Generation](https://arxiv.org/abs/2307.09906)<br>:star:[code](https://github.com/harlanhong/ICCV2023-MCNET)
  * [Efficient Emotional Adaptation for Audio-Driven Talking-Head Generation](http://arxiv.org/abs/2309.04946v1)<br>:star:[code](https://yuangan.github.io/eat/)
* 说话人脸合成
  * [EMMN: Emotional Motion Memory Network for Audio-driven Emotional Talking Face Generation](https://openaccess.thecvf.com/content/ICCV2023/papers/Tan_EMMN_Emotional_Motion_Memory_Network_for_Audio-driven_Emotional_Talking_Face_ICCV_2023_paper.pdf)
* 人脸交换
  * [BlendFace: Re-designing Identity Encoders for Face-Swapping](http://arxiv.org/abs/2307.10854v1)<br>:star:[code](https://github.com/mapooon/BlendFace)<br>:star:[code](https://mapooon.github.io/BlendFacePage/)
* 假脸检测
  * [Controllable Guide-Space for Generalizable Face Forgery Detection](http://arxiv.org/abs/2307.14039v1)
* 人脸再现
  * [HyperReenact: One-Shot Reenactment via Jointly Learning to Refine and Retarget Faces](http://arxiv.org/abs/2307.10797v1)<br>:star:[code](https://github.com/StelaBou/HyperReenact)<br>:star:[code](https://stelabou.github.io/hyperreenact.github.io/)
* 文本驱动的人脸处理
  * [FaceCLIPNeRF: Text-driven 3D Face Manipulation using Deformable Neural Radiance Fields](http://arxiv.org/abs/2307.11418v1)
* 人脸表情
  * [GaFET: Learning Geometry-aware Facial Expression Translation from In-The-Wild Images](http://arxiv.org/abs/2308.03413v1)
  * [Latent-OFER: Detect, Mask, and Reconstruct with Latent Vectors for Occluded Facial Expression Recognition](https://openaccess.thecvf.com/content/ICCV2023/papers/Lee_Latent-OFER_Detect_Mask_and_Reconstruct_with_Latent_Vectors_for_Occluded_ICCV_2023_paper.pdf)
* 人脸识别
  * [IDiff-Face: Synthetic-based Face Recognition through Fizzy Identity-Conditioned Diffusion Models](http://arxiv.org/abs/2308.04995v1)
  * [Benchmarking Algorithmic Bias in Face Recognition: An Experimental Approach Using Synthetic Faces and Human Evaluation](http://arxiv.org/abs/2308.05441v1)
  * [TransFace: Calibrating Transformer Training for Face Recognition from a Data-Centric Perspective](http://arxiv.org/abs/2308.10133v1)<br>:star:[code](https://github.com/DanJun6737/TransFace)
  * [Privacy-Preserving Face Recognition Using Random Frequency Components](http://arxiv.org/abs/2308.10461v1)<br>:star:[code](https://github.com/Tencent/TFace)
* 人脸聚类
  * [Face Clustering via Graph Convolutional Networks with Confidence Edges](https://openaccess.thecvf.com/content/ICCV2023/papers/Wu_Face_Clustering_via_Graph_Convolutional_Networks_with_Confidence_Edges_ICCV_2023_paper.pdf)
* 人脸合成
  * [LPFF: A Portrait Dataset for Face Generators Across Large Poses](http://arxiv.org/abs/2303.14407)
* 3D 人脸
  * [Unpaired Multi-domain Attribute Translation of 3D Facial Shapes with a Square and Symmetric Geometric Map](http://arxiv.org/abs/2308.13245)
  * 3D人脸合成
    * [Towards High-Fidelity Text-Guided 3D Face Generation and Manipulation Using only Images](http://arxiv.org/abs/2308.16758v1)
  * 3D 人脸动画
    * [EmoTalk: Speech-Driven Emotional Disentanglement for 3D Face Animation](http://arxiv.org/abs/2303.11089)
  * 3D 人脸重建
    * [Template Inversion Attack against Face Recognition Systems using 3D Face Reconstruction](https://openaccess.thecvf.com/content/ICCV2023/papers/Shahreza_Template_Inversion_Attack_against_Face_Recognition_Systems_using_3D_Face_ICCV_2023_paper.pdf)

<a name="5"/>

## 5.Biometric Recognition(生物特征识别)
* 掌纹识别
  * [RPG-Palm: Realistic Pseudo-data Generation for Palmprint Recognition](https://arxiv.org/abs/2307.14016)<br>:star:[code](https://github.com/RayshenSL/RPG-PALM)

<a name="4"/>

## 4.Object Tracking(目标跟踪)
* [Humans in 4D: Reconstructing and Tracking Humans with Transformers](http://arxiv.org/abs/2305.20091)
* 多目标跟踪
  * [MeMOTR: Long-Term Memory-Augmented Transformer for Multi-Object Tracking](http://arxiv.org/abs/2307.15700v1)<br>:star:[code](https://github.com/MCG-NJU/MeMOTR)
  * [Uncertainty-aware Unsupervised Multi-Object Tracking](http://arxiv.org/abs/2307.15409v1)
  * [TrackFlow: Multi-Object Tracking with Normalizing Flows](http://arxiv.org/abs/2308.11513v1)
  * [3DMOTFormer: Graph Transformer for Online 3D Multi-Object Tracking](http://arxiv.org/abs/2308.06635v1)<br>:star:[code](https://github.com/dsx0511/3DMOTFormer)
  * [ReST: A Reconfigurable Spatial-Temporal Graph Model for Multi-Camera Multi-Object Tracking](http://arxiv.org/abs/2308.13229v1)
  * [Object-Centric Multiple Object Tracking](http://arxiv.org/abs/2309.00233v1)
  * [Heterogeneous Diversity Driven Active Learning for Multi-Object Tracking](https://openaccess.thecvf.com/content/ICCV2023/papers/Li_Heterogeneous_Diversity_Driven_Active_Learning_for_Multi-Object_Tracking_ICCV_2023_paper.pdf)
* 视觉跟踪
  * [Exploring Lightweight Hierarchical Vision Transformers for Efficient Visual Tracking](http://arxiv.org/abs/2308.06904v1)
  * [CiteTracker: Correlating Image and Text for Visual Tracking](http://arxiv.org/abs/2308.11322v1)
  * [Robust Object Modeling for Visual Tracking](http://arxiv.org/abs/2308.05140v1)<br>:star:[code](https://github.com/dawnyc/ROMTrack)
  * [PVT++: A Simple End-to-End Latency-Aware Visual Tracking Framework](https://openaccess.thecvf.com/content/ICCV2023/papers/Li_PVT_A_Simple_End-to-End_Latency-Aware_Visual_Tracking_Framework_ICCV_2023_paper.pdf)
  * [Integrating Boxes and Masks: A Multi-Object Framework for Unified Visual Tracking and Segmentation](http://arxiv.org/abs/2308.13266v1)<br>:star:[code](https://github.com/yoxu515/MITS)
  * [Foreground-Background Distribution Modeling Transformer for Visual Object Tracking](https://openaccess.thecvf.com/content/ICCV2023/papers/Yang_Foreground-Background_Distribution_Modeling_Transformer_for_Visual_Object_Tracking_ICCV_2023_paper.pdf)
* 3D目标跟踪
  * [Delving into Motion-Aware Matching for Monocular 3D Object Tracking](http://arxiv.org/abs/2308.11607v1)<br>:star:[code](https://github.com/kuanchihhuang/MoMA-M3T)
  * [Synchronize Feature Extracting and Matching: A Single Branch Framework for 3D Object Tracking](http://arxiv.org/abs/2308.12549v1)
  * [MixCycle: Mixup Assisted Semi-Supervised 3D Single Object Tracking with Cycle Consistency](http://arxiv.org/abs/2303.09219)

<a name="3"/>

## 3.Object Detection(目标检测)
* [Shift from Texture-bias to Shape-bias: Edge Deformation-based Augmentation for Robust Object Recognition](https://openaccess.thecvf.com/content/ICCV2023/papers/He_Shift_from_Texture-bias_to_Shape-bias_Edge_Deformation-based_Augmentation_for_Robust_ICCV_2023_paper.pdf)
* [Periodically Exchange Teacher-Student for Source-Free Object Detection](https://openaccess.thecvf.com/content/ICCV2023/papers/Liu_Periodically_Exchange_Teacher-Student_for_Source-Free_Object_Detection_ICCV_2023_paper.pdf)
* [CoTDet: Affordance Knowledge Prompting for Task Driven Object Detection](http://arxiv.org/abs/2309.01093v1)
* [A Dynamic Dual-Processing Object Detection Framework Inspired by the Brain's Recognition Mechanism](https://openaccess.thecvf.com/content/ICCV2023/papers/Zhang_A_Dynamic_Dual-Processing_Object_Detection_Framework_Inspired_by_the_Brains_ICCV_2023_paper.pdf)
* [ASAG: Building Strong One-Decoder-Layer Sparse Detectors via Adaptive Sparse Anchor Generation](http://arxiv.org/abs/2308.09242v1)<br>:star:[code](https://github.com/iSEE-Laboratory/ASAG)
* [FeatEnHancer: Enhancing Hierarchical Features for Object Detection and Beyond Under Low-Light Vision](http://arxiv.org/abs/2308.03594v1)
* [FB-BEV: BEV Representation from Forward-Backward View Transformations](https://arxiv.org/abs/2308.02236)<br>:star:[code](https://github.com/NVlabs/FB-BEV)
* [DETR Doesn't Need Multi-Scale or Locality Design](http://arxiv.org/abs/2308.01904v1)<br>:star:[code](https://github.com/impiga/Plain-DETR)
* [RecursiveDet: End-to-End Region-based Recursive Object Detection](http://arxiv.org/abs/2307.13619v1)<br>:star:[code](https://github.com/bravezzzzzz/RecursiveDet)
* [Less is More: Focus Attention for Efficient DETR](http://arxiv.org/abs/2307.12612v1)<br>:star:[code](https://github.com/huawei-noah/noah-research/tree/master/Focus-DETR)<br>:house:[project](https://gitee.com/mindspore/models/tree/master/research/cv/Focus-DETR)
* [Spatial Self-Distillation for Object Detection with Inaccurate Bounding Boxes](http://arxiv.org/abs/2307.12101v1)<br>:star:[code](https://github.com/ucas-vg/PointTinyBenchmark/tree/SSD-Det)
* [AlignDet: Aligning Pre-training and Fine-tuning in Object Detection](http://arxiv.org/abs/2307.11077v1)<br>:star:[code](https://liming-ai.github.io/AlignDet)
* [Augmented Box Replay: Overcoming Foreground Shift for Incremental Object Detection](http://arxiv.org/abs/2307.12427v1)
* [Deep Directly-Trained Spiking Neural Networks for Object Detection](http://arxiv.org/abs/2307.11411v1)
* [Cascade-DETR: Delving into High-Quality Universal Object Detection](http://arxiv.org/abs/2307.11035v1)<br>:star:[code](https://github.com/SysCV/cascade-detr)
* [Object-aware Gaze Target Detection](http://arxiv.org/abs/2307.09662v1)<br>:star:[code](https://github.com/francescotonini/object-aware-gaze-target-detection)
* 3D OD
  * [PG-RCNN: Semantic Surface Point Generation for 3D Object Detection](http://arxiv.org/abs/2307.12637v1)<br>:star:[code](https://github.com/quotation2520/PG-RCNN)
  * [SupFusion: Supervised LiDAR-Camera Fusion for 3D Object Detection](http://arxiv.org/abs/2309.07084v1)
  * [ObjectFusion: Multi-modal 3D Object Detection with Object-Centric Fusion](https://openaccess.thecvf.com/content/ICCV2023/papers/Cai_ObjectFusion_Multi-modal_3D_Object_Detection_with_Object-Centric_Fusion_ICCV_2023_paper.pdf)
  * [Predict to Detect: Prediction-guided 3D Object Detection using Sequential Images](http://arxiv.org/abs/2306.08528)
  * [A Simple Vision Transformer for Weakly Semi-supervised 3D Object Detection](https://openaccess.thecvf.com/content/ICCV2023/papers/Zhang_A_Simple_Vision_Transformer_for_Weakly_Semi-supervised_3D_Object_Detection_ICCV_2023_paper.pdf)
  * [A Fast Unified System for 3D Object Detection and Tracking](https://openaccess.thecvf.com/content/ICCV2023/papers/Heitzinger_A_Fast_Unified_System_for_3D_Object_Detection_and_Tracking_ICCV_2023_paper.pdf)
  * [3DPPE: 3D Point Positional Encoding for Transformer-based Multi-Camera 3D Object Detection](https://openaccess.thecvf.com/content/ICCV2023/papers/Shu_3DPPE_3D_Point_Positional_Encoding_for_Transformer-based_Multi-Camera_3D_Object_ICCV_2023_paper.pdf)
  * [PARTNER: Level up the Polar Representation for LiDAR 3D Object Detection](http://arxiv.org/abs/2308.03982v1)
  * [SHIFT3D: Synthesizing Hard Inputs For Tricking 3D Detectors](http://arxiv.org/abs/2309.05810v1)
  * [ImGeoNet: Image-induced Geometry-aware Voxel Representation for Multi-view 3D Object Detection](http://arxiv.org/abs/2308.09098v1)<br>:star:[code](https://ttaoretw.github.io/imgeonet/)
  * [MonoNeRD: NeRF-like Representations for Monocular 3D Object Detection](http://arxiv.org/abs/2308.09421v1)<br>:star:[code](https://github.com/cskkxjk/MonoNeRD)
  * [SparseBEV: High-Performance Sparse 3D Object Detection from Multi-Camera Videos](http://arxiv.org/abs/2308.09244v1)<br>:star:[code](https://github.com/MCG-NJU/SparseBEV)
  * [QD-BEV : Quantization-aware View-guided Distillation for Multi-view 3D Object Detection](http://arxiv.org/abs/2308.10515v1)
  * [Representation Disparity-aware Distillation for 3D Object Detection](http://arxiv.org/abs/2308.10308v1)
  * [GPA-3D: Geometry-aware Prototype Alignment for Unsupervised Domain Adaptive 3D Object Detection from Point Clouds](http://arxiv.org/abs/2308.08140v1)<br>:star:[code](https://github.com/Liz66666/GPA3D)
  * [FocalFormer3D : Focusing on Hard Instance for 3D Object Detection](http://arxiv.org/abs/2308.04556v1)<br>:star:[code](https://github.com/NVlabs/FocalFormer3D)
  * [NeRF-Det: Learning Geometry-Aware Volumetric Representation for Multi-View 3D Object Detection](http://arxiv.org/abs/2307.14620v1)<br>:star:[code](https://github.com/facebookresearch/NeRF-Det)
  * [Exploring Object-Centric Temporal Modeling for Efficient Multi-View 3D Object Detection](https://arxiv.org/pdf/2303.11926.pdf)<br>:star:[code](https://github.com/exiawsh/StreamPETR)
  * [Cross Modal Transformer: Towards Fast and Robust 3D Object Detection](https://arxiv.org/pdf/2301.01283.pdf)<br>:star:[code](https://github.com/junjie18/CMT)
  * [CoIn: Contrastive Instance Feature Mining for Outdoor 3D Object Detection with Very Limited Annotations](https://openaccess.thecvf.com/content/ICCV2023/papers/Xia_CoIn_Contrastive_Instance_Feature_Mining_for_Outdoor_3D_Object_Detection_ICCV_2023_paper.pdf)
* 开放词汇目标检测
  * [EdaDet: Open-Vocabulary Object Detection Using Early Dense Alignment](http://arxiv.org/abs/2309.01151v1)<br>:star:[code](https://chengshiest.github.io/edadet)
* 弱监督目标检测
  * [ALWOD: Active Learning for Weakly-Supervised Object Detection](http://arxiv.org/abs/2309.07914v1)<br>:star:[code](https://github.com/seqam-lab/ALWOD)
* 密集目标检测
  * [Bridging Cross-task Protocol Inconsistency for Distillation in Dense Object Detection](http://arxiv.org/abs/2308.14286v1)<br>:star:[code](https://github.com/TinyTigerPan/BCKD)
* 视频目标检测
  * [Objects do not disappear: Video object detection by single-frame object location anticipation](http://arxiv.org/abs/2308.04770v1)<br>:star:[code](https://github.com/L-KID/Videoobject-detection-by-location-anticipation)
  * [Identity-Consistent Aggregation for Video Object Detection](http://arxiv.org/abs/2308.07737v1)
* 小目标检测
  * [Small Object Detection via Coarse-to-fine Proposal Generation and Imitation Learning](http://arxiv.org/abs/2308.09534v1)<br>:star:[code](https://github.com/shaunyuan22/CFINet)
* 目标定位
  * [Generative Prompt Model for Weakly Supervised Object Localization](http://arxiv.org/abs/2307.09756v1)<br>:star:[code](https://github.com/callsys/GenPromp)
  * [Unsupervised Object Localization with Representer Point Selection](http://arxiv.org/abs/2309.04172v1)
  * [EgoLoc: Revisiting 3D Object Localization from Egocentric Videos with Visual Queries](http://arxiv.org/abs/2212.06969)
  * [Unsupervised Open-Vocabulary Object Localization in Videos](http://arxiv.org/abs/2309.09858v1)
* 影子检测
  * [SILT: Shadow-aware Iterative Label Tuning for Learning to Detect Shadows from Noisy Labels](http://arxiv.org/abs/2308.12064v1)

<a name="2"/>

## 2.3D(三维重建\三维视觉)
* [Semantify: Simplifying the Control of 3D Morphable Models Using CLIP](http://arxiv.org/abs/2308.07415)
* [Vox-E: Text-Guided Voxel Editing of 3D Objects](https://openaccess.thecvf.com/content/ICCV2023/papers/Sella_Vox-E_Text-Guided_Voxel_Editing_of_3D_Objects_ICCV_2023_paper.pdf)
* [Tiled Multiplane Images for Practical 3D Photography](http://arxiv.org/abs/2309.14291v1)
* [DeFormer: Integrating Transformers with Deformable Models for 3D Shape Abstraction from a Single Image](http://arxiv.org/abs/2309.12594v1)
* [HoloFusion: Towards Photo-realistic 3D Generative Modeling](http://arxiv.org/abs/2308.14244v1)<br>:star:[code](https://holodiffusion.github.io/holofusion)
* [PlaneRecTR: Unified Query learning for 3D Plane Recovery from a Single View](http://arxiv.org/abs/2307.13756v1)<br>:house:[project](https://youtu.be/YBB7totHGJg)<br>:star:[code](https://github.com/SJingjia/PlaneRecTR)
* [OmnimatteRF: Robust Omnimatte with 3D Background Modeling](http://arxiv.org/abs/2309.07749v1)<br>:star:[code](https://omnimatte-rf.github.io/)
* 三维重建
  * [Metric3D: Towards Zero-shot Metric 3D Prediction from A Single Image](http://arxiv.org/abs/2307.10984v1)<br>:star:[code](https://github.com/YvanYin/Metric3D)
  * [LIST: Learning Implicitly from Spatial Transformers for Single-View 3D Reconstruction](http://arxiv.org/abs/2307.12194v1)
  * [Long-Range Grouping Transformer for Multi-View 3D Reconstruction](http://arxiv.org/abs/2308.08724v1)<br>:star:[code](https://github.com/LiyingCV/Long-Range-Grouping-Transformer)
  * [Doppelgangers: Learning to Disambiguate Images of Similar Structures](http://arxiv.org/abs/2309.02420v1)<br>:star:[code](http://doppelgangers-3d.github.io/)
  * [Iterative Superquadric Recomposition of 3D Objects from Multiple Views](http://arxiv.org/abs/2309.02102v1)<br>:star:[code](https://github.com/ExplainableML/ISCO)
  * [CHORUS: Learning Canonicalized 3D Human-Object Spatial Relations from Unbounded Synthesized Images](http://arxiv.org/abs/2308.12288v1)<br>:star:[code](https://jellyheadandrew.github.io/projects/chorus)
  * [PlankAssembly: Robust 3D Reconstruction from Three Orthographic Views with Learnt Shape Programs](http://arxiv.org/abs/2308.05744v1)<br>:star:[code](https://manycore-research.github.io/PlankAssembly)
  * [Coordinate Quantized Neural Implicit Representations for Multi-view Reconstruction](http://arxiv.org/abs/2308.11025v1)<br>:star:[code](https://github.com/MachinePerceptionLab/CQ-NIR)
  * [Root Pose Decomposition Towards Generic Non-rigid 3D Reconstruction with Monocular Videos](http://arxiv.org/abs/2308.10089v1)<br>:star:[code](https://rpd-share.github.io)
  * [R3D3: Dense 3D Reconstruction of Dynamic Scenes from Multiple Cameras](http://arxiv.org/abs/2308.14713v1)<br>:house:[project](https://www.vis.xyz/pub/r3d3/)
  * 三维场景重建
    * [FrozenRecon: Pose-free 3D Scene Reconstruction with Frozen Depth Models](http://arxiv.org/abs/2308.05733v1)<br>:star:[code](https://aim-uofa.github.io/FrozenRecon/)
    * [Spacetime Surface Regularization for Neural Dynamic Scene Reconstruction](https://openaccess.thecvf.com/content/ICCV2023/papers/Choe_Spacetime_Surface_Regularization_for_Neural_Dynamic_Scene_Reconstruction_ICCV_2023_paper.pdf)
* 深度估计
  * [MAMo: Leveraging Memory and Attention for Monocular Video Depth Estimation](http://arxiv.org/abs/2307.14336v1)
  * [GasMono: Geometry-Aided Self-Supervised Monocular Depth Estimation for Indoor Scenes](https://openaccess.thecvf.com/content/ICCV2023/papers/Zhao_GasMono_Geometry-Aided_Self-Supervised_Monocular_Depth_Estimation_for_Indoor_Scenes_ICCV_2023_paper.pdf)
  * [NDDepth: Normal-Distance Assisted Monocular Depth Estimation](http://arxiv.org/abs/2309.10592v1)
  * [V-FUSE: Volumetric Depth Map Fusion with Long-Range Constraints](http://arxiv.org/abs/2308.08715v1)
  * [Robust Monocular Depth Estimation under Challenging Conditions](http://arxiv.org/abs/2308.09711v1)<br>:star:[code](https://md4all.github.io)
  * [Self-Supervised Monocular Depth Estimation by Direction-aware Cumulative Convolution Network](http://arxiv.org/abs/2308.05605v1)
  * [Self-supervised Monocular Depth Estimation: Let's Talk About The Weather](https://openaccess.thecvf.com/content/ICCV2023/papers/Saunders_Self-supervised_Monocular_Depth_Estimation_Lets_Talk_About_The_Weather_ICCV_2023_paper.pdf)
  * [Calibrating Panoramic Depth Estimation for Practical Localization and Mapping](http://arxiv.org/abs/2308.14005v1)
  * [Two-in-One Depth: Bridging the Gap Between Monocular and Binocular Self-supervised Depth Estimation](http://arxiv.org/abs/2309.00933v1)<br>:star:[code](https://github.com/ZM-Zhou/TiO-Depth_pytorch)
  * [Robust Geometry-Preserving Depth Estimation Using Differentiable Rendering](http://arxiv.org/abs/2309.09724v1)
  * [GEDepth: Ground Embedding for Monocular Depth Estimation](http://arxiv.org/abs/2309.09975v1)
* 深度补全  
  * [AGG-Net: Attention Guided Gated-convolutional Network for Depth Image Completion](http://arxiv.org/abs/2309.01624v1)
* Stereo Matching
  * [Uncertainty Guided Adaptive Warping for Robust and Efficient Stereo Matching](http://arxiv.org/abs/2307.14071v1)
  * [Learning Depth Estimation for Transparent and Mirror Surfaces](http://arxiv.org/abs/2307.15052v1)<br>:star:[code](https://cvlab-unibo.github.io/Depth4ToM)
  * [ELFNet: Evidential Local-global Fusion for Stereo Matching](http://arxiv.org/abs/2308.00728v1)<br>:star:[code](https://github.com/jimmy19991222/ELFNet)
* 3D形状生成
  *[Learning Versatile 3D Shape Generation with Improved AR Models](https://arxiv.org/pdf/2303.14700.pdf)

<a name="1"/>

## 1.其它(others)
* [Surface Extraction from Neural Unsigned Distance Fields](http://arxiv.org/abs/2309.08878)
* [Towards Nonlinear-Motion-Aware and Occlusion-Robust Rolling Shutter Correction](http://arxiv.org/abs/2303.18125)
* [Perpetual Humanoid Control for Real-time Simulated Avatars](https://openaccess.thecvf.com/content/ICCV2023/papers/Luo_Perpetual_Humanoid_Control_for_Real-time_Simulated_Avatars_ICCV_2023_paper.pdf)
* [Efficient Deep Space Filling Curve](https://openaccess.thecvf.com/content/ICCV2023/papers/Chen_Efficient_Deep_Space_Filling_Curve_ICCV_2023_paper.pdf)
* [Pixel-Wise Contrastive Distillation](http://arxiv.org/abs/2211.00218)
* [Anchor Structure Regularization Induced Multi-view Subspace Clustering via Enhanced Tensor Rank Minimization](https://openaccess.thecvf.com/content/ICCV2023/papers/Ji_Anchor_Structure_Regularization_Induced_Multi-view_Subspace_Clustering_via_Enhanced_Tensor_ICCV_2023_paper.pdf)
* [FACTS: First Amplify Correlations and Then Slice to Discover Bias](https://openaccess.thecvf.com/content/ICCV2023/papers/Yenamandra_FACTS_First_Amplify_Correlations_and_Then_Slice_to_Discover_Bias_ICCV_2023_paper.pdf)
* [Pairwise Similarity Learning is SimPLE](https://openaccess.thecvf.com/content/ICCV2023/papers/Wen_Pairwise_Similarity_Learning_is_SimPLE_ICCV_2023_paper.pdf)
* [PanFlowNet: A Flow-Based Deep Network for Pan-Sharpening](http://arxiv.org/abs/2305.07774)
* [Visual Explanations via Iterated Integrated Attributions](https://openaccess.thecvf.com/content/ICCV2023/papers/Barkan_Visual_Explanations_via_Iterated_Integrated_Attributions_ICCV_2023_paper.pdf)
* [Make-It-3D: High-fidelity 3D Creation from A Single Image with Diffusion Prior](https://openaccess.thecvf.com/content/ICCV2023/papers/Tang_Make-It-3D_High-fidelity_3D_Creation_from_A_Single_Image_with_Diffusion_ICCV_2023_paper.pdf)
* [Fully Attentional Networks with Self-emerging Token Labeling](https://openaccess.thecvf.com/content/ICCV2023/papers/Zhao_Fully_Attentional_Networks_with_Self-emerging_Token_Labeling_ICCV_2023_paper.pdf)
* [DMNet: Delaunay Meshing Network for 3D Shape Representation](https://openaccess.thecvf.com/content/ICCV2023/papers/Zhang_DMNet_Delaunay_Meshing_Network_for_3D_Shape_Representation_ICCV_2023_paper.pdf)
* [Eulerian Single-Photon Vision](https://openaccess.thecvf.com/content/ICCV2023/papers/Gupta_Eulerian_Single-Photon_Vision_ICCV_2023_paper.pdf)
* [Adaptive Calibrator Ensemble: Navigating Test Set Difficulty in Out-of-Distribution Scenarios](https://openaccess.thecvf.com/content/ICCV2023/papers/Zou_Adaptive_Calibrator_Ensemble_Navigating_Test_Set_Difficulty_in_Out-of-Distribution_Scenarios_ICCV_2023_paper.pdf)
* [Efficient Converted Spiking Neural Network for 3D and 2D Classification](https://openaccess.thecvf.com/content/ICCV2023/papers/Lan_Efficient_Converted_Spiking_Neural_Network_for_3D_and_2D_Classification_ICCV_2023_paper.pdf)
* [Sequential Texts Driven Cohesive Motions Synthesis with Natural Transitions](https://openaccess.thecvf.com/content/ICCV2023/papers/Li_Sequential_Texts_Driven_Cohesive_Motions_Synthesis_with_Natural_Transitions_ICCV_2023_paper.pdf)
* [Locomotion-Action-Manipulation: Synthesizing Human-Scene Interactions in Complex 3D Environments](https://openaccess.thecvf.com/content/ICCV2023/papers/Lee_Locomotion-Action-Manipulation_Synthesizing_Human-Scene_Interactions_in_Complex_3D_Environments_ICCV_2023_paper.pdf)
* [Human Preference Score: Better Aligning Text-to-Image Models with Human Preference](http://arxiv.org/abs/2303.14420)
* [D-IF: Uncertainty-aware Human Digitization via Implicit Distribution Field](https://openaccess.thecvf.com/content/ICCV2023/papers/Yang_D-IF_Uncertainty-aware_Human_Digitization_via_Implicit_Distribution_Field_ICCV_2023_paper.pdf)
* [Unsupervised Manifold Linearizing and Clustering](http://arxiv.org/abs/2301.01805)
* [P1AC: Revisiting Absolute Pose From a Single Affine Correspondence](http://arxiv.org/abs/2011.08790)
* [Mining bias-target Alignment from Voronoi Cells](http://arxiv.org/abs/2305.03691)
* [Exploring the Benefits of Visual Prompting in Differential Privacy](http://arxiv.org/abs/2303.12247)
* [The Victim and The Beneficiary: Exploiting a Poisoned Model to Train a Clean Model on Poisoned Data](https://openaccess.thecvf.com/content/ICCV2023/papers/Zhu_The_Victim_and_The_Beneficiary_Exploiting_a_Poisoned_Model_to_ICCV_2023_paper.pdf)
* [End2End Multi-View Feature Matching with Differentiable Pose Optimization](https://openaccess.thecvf.com/content/ICCV2023/papers/Roessle_End2End_Multi-View_Feature_Matching_with_Differentiable_Pose_Optimization_ICCV_2023_paper.pdf)
* [Task Agnostic Restoration of Natural Video Dynamics](http://arxiv.org/abs/2206.03753)
* [GPFL: Simultaneously Learning Global and Personalized Feature Information for Personalized Federated Learning](http://arxiv.org/abs/2308.10279)
* [Scene Graph Contrastive Learning for Embodied Navigation](https://openaccess.thecvf.com/content/ICCV2023/papers/Singh_Scene_Graph_Contrastive_Learning_for_Embodied_Navigation_ICCV_2023_paper.pdf)
* [Mastering Spatial Graph Prediction of Road Networks](http://arxiv.org/abs/2210.00828)
* [ETran: Energy-Based Transferability Estimation](http://arxiv.org/abs/2308.02027)
* [Inverse Problem Regularization with Hierarchical Variational Autoencoders](http://arxiv.org/abs/2303.11217)
* [Learning a Room with the Occ-SDF Hybrid: Signed Distance Function Mingled with Occupancy Aids Scene Representation](http://arxiv.org/abs/2303.09152)
* [Context-Aware Planning and Environment-Aware Memory for Instruction Following Embodied Agents](http://arxiv.org/abs/2308.07241)
* [Minimal Solutions to Uncalibrated Two-view Geometry with Known Epipoles](https://openaccess.thecvf.com/content/ICCV2023/papers/Nakano_Minimal_Solutions_to_Uncalibrated_Two-view_Geometry_with_Known_Epipoles_ICCV_2023_paper.pdf)
* [Deep Geometry-Aware Camera Self-Calibration from Video](https://openaccess.thecvf.com/content/ICCV2023/papers/Hagemann_Deep_Geometry-Aware_Camera_Self-Calibration_from_Video_ICCV_2023_paper.pdf)
* [Feature Proliferation -- the "Cancer" in StyleGAN and its Treatments](https://openaccess.thecvf.com/content/ICCV2023/papers/Song_Feature_Proliferation_--_the_Cancer_in_StyleGAN_and_its_Treatments_ICCV_2023_paper.pdf)
* [Adaptive Testing of Computer Vision Models](http://arxiv.org/abs/2212.02774)
* [Semantic Attention Flow Fields for Monocular Dynamic Scene Decomposition](https://openaccess.thecvf.com/content/ICCV2023/papers/Liang_Semantic_Attention_Flow_Fields_for_Monocular_Dynamic_Scene_Decomposition_ICCV_2023_paper.pdf)
* [Time-to-Contact Map by Joint Estimation of Up-to-Scale Inverse Depth and Global Motion using a Single Event Camera](https://openaccess.thecvf.com/content/ICCV2023/papers/Nunes_Time-to-Contact_Map_by_Joint_Estimation_of_Up-to-Scale_Inverse_Depth_and_ICCV_2023_paper.pdf)
* [Cross-modal Latent Space Alignment for Image to Avatar Translation](https://openaccess.thecvf.com/content/ICCV2023/papers/de_Guevara_Cross-modal_Latent_Space_Alignment_for_Image_to_Avatar_Translation_ICCV_2023_paper.pdf)
* [CC3D: Layout-Conditioned Generation of Compositional 3D Scenes](http://arxiv.org/abs/2303.12074)
* [Partition-And-Debias: Agnostic Biases Mitigation via a Mixture of Biases-Specific Experts](https://openaccess.thecvf.com/content/ICCV2023/papers/Li_Partition-And-Debias_Agnostic_Biases_Mitigation_via_a_Mixture_of_Biases-Specific_Experts_ICCV_2023_paper.pdf)
* [Learning to Transform for Generalizable Instance-wise Invariance](https://openaccess.thecvf.com/content/ICCV2023/papers/Singhal_Learning_to_Transform_for_Generalizable_Instance-wise_Invariance_ICCV_2023_paper.pdf)
* [DeePoint: Visual Pointing Recognition and Direction Estimation](http://arxiv.org/abs/2304.06977)
* [Sigmoid Loss for Language Image Pre-Training](http://arxiv.org/abs/2303.15343)
* [Tracking by 3D Model Estimation of Unknown Objects in Videos](http://arxiv.org/abs/2304.06419)
* [Physically-Plausible Illumination Distribution Estimation](https://openaccess.thecvf.com/content/ICCV2023/papers/Ershov_Physically-Plausible_Illumination_Distribution_Estimation_ICCV_2023_paper.pdf)
* [Exploiting Proximity-Aware Tasks for Embodied Social Navigation](http://arxiv.org/abs/2212.00767)
* [SkeletonMAE: Graph-based Masked Autoencoder for Skeleton Sequence Pre-training](http://arxiv.org/abs/2307.08476)
* [Studying How to Efficiently and Effectively Guide Models with Explanations](https://openaccess.thecvf.com/content/ICCV2023/papers/Rao_Studying_How_to_Efficiently_and_Effectively_Guide_Models_with_Explanations_ICCV_2023_paper.pdf)
* [NeuRBF: A Neural Fields Representation with Adaptive Radial Basis Functions](http://arxiv.org/abs/2309.15426v1)<br>:star:[code](https://oppo-us-research.github.io/NeuRBF-website/)
* [DECO: Dense Estimation of 3D Human-Scene Contact In The Wild](http://arxiv.org/abs/2309.15273v1)<br>:house:[project](https://deco.is.tue.mpg.de)
* [PHRIT: Parametric Hand Representation with Implicit Template](http://arxiv.org/abs/2309.14916v1)
* [Improving Unsupervised Visual Program Inference with Code Rewriting Families](http://arxiv.org/abs/2309.14972v1)<br>:star:[code](https://bardofcodes.github.io/coref/)
* [Generating Visual Scenes from Touch](http://arxiv.org/abs/2309.15117v1)<br>:star:[code](https://fredfyyang.github.io/vision-from-touch/)
* [LOGICSEG: Parsing Visual Semantics with Neural Logic Learning and Reasoning](http://arxiv.org/abs/2309.13556v1)<br>:star:[code](https://github.com/lingorX/LogicSeg/)
* [Automatic Animation of Hair Blowing in Still Portrait Photos](http://arxiv.org/abs/2309.14207v1)<br>:star:[code](https://nevergiveu.github.io/AutomaticHairBlowing/)
* [Cross-Modal Translation and Alignment for Survival Analysis](http://arxiv.org/abs/2309.12855v1)
* [Contrastive Pseudo Learning for Open-World DeepFake Attribution](http://arxiv.org/abs/2309.11132v1)
* [Dense 2D-3D Indoor Prediction with Sound via Aligned Cross-Modal Distillation](http://arxiv.org/abs/2309.11081v1)
* [Segmentation of Tubular Structures Using Iterative Training with Tailored Samples](http://arxiv.org/abs/2309.08727v1)
* [Active Stereo Without Pattern Projector](http://arxiv.org/abs/2309.12315v1)<br>:star:[code](https://vppstereo.github.io)<br>:star:[code](https://github.com/bartn8/vppstereo)
* [TinyCLIP: CLIP Distillation via Affinity Mimicking and Weight Inheritance](http://arxiv.org/abs/2309.12314v1)<br>:house:[project](https://aka.ms/tinyclip)
* [BANSAC: A dynamic BAyesian Network for adaptive SAmple Consensus](http://arxiv.org/abs/2309.08690v1)
* [Tree-Structured Shading Decomposition](http://arxiv.org/abs/2309.07122v1)<br>:house:[project](https://chen-geng.com/inv-shade-trees)
* [Keep It SimPool: Who Said Supervised Transformers Suffer from Attention Deficit?](http://arxiv.org/abs/2309.06891v1)<br>:star:[code](https://github.com/billpsomas/simpool)
* [Beyond Skin Tone: A Multidimensional Measure of Apparent Skin Color](http://arxiv.org/abs/2309.05148v1)
* [Multi3DRefer: Grounding Text Description to Multiple 3D Objects](http://arxiv.org/abs/2309.05251v1)
* [Panoramas from Photons](http://arxiv.org/abs/2309.03811v1)<br>:house:[project](https://wisionlab.com/project/panoramas-from-photons/)
* [SimNP: Learning Self-Similarity Priors Between Neural Points](http://arxiv.org/abs/2309.03809v1)
* [Multi-label affordance mapping from egocentric vision](http://arxiv.org/abs/2309.02120v1)
* [SoDaCam: Software-defined Cameras via Single-Photon Imaging](http://arxiv.org/abs/2309.00066v1)<br>:house:[project](https://wisionlab.com/project/sodacam/)
* [PivotNet: Vectorized Pivot Learning for End-to-end HD Map Construction](http://arxiv.org/abs/2308.16477v1)
* [Active Neural Mapping](http://arxiv.org/abs/2308.16246v1)<br>:star:[code](https://zikeyan.github.io/active-INR/index.html)
* [Reconstructing Groups of People with Hypergraph Relational Reasoning](http://arxiv.org/abs/2308.15844v1)<br>:star:[code](https://github.com/boycehbz/GroupRec)
* [Learning to Upsample by Learning to Sample](http://arxiv.org/abs/2308.15085v1)<br>:star:[code](https://github.com/tiny-smart/dysample)
* [Efficient Discovery and Effective Evaluation of Visual Perceptual Similarity: A Benchmark and Beyond](http://arxiv.org/abs/2308.14753v1)
* [S-TREK: Sequential Translation and Rotation Equivariant Keypoints for local feature extraction](http://arxiv.org/abs/2308.14598v1)
* [Unaligned 2D to 3D Translation with Conditional Vector-Quantized Code Diffusion using Transformers](http://arxiv.org/abs/2308.14152v1)
* [4D Myocardium Reconstruction with Decoupled Motion and Shape Model](http://arxiv.org/abs/2308.14083v1)
* [Hierarchical Contrastive Learning for Pattern-Generalizable Image Corruption Detection](http://arxiv.org/abs/2308.14061v1)<br>:star:[code](https://github.com/xyfJASON/HCL)
* [LDL: Line Distance Functions for Panoramic Localization](http://arxiv.org/abs/2308.13989v1)<br>:star:[code](https://github.com/82magnolia/panoramic-localization)
* [Generalized Lightness Adaptation with Channel Selective Normalization](http://arxiv.org/abs/2308.13783v1)<br>:star:[code](https://github.com/mdyao/CSNorm/)<br>:star:[code](https://github.com/mdyao/CSNorm)
* [MST-compression: Compressing and Accelerating Binary Neural Networks with Minimum Spanning Tree](http://arxiv.org/abs/2308.13735v1)
* [Late Stopping: Avoiding Confidently Learning from Mislabeled Examples](http://arxiv.org/abs/2308.13862v1)
* [Motion-Guided Masking for Spatiotemporal Representation Learning](http://arxiv.org/abs/2308.12962v1)
* [A Parse-Then-Place Approach for Generating Graphic Layouts from Textual Descriptions](http://arxiv.org/abs/2308.12700v1)
* [Self-supervised Learning of Implicit Shape Representation with Dense Correspondence for Deformable Objects](http://arxiv.org/abs/2308.12590v1)<br>:star:[code](https://iscas3dv.github.io/deformshape)
* [SUMMIT: Source-Free Adaptation of Uni-Modal Models to Multi-Modal Targets](http://arxiv.org/abs/2308.11880v1)<br>:star:[code](https://github.com/csimo005/SUMMIT)
* [DR-Tune: Improving Fine-tuning of Pretrained Visual Models by Distribution Regularization with Semantic Calibration](http://arxiv.org/abs/2308.12058v1)<br>:star:[code](https://github.com/weeknan/DR-Tune)
* [RankMixup: Ranking-Based Mixup Training for Network Calibration](http://arxiv.org/abs/2308.11990v1)
* [ACLS: Adaptive and Conditional Label Smoothing for Network Calibration](http://arxiv.org/abs/2308.11911v1)
* [Enhancing NeRF akin to Enhancing LLMs: Generalizable NeRF Transformer with Mixture-of-View-Experts](http://arxiv.org/abs/2308.11793v1)<br>:star:[code](https://github.com/VITA-Group/GNT-MOVE)
* [SPANet: Frequency-balancing Token Mixer using Spectral Pooling Aggregation Modulation](http://arxiv.org/abs/2308.11568v1)<br>:star:[code](https://doranlyong.github.io/projects/spanet/)
* [Learning a More Continuous Zero Level Set in Unsigned Distance Fields through Level Set Projection](http://arxiv.org/abs/2308.11441v1)<br>:star:[code](https://github.com/junshengzhou/LevelSetUDF)
* [CAME: Contrastive Automated Model Evaluation](http://arxiv.org/abs/2308.11111v1)
* [LDP-Feat: Image Features with Local Differential Privacy](http://arxiv.org/abs/2308.11223v1)
* [DiffCloth: Diffusion Based Garment Synthesis and Manipulation via Structural Cross-modal Semantic Alignment](http://arxiv.org/abs/2308.11206v1)
* [Diffusion Model as Representation Learner](http://arxiv.org/abs/2308.10916v1)<br>:star:[code](https://github.com/Adamdad/Repfusion)
* [MetaGCD: Learning to Continually Learn in Generalized Category Discovery](http://arxiv.org/abs/2308.11063v1)
* [Few-Shot Physically-Aware Articulated Mesh Generation via Hierarchical Deformation](http://arxiv.org/abs/2308.10898v1)<br>:star:[code](https://meowuu7.github.io/few-arti-obj-gen)
* [Improving Adversarial Robustness of Masked Autoencoders via Test-time Frequency-domain Prompting](http://arxiv.org/abs/2308.10315v1)<br>:star:[code](https://github.com/shikiw/RobustMAE)
* [Robust Mixture-of-Expert Training for Convolutional Neural Networks](http://arxiv.org/abs/2308.10110v1)<br>:star:[code](https://github.com/OPTML-Group/Robust-MoE-CNN)
* [Single Image Reflection Separation via Component Synergy](http://arxiv.org/abs/2308.10027v1)<br>:star:[code](https://github.com/mingcv/DSRNet)
* [Partition-and-Debias: Agnostic Biases Mitigation via A Mixture of Biases-Specific Experts](http://arxiv.org/abs/2308.10005v1)<br>:star:[code](https://github.com/Jiaxuan-Li/PnD)
* [On the Robustness of Open-World Test-Time Training: Self-Training with Dynamic Prototype Expansion](http://arxiv.org/abs/2308.09942v1)<br>:star:[code](https://github.com/Yushu-Li/OWTTT)
* [Understanding Self-attention Mechanism via Dynamical System Perspective](http://arxiv.org/abs/2308.09939v1)
* [A Theory of Topological Derivatives for Inverse Rendering of Geometry](http://arxiv.org/abs/2308.09865v1)<br>:star:[code](https://ishit.github.io/td/)
* [X-VoE: Measuring eXplanatory Violation of Expectation in Physical Events](http://arxiv.org/abs/2308.10441v1)<br>:house:[project](https://pku.ai/publication/intuitive2023iccv/)
* [ClothesNet: An Information-Rich 3D Garment Model Repository with Simulated Clothes Environment](http://arxiv.org/abs/2308.09987v1)
* [Leveraging Intrinsic Properties for Non-Rigid Garment Alignment](http://arxiv.org/abs/2308.09519v1)<br>:star:[code](https://jsnln.github.io/iccv2023_intrinsic/index.html)
* [Event-Guided Procedure Planning from Instructional Videos with Text Supervision](http://arxiv.org/abs/2308.08885v1)
* [Spatially and Spectrally Consistent Deep Functional Maps](http://arxiv.org/abs/2308.08871v1)<br>:star:[code](https://github.com/rqhuang88/Spatiallyand-Spectrally-Consistent-Deep-Functional-Maps)
* [Realistic Full-Body Tracking from Sparse Observations via Joint-Level Modeling](http://arxiv.org/abs/2308.08855v1)<br>:star:[code](https://zxz267.github.io/AvatarJLM)
* [Label Shift Adapter for Test-Time Adaptation under Covariate and Label Shifts](http://arxiv.org/abs/2308.08810v1)
* [ALIP: Adaptive Language-Image Pre-training with Synthetic Caption](http://arxiv.org/abs/2308.08428v1)<br>:star:[code](https://github.com/deepglint/ALIP)
* [Membrane Potential Batch Normalization for Spiking Neural Networks](http://arxiv.org/abs/2308.08359v1)<br>:star:[code](https://github.com/yfguo91/MPBN)
* [OmniZoomer: Learning to Move and Zoom in on Sphere at High-Resolution](http://arxiv.org/abs/2308.08114v1)<br>:star:[code](http://vlislab22.github.io/OmniZoomer/)
* [Inherent Redundancy in Spiking Neural Networks](http://arxiv.org/abs/2308.08227v1)<br>:star:[code](https://github.com/BICLab/ASA-SNN)
* [One-bit Flip is All You Need: When Bit-flip Attack Meets Model Training](http://arxiv.org/abs/2308.07934v1)<br>:star:[code](https://github.com/jianshuod/TBA)
* [ObjectSDF++: Improved Object-Compositional Neural Implicit Surfaces](http://arxiv.org/abs/2308.07868v1)<br>:star:[code](https://qianyiwu.github.io/objectsdf++)<br>:star:[code](https://github.com/QianyiWu/objectsdf_plus)
* [UniTR: A Unified and Efficient Multi-Modal Transformer for Bird's-Eye-View Representation](http://arxiv.org/abs/2308.07732v1)<br>:star:[code](https://github.com/Haiyang-W/UniTR)
* [PARIS: Part-level Reconstruction and Motion Analysis for Articulated Objects](http://arxiv.org/abs/2308.07391v1)<br>:star:[code](https://3dlg-hcvc.github.io/paris/)<br>:house:[project](https://youtu.be/tDSrROPCgUc)
* [Boosting Multi-modal Model Performance with Adaptive Gradient Modulation](http://arxiv.org/abs/2308.07686v1)<br>:star:[code](https://github.com/lihong2303/AGM_ICCV2023)
* [Towards Open-Set Test-Time Adaptation Utilizing the Wisdom of Crowds in Entropy Minimization](http://arxiv.org/abs/2308.06879v1)
* [RMP-Loss: Regularizing Membrane Potential Distribution for Spiking Neural Networks](http://arxiv.org/abs/2308.06787v1)
* [Estimator Meets Equilibrium Perspective: A Rectified Straight Through Estimator for Binary Neural Networks Training](http://arxiv.org/abs/2308.06689v1)
* [GIFD: A Generative Gradient Inversion Method with Feature Domain Optimization](http://arxiv.org/abs/2308.04699v1)
* [Prune Spatio-temporal Tokens by Semantic-aware Temporal Accumulation](http://arxiv.org/abs/2308.04549v1)<br>:star:[code](https://github.com/Mark12Ding/STA)
* [TIJO: Trigger Inversion with Joint Optimization for Defending Multimodal Backdoored Models](http://arxiv.org/abs/2308.03906v1)<br>:star:[code](https://github.com/SRI-CSL/TIJO)
* [3D Motion Magnification: Visualizing Subtle Motions with Time Varying Radiance Fields](http://arxiv.org/abs/2308.03757v1)<br>:star:[code](https://3d-motion-magnification.github.io)
* [Improving Pixel-based MIM by Reducing Wasted Modeling Capability](http://arxiv.org/abs/2308.00261v1)<br>:star:[code](https://github.com/open-mmlab/mmpretrain)
* [Revisiting the Parameter Efficiency of Adapters from the Perspective of Precision Redundancy](http://arxiv.org/abs/2307.16867v1)<br>:star:[code](https://github.com/JieShibo/PETL-ViT)
* [Towards General Low-Light Raw Noise Synthesis and Modeling](http://arxiv.org/abs/2307.16508v1)<br>:star:[code](https://github.com/fengzhang427/LRD)
* [Supervised Homography Learning with Realistic Dataset Generation](http://arxiv.org/abs/2307.15353v1)<br>:star:[code](https://github.com/megvii-research/RealSH)
* [Dynamic PlenOctree for Adaptive Sampling Refinement in Explicit NeRF](http://arxiv.org/abs/2307.15333v1)<br>:house:[project](https://vlislab22.github.io/DOT)
* [Spatio-Temporal Domain Awareness for Multi-Agent Collaborative Perception](http://arxiv.org/abs/2307.13929v1)
* [Conditional Cross Attention Network for Multi-Space Embedding without Entanglement in Only a SINGLE Network](http://arxiv.org/abs/2307.13254v1)
* [Rethinking Data Distillation: Do Not Overlook Calibration](http://arxiv.org/abs/2307.12463v1)
* [GlobalMapper: Arbitrary-Shaped Urban Layout Generation](http://arxiv.org/abs/2307.09693v1)
* [Kick Back & Relax: Learning to Reconstruct the World by Watching SlowTV](http://arxiv.org/abs/2307.10713v1)<br>:star:[code](https://github.com/jspenmar/slowtv_monodepth)
* [Towards Viewpoint-Invariant Visual Recognition via Adversarial Training](http://arxiv.org/abs/2307.10235v1)
* [Tuning Pre-trained Model via Moment Probing](http://arxiv.org/abs/2307.11342v1)<br>:star:[code](https://github.com/mingzeG/Moment-Probing)
* [Replay: Multi-modal Multi-view Acted Videos for Casual Holography](https://arxiv.org/abs/2307.12067)
* [TextManiA: Enriching Visual Feature by Text-driven Manifold Augmentation](http://arxiv.org/abs/2307.14611v1)
* [Rethinking Mobile Block for Efficient Attention-based Models](https://arxiv.org/pdf/2301.01146.pdf)
* [RLSAC: Reinforcement Learning enhanced Sample Consensus for End-to-End Robust Estimation](http://arxiv.org/abs/2308.05318v1)<br>:star:[code](https://github.com/IRMVLab/RLSAC)
* [Cross-Domain Product Representation Learning for Rich-Content E-Commerce](http://arxiv.org/abs/2308.05550v1)
* [Interaction-aware Joint Attention Estimation Using People Attributes](http://arxiv.org/abs/2308.05382v1)<br>:house:[project](https://anonymous.4open.science/r/anonymized_codes-ECA4)
* 数据增强
  * [HybridAugment++: Unified Frequency Spectra Perturbations for Model Robustness](http://arxiv.org/abs/2307.11823v1)
  * [MixBag: Bag-Level Data Augmentation for Learning from Label Proportions](http://arxiv.org/abs/2308.08822v1)

### 扫码CV君微信(注明：paper)入微信交流群：
![9475fa20fd5e95235d9fa23ae9587a2](https://user-images.githubusercontent.com/62801906/156720309-de92964f-a6da-464a-b21f-cfb270c13e27.png)